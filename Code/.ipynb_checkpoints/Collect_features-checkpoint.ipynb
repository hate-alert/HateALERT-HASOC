{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:55:35.661017Z",
     "start_time": "2019-08-06T11:55:32.784889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....start_cleaning.........\n",
      "<hashtag> britain exit <hashtag> rape refugee\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install tweet-preprocessor\n",
    "import pandas as pd\n",
    "from commen_preprocess import *\n",
    "import preprocessor as p\n",
    "from tqdm import tqdm\n",
    "%load_ext jupyternotify\n",
    "eng_train_dataset = pd.read_csv('../Data/english_dataset/english_dataset.tsv', sep='\\t')\n",
    "hindi_train_dataset = pd.read_csv('../Data/hindi_dataset/hindi_dataset.tsv', sep='\\t')\n",
    "german_train_dataset = pd.read_csv('../Data/german_dataset/german_dataset.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:55:36.555431Z",
     "start_time": "2019-08-06T11:55:36.520016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasoc_de_1</td>\n",
       "      <td>Frank Rennicke – Ich bin stolz https://t.co/Cm...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasoc_de_2</td>\n",
       "      <td>ANSEHEN.....und danach bitte TEILEN...TEILEN.....</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasoc_de_3</td>\n",
       "      <td>#Koeln Mohamed erkennt kein deutsches Recht so...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasoc_de_4</td>\n",
       "      <td>#SaudiArabien ist eine brutale islamische Dikt...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasoc_de_5</td>\n",
       "      <td>Bundespolizei #München hat im 1. Quartal 2019 ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_id                                               text task_1 task_2\n",
       "0  hasoc_de_1  Frank Rennicke – Ich bin stolz https://t.co/Cm...    NOT   NONE\n",
       "1  hasoc_de_2  ANSEHEN.....und danach bitte TEILEN...TEILEN.....    NOT   NONE\n",
       "2  hasoc_de_3  #Koeln Mohamed erkennt kein deutsches Recht so...    NOT   NONE\n",
       "3  hasoc_de_4  #SaudiArabien ist eine brutale islamische Dikt...    NOT   NONE\n",
       "4  hasoc_de_5  Bundespolizei #München hat im 1. Quartal 2019 ...    NOT   NONE"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T05:42:23.050642Z",
     "start_time": "2019-08-07T05:42:01.982342Z"
    }
   },
   "outputs": [],
   "source": [
    "#### commen means our method and lib means to use the library tokenizer\n",
    "def task1_save_two_preprocessing_files(path_to_folder,dataset,type_of_language):\n",
    "    folder_to_commen = path_to_folder+'_commen.txt'\n",
    "    folder_to_lib = path_to_folder+'_lib.txt'\n",
    "    with open(folder_to_commen,'w') as f:\n",
    "        for i in range(len(dataset['text'])):\n",
    "            if(type_of_language==\"hindi\"):\n",
    "                f.write(clean_hindi(dataset['text'][i])+\"\\n\")\n",
    "            else:\n",
    "                f.write(clean_english(dataset['text'][i])+\"\\n\")\n",
    "            \n",
    "    with open(folder_to_lib,'w') as f:\n",
    "        for i in range(len(dataset['text'])):\n",
    "            f.write(p.clean(dataset['text'][i])+\"\\n\")\n",
    "        \n",
    "\n",
    "####task1    \n",
    "save_two_preprocessing_files(\"../Data/english_dataset/english_data_task1\",eng_train_dataset,type_of_language=\"english\")\n",
    "save_two_preprocessing_files(\"../Data/german_dataset/german_data_task1\",german_train_dataset,type_of_language=\"german\")\n",
    "save_two_preprocessing_files(\"../Data/hindi_dataset/hindi_data_task1\",hindi_train_dataset,type_of_language=\"hindi\")\n",
    "\n",
    "eng_train_dataset_HOF = eng_train_dataset.loc[eng_train_dataset['task_1'] == 'HOF'].reset_index()\n",
    "german_train_dataset_HOF = german_train_dataset.loc[german_train_dataset['task_1'] == 'HOF'].reset_index()\n",
    "hindi_train_dataset_HOF = hindi_train_dataset.loc[hindi_train_dataset['task_1'] == 'HOF'].reset_index()\n",
    "\n",
    "####task2 and 3  \n",
    "save_two_preprocessing_files(\"../Data/english_dataset/english_data_task23\",eng_train_dataset_HOF,type_of_language=\"english\")\n",
    "save_two_preprocessing_files(\"../Data/german_dataset/german_data_task23\",german_train_dataset_HOF,type_of_language=\"german\")\n",
    "save_two_preprocessing_files(\"../Data/hindi_dataset/hindi_data_task23\",hindi_train_dataset_HOF,type_of_language=\"hindi\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T05:44:51.571051Z",
     "start_time": "2019-08-07T05:44:51.550233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_2</th>\n",
       "      <th>task_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hasoc_en_2</td>\n",
       "      <td>@politico No. We should remember very clearly ...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>hasoc_en_8</td>\n",
       "      <td>#ADOS #trendingnow #blacklivesmatter #justice ...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>hasoc_en_12</td>\n",
       "      <td>I don’t know how much more I can take! 45 is a...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>hasoc_en_16</td>\n",
       "      <td>Good work @ICC keep going just destroy the who...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>PRFN</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>hasoc_en_24</td>\n",
       "      <td>#ShameOnICC  1. ICC on Dhoni's gloves         ...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index      text_id                                               text  \\\n",
       "0      1   hasoc_en_2  @politico No. We should remember very clearly ...   \n",
       "1      7   hasoc_en_8  #ADOS #trendingnow #blacklivesmatter #justice ...   \n",
       "2     11  hasoc_en_12  I don’t know how much more I can take! 45 is a...   \n",
       "3     15  hasoc_en_16  Good work @ICC keep going just destroy the who...   \n",
       "4     23  hasoc_en_24  #ShameOnICC  1. ICC on Dhoni's gloves         ...   \n",
       "\n",
       "  task_1 task_2 task_3  \n",
       "0    HOF   HATE    TIN  \n",
       "1    HOF   PRFN    TIN  \n",
       "2    HOF   HATE    TIN  \n",
       "3    HOF   PRFN    TIN  \n",
       "4    HOF   HATE    TIN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:06:38.694147Z",
     "start_time": "2019-08-06T12:06:35.820132Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "#bert-base-multilingual-casimport torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "#from pytorch_tranformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "#from pytorch_transformers import *\n",
    "import torch\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T12:06:55.708209Z",
     "start_time": "2019-08-06T12:06:42.262166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T05:50:11.923278Z",
     "start_time": "2019-08-07T05:50:11.911043Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bert_embeddings(df,type_of_language=\"hindi\"):\n",
    "        data_df=list(df.text.values)\n",
    "        all_sentence_embeddings = []\n",
    "        for data in tqdm(data_df):\n",
    "            if(type_of_language==\"hindi\"):\n",
    "                marked_text = \"[CLS] \" + clean_hindi(data) + \" [SEP]\"\n",
    "            else:\n",
    "                marked_text = \"[CLS] \" + clean_english(data) + \" [SEP]\"\n",
    "            tokenized_text = tokenizer.tokenize(marked_text)\n",
    "            indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            segments_ids = [1] * len(tokenized_text)\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            segments_tensors = torch.tensor([segments_ids])\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "            sentence_embedding = torch.mean(encoded_layers[11], 1)\n",
    "            sentence_embedding = sentence_embedding.data.numpy()\n",
    "            all_sentence_embeddings.append(sentence_embedding[0])\n",
    "        \n",
    "        return all_sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T05:50:12.685216Z",
     "start_time": "2019-08-07T05:50:12.661703Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_bert_embeddings() got an unexpected keyword argument 'type_of_langauge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-98aa5c3b5345>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meng_sentence_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_bert_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype_of_langauge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mger_sentence_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_bert_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgerman_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype_of_langauge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"german\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhin_sentence_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_bert_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhindi_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype_of_langauge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hindi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_bert_embeddings() got an unexpected keyword argument 'type_of_langauge'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "eng_sentence_embeddings=get_bert_embeddings(eng_train_dataset,type_of_language=\"english\")\n",
    "ger_sentence_embeddings=get_bert_embeddings(german_train_dataset,type_of_language=\"german\")\n",
    "hin_sentence_embeddings=get_bert_embeddings(hindi_train_dataset,type_of_langauge=\"hindi\")\n",
    "\n",
    "\n",
    "filename = '../Data/english_dataset/no_preprocess_bert_embed_task1.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(eng_sentence_embeddings,outfile)\n",
    "outfile.close()\n",
    "\n",
    "\n",
    "filename = '../Data/german_dataset/no_preprocess_bert_embed_task1.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(ger_sentence_embeddings,outfile)\n",
    "outfile.close()\n",
    "\n",
    "filename = '../Data/hindi_dataset/no_preprocess_bert_embed_task1.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(hin_sentence_embeddings,outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-07T05:50:03.613Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2261/2261 [02:11<00:00, 17.99it/s]\n",
      " 69%|██████▉   | 1558/2261 [01:29<00:46, 15.05it/s]"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "eng_sentence_embeddings=get_bert_embeddings(eng_train_dataset_HOF,type_of_language=\"english\")\n",
    "ger_sentence_embeddings=get_bert_embeddings(german_train_dataset_HOF,type_of_language=\"german\")\n",
    "hin_sentence_embeddings=get_bert_embeddings(hindi_train_dataset_HOF,type_of_language=\"hindi\")\n",
    "\n",
    "\n",
    "filename = '../Data/english_dataset/no_preprocess_bert_embed_task23.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(eng_sentence_embeddings,outfile)\n",
    "outfile.close()\n",
    "\n",
    "\n",
    "filename = '../Data/german_dataset/no_preprocess_bert_embed_task23.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(ger_sentence_embeddings,outfile)\n",
    "outfile.close()\n",
    "\n",
    "filename = '../Data/hindi_dataset/no_preprocess_bert_embed_task23.pkl'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(hin_sentence_embeddings,outfile)\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-02T10:30:26.611484Z",
     "start_time": "2019-07-02T10:30:26.605344Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:41:42.760400Z",
     "start_time": "2019-08-06T11:41:42.054348Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_english(german_train_dataset[1][50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T11:08:47.996730Z",
     "start_time": "2019-08-06T11:08:47.889409Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_hindi(hindi_train_dataset[1][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T01:10:50.207412Z",
     "start_time": "2019-07-04T01:10:50.044545Z"
    }
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "all_emojis=[]\n",
    "for i in range(len(german_train_dataset[1])):\n",
    "    parsed_tweet = p.parse(german_train_dataset[1][i])\n",
    "    if parsed_tweet.mentions is not None:\n",
    "        for i in range(len(parsed_tweet.mentions)):\n",
    "            all_emojis.append(parsed_tweet.mentions[i].match)\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T01:10:52.212955Z",
     "start_time": "2019-07-04T01:10:52.207856Z"
    }
   },
   "outputs": [],
   "source": [
    "print(parsed_tweet.emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T01:11:00.795387Z",
     "start_time": "2019-07-04T01:10:53.153836Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "uniq_emojis=list(set(all_emojis))\n",
    "histo = []\n",
    "\n",
    "for i in uniq_emojis:\n",
    "    histo.append(all_emojis.count(i)) ## add the number of occurances to the histo list\n",
    "\n",
    "plt.bar(uniq_emojis, histo)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-04T01:11:00.805660Z",
     "start_time": "2019-07-04T01:11:00.799303Z"
    }
   },
   "outputs": [],
   "source": [
    "len(uniq_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T15:03:27.963694Z",
     "start_time": "2019-07-18T15:03:27.957609Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "print (marked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T06:13:55.363897Z",
     "start_time": "2019-07-17T06:13:55.357999Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T06:13:55.682120Z",
     "start_time": "2019-07-17T06:13:55.672123Z"
    }
   },
   "outputs": [],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "  print (tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T06:13:56.265461Z",
     "start_time": "2019-07-17T06:13:56.260299Z"
    }
   },
   "outputs": [],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T06:29:10.217899Z",
     "start_time": "2019-07-17T06:22:30.355495Z"
    }
   },
   "outputs": [],
   "source": [
    "%%notify\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T06:31:05.578893Z",
     "start_time": "2019-07-17T06:31:04.911160Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T06:31:24.377043Z",
     "start_time": "2019-07-17T06:31:24.367346Z"
    }
   },
   "outputs": [],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T06:32:57.218728Z",
     "start_time": "2019-07-17T06:32:57.206408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the hidden state embeddings into single token vectors\n",
    "\n",
    "# Holds the list of 12 layer embeddings for each token\n",
    "# Will have the shape: [# tokens, # layers, # features]\n",
    "token_embeddings = [] \n",
    "\n",
    "# For each token in the sentence...\n",
    "for token_i in range(len(tokenized_text)):\n",
    "  \n",
    "  # Holds 12 layers of hidden states for each token \n",
    "  hidden_layers = [] \n",
    "  \n",
    "  # For each of the 12 layers...\n",
    "  for layer_i in range(len(encoded_layers)):\n",
    "    \n",
    "    # Lookup the vector for `token_i` in `layer_i`\n",
    "    vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "    \n",
    "    hidden_layers.append(vec)\n",
    "    \n",
    "  token_embeddings.append(hidden_layers)\n",
    "\n",
    "# Sanity check the dimensions:\n",
    "print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
    "print (\"Number of layers per token:\", len(token_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T06:33:04.211487Z",
     "start_time": "2019-07-17T06:33:04.202544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), 0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T06:35:00.618109Z",
     "start_time": "2019-07-17T06:35:00.613100Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_embedding = torch.mean(encoded_layers[11], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T06:35:29.242901Z",
     "start_time": "2019-07-17T06:35:29.133022Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_embedding.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HASOC]",
   "language": "python",
   "name": "conda-env-HASOC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
