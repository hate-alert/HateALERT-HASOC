{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:32.930444Z",
     "start_time": "2019-09-03T04:36:30.145969Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:32.950047Z",
     "start_time": "2019-09-03T04:36:32.934100Z"
    }
   },
   "outputs": [],
   "source": [
    "# file used to write preserve the results of the classfier\n",
    "# confusion matrix and precision recall fscore matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    \n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:33.113643Z",
     "start_time": "2019-09-03T04:36:32.953108Z"
    }
   },
   "outputs": [],
   "source": [
    "##saving the classification report\n",
    "def pandas_classification_report(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='macro'))\n",
    "    avg.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support','accuracy']\n",
    "    list_all=list(metrics_summary)\n",
    "    list_all.append(cm.diagonal())\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list_all,\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-2] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:34.233111Z",
     "start_time": "2019-09-03T04:36:33.117827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....start_cleaning.........\n",
      "hashtag britain exit hashtag rape refugee\n"
     ]
    }
   ],
   "source": [
    "from commen_preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:35.173422Z",
     "start_time": "2019-09-03T04:36:34.235827Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold as skf\n",
    "\n",
    "\n",
    "###all classifier \n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "import lightgbm as lgbm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:36.144869Z",
     "start_time": "2019-09-03T04:36:35.176888Z"
    }
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore('store_english_test.h5')\n",
    "df_english = store['df']\n",
    "store = pd.HDFStore('store_hindi_test.h5')\n",
    "df_hindi = store['df']\n",
    "store = pd.HDFStore('store_german_test.h5')\n",
    "df_german = store['df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:36.542433Z",
     "start_time": "2019-09-03T04:36:36.147521Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_task1_model = joblib.load('light_gbm_eng_task_1.joblib.pkl')\n",
    "eng_task2_model = joblib.load('light_gbm_eng_task_2.joblib.pkl')\n",
    "eng_task3_model = joblib.load('light_gbm_eng_task_3.joblib.pkl')\n",
    "hin_task1_model = joblib.load('light_gbm_hin_task_1.joblib.pkl')\n",
    "hin_task2_model = joblib.load('light_gbm_hin_task_2.joblib.pkl')\n",
    "hin_task3_model = joblib.load('light_gbm_hin_task_3.joblib.pkl')\n",
    "ger_task1_model = joblib.load('light_gbm_ger_task_1.joblib.pkl')\n",
    "ger_task2_model = joblib.load('light_gbm_ger_task_2.joblib.pkl')\n",
    "\n",
    "\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:36.552953Z",
     "start_time": "2019-09-03T04:36:36.546413Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    list_all=[]\n",
    "    for i in range(len(df)):\n",
    "        list_all.append(list(df['laser_commen'][i])+\n",
    "                        list(df['laser_lib'][i])+\n",
    "                        list(df['bert_embed'][i]))\n",
    "    return list_all\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:39.120336Z",
     "start_time": "2019-09-03T04:36:36.556098Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_english=np.array(get_features(df_english))\n",
    "feat_german=np.array(get_features(df_german))\n",
    "feat_hindi=np.array(get_features(df_hindi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:39.131640Z",
     "start_time": "2019-09-03T04:36:39.124982Z"
    }
   },
   "outputs": [],
   "source": [
    "convert_reverse_label={\n",
    "    1.0:'HOF',\n",
    "    0.0:'NOT'\n",
    "}\n",
    "\n",
    "\n",
    "def convert_submission(list1):\n",
    "    list_result=[]\n",
    "    for l in list1:\n",
    "        list_result.append(convert_reverse_label[l])\n",
    "    return list_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:39.372795Z",
     "start_time": "2019-09-03T04:36:39.135188Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_task1_predict=convert_submission(list(eng_task1_model.predict(feat_english)))\n",
    "ger_task1_predict=convert_submission(list(ger_task1_model.predict(feat_german)))\n",
    "hin_task1_predict=convert_submission(list(hin_task1_model.predict(feat_hindi)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:40.445407Z",
     "start_time": "2019-09-03T04:36:40.428926Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_submit_task1 = pd.DataFrame(list(zip(list(df_english['text_id']), eng_task1_predict)), \n",
    "               columns =['text_id', 'result'])\n",
    "hin_submit_task1 = pd.DataFrame(list(zip(list(df_hindi['text_id']), hin_task1_predict)), \n",
    "               columns =['text_id', 'result'])\n",
    "ger_submit_task1 = pd.DataFrame(list(zip(list(df_german['text_id']), ger_task1_predict)), \n",
    "               columns =['text_id', 'result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:41.794657Z",
     "start_time": "2019-09-03T04:36:41.734109Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_submit_task1.to_csv(\"HateMonitors_english_task_1_run_1.tsv\",sep='\\t',index=False)\n",
    "ger_submit_task1.to_csv(\"HateMonitors_germany_task_1_run_1.tsv\",sep='\\t',index=False)\n",
    "hin_submit_task1.to_csv(\"HateMonitors_hindi_task_1_run_1.tsv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:42.716869Z",
     "start_time": "2019-09-03T04:36:42.708808Z"
    }
   },
   "outputs": [],
   "source": [
    "df_english['label']=eng_task1_predict\n",
    "df_german['label']=ger_task1_predict\n",
    "df_hindi['label']=hin_task1_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:44.136840Z",
     "start_time": "2019-09-03T04:36:44.115329Z"
    }
   },
   "outputs": [],
   "source": [
    "df_english_next=df_english[df_english['label']=='HOF'].reset_index()\n",
    "df_hindi_next=df_hindi[df_hindi['label']=='HOF'].reset_index()\n",
    "df_german_next=df_german[df_german['label']=='HOF'].reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:46.389842Z",
     "start_time": "2019-09-03T04:36:45.400219Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_english_23=np.array(get_features(df_english_next))\n",
    "feat_german_23=np.array(get_features(df_german_next))\n",
    "feat_hindi_23=np.array(get_features(df_hindi_next))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:47.254611Z",
     "start_time": "2019-09-03T04:36:47.247771Z"
    }
   },
   "outputs": [],
   "source": [
    "convert_reverse_label_task2={\n",
    "    0.0:'HATE',\n",
    "    1.0:'PRFN',\n",
    "    2.0:'OFFN'\n",
    "}\n",
    "\n",
    "\n",
    "def convert_submission_task2(list1):\n",
    "    list_result=[]\n",
    "    for l in list1:\n",
    "        list_result.append(convert_reverse_label_task2[l])\n",
    "    return list_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T04:36:48.143148Z",
     "start_time": "2019-09-03T04:36:48.112527Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_task2_predict=convert_submission_task2(list(eng_task2_model.predict(feat_english_23)))\n",
    "ger_task2_predict=convert_submission_task2(list(ger_task2_model.predict(feat_german_23)))\n",
    "hin_task2_predict=convert_submission_task2(list(hin_task2_model.predict(feat_hindi_23)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T05:06:02.376001Z",
     "start_time": "2019-09-03T05:06:02.362207Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_intm_task2 = pd.DataFrame(list(zip(list(df_english_next['text_id']), eng_task2_predict)), \n",
    "               columns =['text_id', 'result'])\n",
    "hin_intm_task2 = pd.DataFrame(list(zip(list(df_hindi_next['text_id']), hin_task2_predict)), \n",
    "               columns =['text_id', 'result'])\n",
    "ger_intm_task2 = pd.DataFrame(list(zip(list(df_german_next['text_id']), ger_task2_predict)), \n",
    "               columns =['text_id', 'result'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T05:06:03.532767Z",
     "start_time": "2019-09-03T05:06:03.522819Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_actual_result(df1,df2):\n",
    "        list_id = []\n",
    "        list_predict =[]\n",
    "\n",
    "        for index,row in df1.iterrows():\n",
    "            if row['result']=='NOT':\n",
    "                list_id.append(row['text_id'])\n",
    "                list_predict.append('None')\n",
    "            else:\n",
    "                df=df2[df2['text_id']==row['text_id']].reset_index()\n",
    "                list_id.append(row['text_id'])\n",
    "                list_predict.append(df.loc[0]['result'])\n",
    "        df=pd.DataFrame(list(zip(list_id, list_predict)), \n",
    "               columns =['text_id', 'result'])\n",
    "        return df\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T05:06:19.257021Z",
     "start_time": "2019-09-03T05:06:16.663215Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_submit_task2 = get_actual_result(eng_submit_task1,eng_intm_task2)\n",
    "hin_submit_task2 = get_actual_result(hin_submit_task1,hin_intm_task2)\n",
    "ger_submit_task2 = get_actual_result(ger_submit_task1,ger_intm_task2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T05:07:22.474406Z",
     "start_time": "2019-09-03T05:07:22.452421Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_submit_task2.to_csv(\"HateMonitors_english_task_2_run_1.tsv\",sep='\\t',index=False)\n",
    "ger_submit_task2.to_csv(\"HateMonitors_germany_task_2_run_1.tsv\",sep='\\t',index=False)\n",
    "hin_submit_task2.to_csv(\"HateMonitors_hindi_task_2_run_1.tsv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T05:07:25.642430Z",
     "start_time": "2019-09-03T05:07:25.635526Z"
    }
   },
   "outputs": [],
   "source": [
    "convert_reverse_label_task3={\n",
    "    0:'TIN',\n",
    "    1:'UNT',\n",
    "}\n",
    "\n",
    "def convert_submission_task3(list1):\n",
    "    list_result=[]\n",
    "    for l in list1:\n",
    "        list_result.append(convert_reverse_label_task3[l])\n",
    "    return list_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T05:07:26.523652Z",
     "start_time": "2019-09-03T05:07:26.493688Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_task3_predict=convert_submission_task3(list(eng_task3_model.predict(feat_english_23)))\n",
    "#ger_task3_predict=convert_submission_task3(list(ger_task3_model.predict(feat_german_23)))\n",
    "hin_task3_predict=convert_submission_task3(list(hin_task3_model.predict(feat_hindi_23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T12:15:49.351601Z",
     "start_time": "2019-08-22T12:15:49.338933Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_intm_task3 = pd.DataFrame(list(zip(list(df_english_next['text_id']), eng_task3_predict)), \n",
    "               columns =['text_id', 'result'])\n",
    "hin_intm_task3 = pd.DataFrame(list(zip(list(df_hindi_next['text_id']), hin_task3_predict)), \n",
    "               columns =['text_id', 'result'])\n",
    "# ger_submit_task3 = pd.DataFrame(list(zip(list(df_german_next['text_id']), ger_task3_predict)), \n",
    "#                columns =['text_id', 'result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T12:16:44.119835Z",
     "start_time": "2019-08-22T12:16:44.106927Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_submit_task3.to_csv(\"HateMonitors_english_task_3_run_1.tsv\",sep='\\t',index=False)\n",
    "#ger_submit_task3.to_csv(\"HateMonitors_germany_task_3_run_1.tsv\",sep='\\t',index=False)\n",
    "hin_submit_task3.to_csv(\"HateMonitors_hindi_task_3_run_1.tsv\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HASOC]",
   "language": "python",
   "name": "conda-env-HASOC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
