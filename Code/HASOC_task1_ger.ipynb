{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:39:41.187667Z",
     "start_time": "2019-08-22T06:39:39.994385Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:39:41.752573Z",
     "start_time": "2019-08-22T06:39:41.729956Z"
    }
   },
   "outputs": [],
   "source": [
    "# file used to write preserve the results of the classfier\n",
    "# confusion matrix and precision recall fscore matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    \n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:39:43.715449Z",
     "start_time": "2019-08-22T06:39:43.703327Z"
    }
   },
   "outputs": [],
   "source": [
    "##saving the classification report\n",
    "def pandas_classification_report(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='macro'))\n",
    "    avg.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support','accuracy']\n",
    "    list_all=list(metrics_summary)\n",
    "    list_all.append(cm.diagonal())\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list_all,\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-2] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:39:44.413681Z",
     "start_time": "2019-08-22T06:39:43.901414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....start_cleaning.........\n",
      "hashtag britain exit hashtag rape refugee\n"
     ]
    }
   ],
   "source": [
    "from commen_preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:39:45.690952Z",
     "start_time": "2019-08-22T06:39:45.578128Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold as skf\n",
    "\n",
    "\n",
    "###all classifier \n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "import lightgbm as lgbm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:40:05.530953Z",
     "start_time": "2019-08-22T06:40:05.487921Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_train_dataset = pd.read_csv('../Data/german_dataset/german_dataset.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:41:41.338315Z",
     "start_time": "2019-08-22T06:41:41.297264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasoc_de_1</td>\n",
       "      <td>Frank Rennicke ‚Äì Ich bin¬†stolz https://t.co/Cm...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasoc_de_2</td>\n",
       "      <td>ANSEHEN.....und danach bitte TEILEN...TEILEN.....</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasoc_de_3</td>\n",
       "      <td>#Koeln Mohamed erkennt kein deutsches Recht so...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasoc_de_4</td>\n",
       "      <td>#SaudiArabien ist eine brutale islamische Dikt...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasoc_de_5</td>\n",
       "      <td>Bundespolizei #M√ºnchen hat im 1. Quartal 2019 ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hasoc_de_6</td>\n",
       "      <td>#1EuropaFuerAlle oder wie es die #SPD heute no...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hasoc_de_7</td>\n",
       "      <td>#Hannover: Weil ihm die Freizeitgestaltung und...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hasoc_de_8</td>\n",
       "      <td>#Deutschland 2019: Ort f√ºr die #EU-Wahlparty d...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hasoc_de_9</td>\n",
       "      <td>#Innsbruck: Tirolerin und T√ºrke knacken 2018 z...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hasoc_de_10</td>\n",
       "      <td>#Wien: Vier Nigerianer bestellen teils hochpre...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hasoc_de_11</td>\n",
       "      <td>#Offenburg: Somalier greift nachts auf der Str...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hasoc_de_12</td>\n",
       "      <td>#Offenburg: Die Attacke des 2015 nach D gekomm...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hasoc_de_13</td>\n",
       "      <td>Kapazit√§ten im ZDF und zur Eurowahl: Den Islam...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hasoc_de_14</td>\n",
       "      <td>mdr Umfrage 'Geh√∂rt der Islam zu Deutschland?'...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hasoc_de_15</td>\n",
       "      <td>Da musste der Moderator wohl 2 mal hinschauen ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hasoc_de_16</td>\n",
       "      <td>EU-Kommissar Frans #Timmermans sagt, der Islam...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hasoc_de_17</td>\n",
       "      <td>Wenn ein #Syrer den @Die_Gruenen erz√§hlt, dass...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hasoc_de_18</td>\n",
       "      <td>#Berlin: Ibrahima D. (24) von der Elfenbeink√ºs...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hasoc_de_19</td>\n",
       "      <td>IS-Terror-Verd√§chtiger Salafist in #Koeln fest...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hasoc_de_20</td>\n",
       "      <td>Einreise per Flugzeug: Regierung erkl√§rt Fl√ºch...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hasoc_de_21</td>\n",
       "      <td>Wegen Volksverhetzung kann nach derzeitiger Re...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hasoc_de_22</td>\n",
       "      <td>@aufklaerer2017 @ZDF @ZDFheute dass 350 Deutsc...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hasoc_de_23</td>\n",
       "      <td>@ZDF @ZDFheute Jetzt stellt euch mal vor, ein ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hasoc_de_24</td>\n",
       "      <td>Weil in #Bremen 50 Korane zerrissen wurden sen...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hasoc_de_25</td>\n",
       "      <td>@der_zmd Reden sie lieber einmal davon wieviel...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hasoc_de_26</td>\n",
       "      <td>Dem Bundestag ist es wichtiger gegen Rechts zu...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hasoc_de_27</td>\n",
       "      <td>alle m√ºssen etwas mehr tun‚ùó    Sonst wird es v...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hasoc_de_28</td>\n",
       "      <td>In Berlin werden k√ºnftig (auch) Studienabbrech...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hasoc_de_29</td>\n",
       "      <td>Ich pers√∂nlich finde, dass die Toilette der ge...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hasoc_de_30</td>\n",
       "      <td>#AfD in #Brandenburg jetzt Nummer 1. Die Wahle...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3789</th>\n",
       "      <td>hasoc_de_3790</td>\n",
       "      <td>@BlondJedi @FrReschke @annewill Gibt es denn W...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3790</th>\n",
       "      <td>hasoc_de_3791</td>\n",
       "      <td>@BlondJedi @annewill Wie krank und Hinterweltl...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3791</th>\n",
       "      <td>hasoc_de_3792</td>\n",
       "      <td>@BlondJedi @annewill Der Londoner Bus liegt na...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3792</th>\n",
       "      <td>hasoc_de_3793</td>\n",
       "      <td>@BlondJedi @annewill Die Hierarchie der Opfer ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>hasoc_de_3794</td>\n",
       "      <td>@BlondJedi @annewill Oder 51j√§hrige Frau in Ho...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>hasoc_de_3795</td>\n",
       "      <td>@BlondJedi @Wieka16 @annewill Warum so umwegig...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>hasoc_de_3796</td>\n",
       "      <td>@BlondJedi @annewill https://t.co/NsZwWOMiCC</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>hasoc_de_3797</td>\n",
       "      <td>@BlondJedi @annewill Armes Deutschland wir ord...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>hasoc_de_3798</td>\n",
       "      <td>@BlondJedi @annewill Mal ehrlich - welche Frau...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3798</th>\n",
       "      <td>hasoc_de_3799</td>\n",
       "      <td>@BlondJedi @IN_ROL_0815 @annewill Weil @annewi...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>hasoc_de_3800</td>\n",
       "      <td>@BlondJedi @annewill #Goldst√ºckAlarm, also mus...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>hasoc_de_3801</td>\n",
       "      <td>@BlondJedi @annewill H√§tten wir von vornerein ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3801</th>\n",
       "      <td>hasoc_de_3802</td>\n",
       "      <td>@BlondJedi @annewill Der Staatsfunk darf das d...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3802</th>\n",
       "      <td>hasoc_de_3803</td>\n",
       "      <td>@BlondJedi @annewill Weil falscher T√§ter? üßê</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3803</th>\n",
       "      <td>hasoc_de_3804</td>\n",
       "      <td>@BlondJedi Ey ich schei√üe auf deinen üëªüòÅ  Ich g...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3804</th>\n",
       "      <td>hasoc_de_3805</td>\n",
       "      <td>@BlondJedi Danke gleichfalls</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3805</th>\n",
       "      <td>hasoc_de_3806</td>\n",
       "      <td>@BlondJedi Ich bin Baujahr 1959 .Wir musten al...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3806</th>\n",
       "      <td>hasoc_de_3807</td>\n",
       "      <td>https://t.co/opezutyb6y</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3807</th>\n",
       "      <td>hasoc_de_3808</td>\n",
       "      <td>Sozialdemokraten haben die Themen von sogenann...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3808</th>\n",
       "      <td>hasoc_de_3809</td>\n",
       "      <td>https://t.co/bsOmR0MzED</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3809</th>\n",
       "      <td>hasoc_de_3810</td>\n",
       "      <td>Offenbach &amp;gt;80% Kinder und Jugendliche sind ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3810</th>\n",
       "      <td>hasoc_de_3811</td>\n",
       "      <td>Vielleicht lebst Du in eine Burg mit zugezogen...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3811</th>\n",
       "      <td>hasoc_de_3812</td>\n",
       "      <td>Wie ich Ihnen schon schrieb,ist die AfD erst s...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3812</th>\n",
       "      <td>hasoc_de_3813</td>\n",
       "      <td>@darksun666 Mit der Teilung derSende und Empfa...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3813</th>\n",
       "      <td>hasoc_de_3814</td>\n",
       "      <td>@darksun666 Ich wohne hier in einem Dorf,das g...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3814</th>\n",
       "      <td>hasoc_de_3815</td>\n",
       "      <td>akquirieren M√§nner,die sich um die Kinder k√ºmm...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3815</th>\n",
       "      <td>hasoc_de_3816</td>\n",
       "      <td>Ja,schon seit Jahren!  Ich mu√ü dran denken,das...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3816</th>\n",
       "      <td>hasoc_de_3817</td>\n",
       "      <td>@Kurkamp @wendt_joachim @Schroeder_Live @lamni...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3817</th>\n",
       "      <td>hasoc_de_3818</td>\n",
       "      <td>EU Wahlen!  AfD f√ºr Kindererziehungszeiten f√ºr...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3818</th>\n",
       "      <td>hasoc_de_3819</td>\n",
       "      <td>Es ist schon merkw√ºrdig,da soll ein Wettstreit...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3819 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            text_id                                               text task_1  \\\n",
       "0        hasoc_de_1  Frank Rennicke ‚Äì Ich bin¬†stolz https://t.co/Cm...    NOT   \n",
       "1        hasoc_de_2  ANSEHEN.....und danach bitte TEILEN...TEILEN.....    NOT   \n",
       "2        hasoc_de_3  #Koeln Mohamed erkennt kein deutsches Recht so...    NOT   \n",
       "3        hasoc_de_4  #SaudiArabien ist eine brutale islamische Dikt...    NOT   \n",
       "4        hasoc_de_5  Bundespolizei #M√ºnchen hat im 1. Quartal 2019 ...    NOT   \n",
       "5        hasoc_de_6  #1EuropaFuerAlle oder wie es die #SPD heute no...    NOT   \n",
       "6        hasoc_de_7  #Hannover: Weil ihm die Freizeitgestaltung und...    NOT   \n",
       "7        hasoc_de_8  #Deutschland 2019: Ort f√ºr die #EU-Wahlparty d...    NOT   \n",
       "8        hasoc_de_9  #Innsbruck: Tirolerin und T√ºrke knacken 2018 z...    NOT   \n",
       "9       hasoc_de_10  #Wien: Vier Nigerianer bestellen teils hochpre...    NOT   \n",
       "10      hasoc_de_11  #Offenburg: Somalier greift nachts auf der Str...    NOT   \n",
       "11      hasoc_de_12  #Offenburg: Die Attacke des 2015 nach D gekomm...    NOT   \n",
       "12      hasoc_de_13  Kapazit√§ten im ZDF und zur Eurowahl: Den Islam...    NOT   \n",
       "13      hasoc_de_14  mdr Umfrage 'Geh√∂rt der Islam zu Deutschland?'...    NOT   \n",
       "14      hasoc_de_15  Da musste der Moderator wohl 2 mal hinschauen ...    NOT   \n",
       "15      hasoc_de_16  EU-Kommissar Frans #Timmermans sagt, der Islam...    NOT   \n",
       "16      hasoc_de_17  Wenn ein #Syrer den @Die_Gruenen erz√§hlt, dass...    NOT   \n",
       "17      hasoc_de_18  #Berlin: Ibrahima D. (24) von der Elfenbeink√ºs...    NOT   \n",
       "18      hasoc_de_19  IS-Terror-Verd√§chtiger Salafist in #Koeln fest...    NOT   \n",
       "19      hasoc_de_20  Einreise per Flugzeug: Regierung erkl√§rt Fl√ºch...    NOT   \n",
       "20      hasoc_de_21  Wegen Volksverhetzung kann nach derzeitiger Re...    NOT   \n",
       "21      hasoc_de_22  @aufklaerer2017 @ZDF @ZDFheute dass 350 Deutsc...    NOT   \n",
       "22      hasoc_de_23  @ZDF @ZDFheute Jetzt stellt euch mal vor, ein ...    NOT   \n",
       "23      hasoc_de_24  Weil in #Bremen 50 Korane zerrissen wurden sen...    NOT   \n",
       "24      hasoc_de_25  @der_zmd Reden sie lieber einmal davon wieviel...    NOT   \n",
       "25      hasoc_de_26  Dem Bundestag ist es wichtiger gegen Rechts zu...    NOT   \n",
       "26      hasoc_de_27  alle m√ºssen etwas mehr tun‚ùó    Sonst wird es v...    NOT   \n",
       "27      hasoc_de_28  In Berlin werden k√ºnftig (auch) Studienabbrech...    NOT   \n",
       "28      hasoc_de_29  Ich pers√∂nlich finde, dass die Toilette der ge...    NOT   \n",
       "29      hasoc_de_30  #AfD in #Brandenburg jetzt Nummer 1. Die Wahle...    NOT   \n",
       "...             ...                                                ...    ...   \n",
       "3789  hasoc_de_3790  @BlondJedi @FrReschke @annewill Gibt es denn W...    NOT   \n",
       "3790  hasoc_de_3791  @BlondJedi @annewill Wie krank und Hinterweltl...    NOT   \n",
       "3791  hasoc_de_3792  @BlondJedi @annewill Der Londoner Bus liegt na...    NOT   \n",
       "3792  hasoc_de_3793  @BlondJedi @annewill Die Hierarchie der Opfer ...    NOT   \n",
       "3793  hasoc_de_3794  @BlondJedi @annewill Oder 51j√§hrige Frau in Ho...    NOT   \n",
       "3794  hasoc_de_3795  @BlondJedi @Wieka16 @annewill Warum so umwegig...    NOT   \n",
       "3795  hasoc_de_3796       @BlondJedi @annewill https://t.co/NsZwWOMiCC    NOT   \n",
       "3796  hasoc_de_3797  @BlondJedi @annewill Armes Deutschland wir ord...    NOT   \n",
       "3797  hasoc_de_3798  @BlondJedi @annewill Mal ehrlich - welche Frau...    HOF   \n",
       "3798  hasoc_de_3799  @BlondJedi @IN_ROL_0815 @annewill Weil @annewi...    NOT   \n",
       "3799  hasoc_de_3800  @BlondJedi @annewill #Goldst√ºckAlarm, also mus...    NOT   \n",
       "3800  hasoc_de_3801  @BlondJedi @annewill H√§tten wir von vornerein ...    NOT   \n",
       "3801  hasoc_de_3802  @BlondJedi @annewill Der Staatsfunk darf das d...    NOT   \n",
       "3802  hasoc_de_3803        @BlondJedi @annewill Weil falscher T√§ter? üßê    NOT   \n",
       "3803  hasoc_de_3804  @BlondJedi Ey ich schei√üe auf deinen üëªüòÅ  Ich g...    NOT   \n",
       "3804  hasoc_de_3805                       @BlondJedi Danke gleichfalls    NOT   \n",
       "3805  hasoc_de_3806  @BlondJedi Ich bin Baujahr 1959 .Wir musten al...    NOT   \n",
       "3806  hasoc_de_3807                            https://t.co/opezutyb6y    NOT   \n",
       "3807  hasoc_de_3808  Sozialdemokraten haben die Themen von sogenann...    NOT   \n",
       "3808  hasoc_de_3809                            https://t.co/bsOmR0MzED    NOT   \n",
       "3809  hasoc_de_3810  Offenbach &gt;80% Kinder und Jugendliche sind ...    NOT   \n",
       "3810  hasoc_de_3811  Vielleicht lebst Du in eine Burg mit zugezogen...    NOT   \n",
       "3811  hasoc_de_3812  Wie ich Ihnen schon schrieb,ist die AfD erst s...    NOT   \n",
       "3812  hasoc_de_3813  @darksun666 Mit der Teilung derSende und Empfa...    NOT   \n",
       "3813  hasoc_de_3814  @darksun666 Ich wohne hier in einem Dorf,das g...    NOT   \n",
       "3814  hasoc_de_3815  akquirieren M√§nner,die sich um die Kinder k√ºmm...    NOT   \n",
       "3815  hasoc_de_3816  Ja,schon seit Jahren!  Ich mu√ü dran denken,das...    NOT   \n",
       "3816  hasoc_de_3817  @Kurkamp @wendt_joachim @Schroeder_Live @lamni...    NOT   \n",
       "3817  hasoc_de_3818  EU Wahlen!  AfD f√ºr Kindererziehungszeiten f√ºr...    NOT   \n",
       "3818  hasoc_de_3819  Es ist schon merkw√ºrdig,da soll ein Wettstreit...    NOT   \n",
       "\n",
       "     task_2  \n",
       "0      NONE  \n",
       "1      NONE  \n",
       "2      NONE  \n",
       "3      NONE  \n",
       "4      NONE  \n",
       "5      NONE  \n",
       "6      NONE  \n",
       "7      NONE  \n",
       "8      NONE  \n",
       "9      NONE  \n",
       "10     NONE  \n",
       "11     NONE  \n",
       "12     NONE  \n",
       "13     NONE  \n",
       "14     NONE  \n",
       "15     NONE  \n",
       "16     NONE  \n",
       "17     NONE  \n",
       "18     NONE  \n",
       "19     NONE  \n",
       "20     NONE  \n",
       "21     NONE  \n",
       "22     NONE  \n",
       "23     NONE  \n",
       "24     NONE  \n",
       "25     NONE  \n",
       "26     NONE  \n",
       "27     NONE  \n",
       "28     NONE  \n",
       "29     NONE  \n",
       "...     ...  \n",
       "3789   NONE  \n",
       "3790   NONE  \n",
       "3791   NONE  \n",
       "3792   NONE  \n",
       "3793   NONE  \n",
       "3794   NONE  \n",
       "3795   NONE  \n",
       "3796   NONE  \n",
       "3797   HATE  \n",
       "3798   NONE  \n",
       "3799   NONE  \n",
       "3800   NONE  \n",
       "3801   NONE  \n",
       "3802   NONE  \n",
       "3803   NONE  \n",
       "3804   NONE  \n",
       "3805   NONE  \n",
       "3806   NONE  \n",
       "3807   NONE  \n",
       "3808   NONE  \n",
       "3809   NONE  \n",
       "3810   NONE  \n",
       "3811   NONE  \n",
       "3812   NONE  \n",
       "3813   NONE  \n",
       "3814   NONE  \n",
       "3815   NONE  \n",
       "3816   NONE  \n",
       "3817   NONE  \n",
       "3818   NONE  \n",
       "\n",
       "[3819 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:40:09.637428Z",
     "start_time": "2019-08-22T06:40:09.626828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total dataset size: 3819 \n",
      " NOT    3412\n",
      "HOF     407\n",
      "Name: task_1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "l=eng_train_dataset['task_1'].value_counts()\n",
    "print(\"the total dataset size:\",len(eng_train_dataset),'\\n',l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:42:58.119506Z",
     "start_time": "2019-08-22T06:42:58.105560Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "####loading laser embeddings for english dataset\n",
    "def load_laser_embeddings():\n",
    "        dim = 1024\n",
    "        engX_commen = np.fromfile(\"../Data/german_dataset/embeddings_ger_task1_commen.raw\", dtype=np.float32, count=-1)                                                                          \n",
    "        engX_lib = np.fromfile(\"../Data/german_dataset/embeddings_ger_task1_lib.raw\", dtype=np.float32, count=-1)                                                                          \n",
    "        engX_commen.resize(engX_commen.shape[0] // dim, dim)                                                                          \n",
    "        engX_lib.resize(engX_lib.shape[0] // dim, dim)                                                                          \n",
    "        return engX_commen,engX_lib\n",
    "    \n",
    "def load_bert_embeddings():\n",
    "        file = open('../Data/german_dataset/no_preprocess_bert_embed_task1.pkl', 'rb')\n",
    "        embeds = pickle.load(file)\n",
    "        return np.array(embeds)\n",
    "        \n",
    "def merge_feature(*args):\n",
    "    feat_all=[]\n",
    "    print(args[0].shape)\n",
    "    for  i in tqdm(range(args[0].shape[0])):\n",
    "        feat=[]\n",
    "        for arg in args:\n",
    "            feat+=list(arg[i])\n",
    "        feat_all.append(feat)\n",
    "    return feat_all\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:42:58.826381Z",
     "start_time": "2019-08-22T06:42:58.821629Z"
    }
   },
   "outputs": [],
   "source": [
    "convert_label={\n",
    "    'HOF':1,\n",
    "    'NOT':0\n",
    "}\n",
    "\n",
    "\n",
    "convert_reverse_label={\n",
    "    1:'HOF',\n",
    "    0:'NOT'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:43:00.059688Z",
     "start_time": "2019-08-22T06:42:59.562853Z"
    }
   },
   "outputs": [],
   "source": [
    "labels=eng_train_dataset['task_1'].values\n",
    "engX_commen,engX_lib=load_laser_embeddings()\n",
    "bert_embeds =load_bert_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:43:02.575030Z",
     "start_time": "2019-08-22T06:43:00.936918Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 824/3819 [00:00<00:00, 4016.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3819, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3819/3819 [00:00<00:00, 4550.10it/s]\n"
     ]
    }
   ],
   "source": [
    "feat_all=merge_feature(engX_commen,engX_lib,bert_embeds)\n",
    "#feat_all=merge_feature(engX_lib)\n",
    "\n",
    "\n",
    "# feat_all=[]\n",
    "# for  i in range(len(labels)):\n",
    "#     feat=list(engX_commen[i])+list(engX_lib[i])\n",
    "#     feat_all.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:43:02.581349Z",
     "start_time": "2019-08-22T06:43:02.577098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2816"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:43:03.695751Z",
     "start_time": "2019-08-22T06:43:02.583146Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "Classifier_Train_X=np.array(feat_all)\n",
    "labels_int=[]\n",
    "for i in range(len(labels)):\n",
    "    labels_int.append(convert_label[labels[i]])\n",
    "\n",
    "Classifier_Train_Y=np.array(labels_int,dtype='float64')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:43:03.798980Z",
     "start_time": "2019-08-22T06:43:03.698003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3819"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type_of_target(Classifier_Train_Y))\n",
    "len(Classifier_Train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T15:34:09.864197Z",
     "start_time": "2019-08-06T15:34:09.823545Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:43:05.393348Z",
     "start_time": "2019-08-22T06:43:05.345793Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_no_ext(Classifier_Train_X,Classifier_Train_Y,model_type,save_model=False):\n",
    "    kf = skf(n_splits=10,shuffle=True)\n",
    "    y_total_preds=[] \n",
    "    y_total=[]\n",
    "    count=0\n",
    "    img_name = 'cm.png'\n",
    "    report_name = 'report.csv'\n",
    "    \n",
    "    scale=list(Classifier_Train_Y).count(0)/list(Classifier_Train_Y).count(1)\n",
    "    print(scale)\n",
    "    \n",
    "    if(save_model==True):\n",
    "        Classifier=get_model(scale,m_type=model_type)\n",
    "        Classifier.fit(Classifier_Train_X,Classifier_Train_Y)\n",
    "        filename = model_type+'_ger_task_1.joblib.pkl'\n",
    "        joblib.dump(Classifier, filename, compress=9)\n",
    "#         filename1 = model_name+'select_features_eng_task1.joblib.pkl'\n",
    "#         joblib.dump(model_featureSelection, filename1, compress=9)\n",
    "    else:\n",
    "        for train_index, test_index in kf.split(Classifier_Train_X,Classifier_Train_Y):\n",
    "            X_train, X_test = Classifier_Train_X[train_index], Classifier_Train_X[test_index]\n",
    "            y_train, y_test = Classifier_Train_Y[train_index], Classifier_Train_Y[test_index]\n",
    "\n",
    "            classifier=get_model(scale,m_type=model_type)\n",
    "            print(type(y_train))\n",
    "            classifier.fit(X_train,y_train)\n",
    "            y_preds = classifier.predict(X_test)\n",
    "            for ele in y_test:\n",
    "                y_total.append(ele)\n",
    "            for ele in y_preds:\n",
    "                y_total_preds.append(ele)\n",
    "            y_pred_train = classifier.predict(X_train)\n",
    "            print(y_pred_train)\n",
    "            print(y_train)\n",
    "            count=count+1       \n",
    "            print('accuracy_train:',accuracy_score(y_train, y_pred_train),'accuracy_test:',accuracy_score(y_test, y_preds))\n",
    "            print('TRAINING:')\n",
    "            print(classification_report( y_train, y_pred_train ))\n",
    "            print(\"TESTING:\")\n",
    "            print(classification_report( y_test, y_preds ))\n",
    "\n",
    "        report = classification_report( y_total, y_total_preds )\n",
    "        cm=confusion_matrix(y_total, y_total_preds)\n",
    "        plt=plot_confusion_matrix(cm,normalize= True,target_names = ['NOT','HOF'],title = \"Confusion Matrix\")\n",
    "        plt.savefig('hin_task1'+model_type+'_'+img_name)\n",
    "        print(classifier)\n",
    "        print(report)\n",
    "        print(accuracy_score(y_total, y_total_preds))\n",
    "        df_result=pandas_classification_report(y_total,y_total_preds)\n",
    "        df_result.to_csv('hin_task1'+model_type+'_'+report_name,  sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:43:06.232702Z",
     "start_time": "2019-08-22T06:43:06.213709Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(scale,m_type=None):\n",
    "    if not m_type:\n",
    "        print(\"ERROR: Please specify a model type!\")\n",
    "        return None\n",
    "    if m_type == 'decision_tree_classifier':\n",
    "        logreg = tree.DecisionTreeClassifier(max_features=1000,max_depth=3,class_weight='balanced')\n",
    "    elif m_type == 'gaussian':\n",
    "        logreg = GaussianNB()\n",
    "    elif m_type == 'logistic_regression':\n",
    "        logreg = LogisticRegression(n_jobs=10, random_state=42,class_weight='balanced',solver='liblinear')\n",
    "    elif m_type == 'MLPClassifier':\n",
    "#         logreg = neural_network.MLPClassifier((500))\n",
    "        logreg = neural_network.MLPClassifier((100),random_state=42,early_stopping=True)\n",
    "    elif m_type == 'KNeighborsClassifier':\n",
    "#         logreg = neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
    "        logreg = neighbors.KNeighborsClassifier()\n",
    "    elif m_type == 'ExtraTreeClassifier':\n",
    "        logreg = tree.ExtraTreeClassifier()\n",
    "    elif m_type == 'ExtraTreeClassifier_2':\n",
    "        logreg = ensemble.ExtraTreesClassifier()\n",
    "    elif m_type == 'RandomForestClassifier':\n",
    "        logreg = ensemble.RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=12, max_depth=7)\n",
    "    elif m_type == 'SVC':\n",
    "        #logreg = LinearSVC(dual=False,max_iter=200)\n",
    "        logreg = SVC(kernel='linear',random_state=1526)\n",
    "    elif m_type == 'Catboost':\n",
    "        logreg = CatBoostClassifier(iterations=100,learning_rate=0.2,l2_leaf_reg=500,depth=10,use_best_model=False, random_state=42,scale_pos_weight=SCALE_POS_WEIGHT)\n",
    "#         logreg = CatBoostClassifier(scale_pos_weight=0.8, random_seed=42,);\n",
    "    elif m_type == 'XGB_classifier':\n",
    "#         logreg=XGBClassifier(silent=False,eta=0.1,objective='binary:logistic',max_depth=5,min_child_weight=0,gamma=0.2,subsample=0.8, colsample_bytree = 0.8,scale_pos_weight=1,n_estimators=500,reg_lambda=3,nthread=12)\n",
    "        logreg=XGBClassifier(silent=False,objective='binary:logistic',scale_pos_weight=SCALE_POS_WEIGHT,reg_lambda=3,nthread=12, random_state=42)\n",
    "    elif m_type == 'light_gbm':\n",
    "        logreg = LGBMClassifier(objective='binary',max_depth=3,learning_rate=0.2,num_leaves=20,scale_pos_weight=scale,boosting_type='gbdt',\n",
    "                                metric='binary_logloss',random_state=5,reg_lambda=20,silent=False)\n",
    "    else:\n",
    "        print(\"give correct model\")\n",
    "    print(logreg)\n",
    "    return logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:43:07.648008Z",
     "start_time": "2019-08-22T06:43:07.643484Z"
    }
   },
   "outputs": [],
   "source": [
    "models_name=['decision_tree_classifier','gaussian','logistic_regression','MLPClassifier','RandomForestClassifier',\n",
    "             'SVC','light_gbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:40:29.327396Z",
     "start_time": "2019-08-22T06:40:29.071663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.383292383292384\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4665, 3819]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-4f342d66f9ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_model_no_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier_Train_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mClassifier_Train_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-e5cfccefeb54>\u001b[0m in \u001b[0;36mtrain_model_no_ext\u001b[0;34m(Classifier_Train_X, Classifier_Train_Y, model_type, save_model)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#         joblib.dump(model_featureSelection, filename1, compress=9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier_Train_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mClassifier_Train_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier_Train_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassifier_Train_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier_Train_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassifier_Train_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0mset\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthat\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \"\"\"\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4665, 3819]"
     ]
    }
   ],
   "source": [
    "for model in models_name:\n",
    "    train_model_no_ext(Classifier_Train_X,Classifier_Train_Y,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:43:11.688732Z",
     "start_time": "2019-08-22T06:43:09.649868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.383292383292384\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='binary_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='binary', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=8.383292383292384,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n"
     ]
    }
   ],
   "source": [
    "train_model_no_ext(Classifier_Train_X,Classifier_Train_Y,models_name[-1],save_model=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T15:40:28.723123Z",
     "start_time": "2019-08-06T15:39:12.662219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.588235294117647\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-aa4049443f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model_no_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifier_Train_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mClassifier_Train_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SVC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-78e45171d317>\u001b[0m in \u001b[0;36mtrain_model_no_ext\u001b[0;34m(Classifier_Train_X, Classifier_Train_Y, model_type, save_model)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model_no_ext(Classifier_Train_X,Classifier_Train_Y,'SVC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HASOC]",
   "language": "python",
   "name": "conda-env-HASOC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
