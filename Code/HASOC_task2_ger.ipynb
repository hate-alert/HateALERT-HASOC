{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:45.313396Z",
     "start_time": "2019-08-22T06:46:44.180667Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:45.326053Z",
     "start_time": "2019-08-22T06:46:45.315660Z"
    }
   },
   "outputs": [],
   "source": [
    "# file used to write preserve the results of the classfier\n",
    "# confusion matrix and precision recall fscore matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    \n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:45.456151Z",
     "start_time": "2019-08-22T06:46:45.327976Z"
    }
   },
   "outputs": [],
   "source": [
    "##saving the classification report\n",
    "def pandas_classification_report(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='macro'))\n",
    "    avg.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support','accuracy']\n",
    "    list_all=list(metrics_summary)\n",
    "    list_all.append(cm.diagonal())\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list_all,\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-2] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:46.002795Z",
     "start_time": "2019-08-22T06:46:45.458066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....start_cleaning.........\n",
      "hashtag britain exit hashtag rape refugee\n"
     ]
    }
   ],
   "source": [
    "from commen_preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:46.032407Z",
     "start_time": "2019-08-22T06:46:46.005022Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_train_dataset = pd.read_csv('../Data/german_dataset/german_dataset.tsv', sep='\\t')\n",
    "# #hindi_train_dataset = pd.read_csv('../Data/hindi_dataset/hindi_dataset.tsv', sep='\\t',header=None)\n",
    "# german_train_dataset = pd.read_csv('../Data/german_dataset/german_dataset_added_features.tsv', sep=',')\n",
    "# eng_train_dataset=eng_train_dataset.drop(['Unnamed: 0'], axis=1)\n",
    "# german_train_dataset=german_train_dataset.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "eng_train_dataset = eng_train_dataset.loc[eng_train_dataset['task_1'] == 'HOF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:47.162790Z",
     "start_time": "2019-08-22T06:46:47.136873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>hasoc_de_39</td>\n",
       "      <td>ðŸ’©ðŸ‘‰'Cohn-Bendit ist kein altersverwirrter Spinn...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>hasoc_de_67</td>\n",
       "      <td>@ArasBacho Wer hat dem kriminellen Grapscher d...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>hasoc_de_83</td>\n",
       "      <td>Alfred Nobel wÃ¼rde sich im Grab rumdrehen.  Ei...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>hasoc_de_97</td>\n",
       "      <td>Das SchÃ¶ne am Klimawandel: Damit lÃ¤sst sich vo...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>hasoc_de_99</td>\n",
       "      <td>@lawyerberlin @Lebensformation ich nenne jeden...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>OFFN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                               text task_1  \\\n",
       "38  hasoc_de_39  ðŸ’©ðŸ‘‰'Cohn-Bendit ist kein altersverwirrter Spinn...    HOF   \n",
       "66  hasoc_de_67  @ArasBacho Wer hat dem kriminellen Grapscher d...    HOF   \n",
       "82  hasoc_de_83  Alfred Nobel wÃ¼rde sich im Grab rumdrehen.  Ei...    HOF   \n",
       "96  hasoc_de_97  Das SchÃ¶ne am Klimawandel: Damit lÃ¤sst sich vo...    HOF   \n",
       "98  hasoc_de_99  @lawyerberlin @Lebensformation ich nenne jeden...    HOF   \n",
       "\n",
       "   task_2  \n",
       "38   OFFN  \n",
       "66   OFFN  \n",
       "82   OFFN  \n",
       "96   OFFN  \n",
       "98   OFFN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:47.388714Z",
     "start_time": "2019-08-22T06:46:47.379885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OFFN    210\n",
      "HATE    111\n",
      "PRFN     86\n",
      "Name: task_2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "l=eng_train_dataset['task_2'].value_counts()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:47.909995Z",
     "start_time": "2019-08-22T06:46:47.889865Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "####loading laser embeddings for english dataset\n",
    "def load_laser_embeddings():\n",
    "        dim = 1024\n",
    "        engX_commen = np.fromfile(\"../Data/german_dataset/embeddings_ger_task23_commen.raw\", dtype=np.float32, count=-1)                                                                          \n",
    "        engX_lib = np.fromfile(\"../Data/german_dataset/embeddings_ger_task23_lib.raw\", dtype=np.float32, count=-1)                                                                          \n",
    "        engX_commen.resize(engX_commen.shape[0] // dim, dim)                                                                          \n",
    "        engX_lib.resize(engX_lib.shape[0] // dim, dim)                                                                          \n",
    "        return engX_commen,engX_lib\n",
    "    \n",
    "def load_bert_embeddings():\n",
    "        file = open('../Data/german_dataset/no_preprocess_bert_embed_task23.pkl', 'rb')\n",
    "        embeds = pickle.load(file)\n",
    "        return np.array(embeds)\n",
    "        \n",
    "def merge_feature(*args):\n",
    "    feat_all=[]\n",
    "    print(args[0].shape)\n",
    "    for  i in tqdm(range(args[0].shape[0])):\n",
    "        feat=[]\n",
    "        for arg in args:\n",
    "            feat+=list(arg[i])\n",
    "        feat_all.append(feat)\n",
    "    return feat_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:48.085415Z",
     "start_time": "2019-08-22T06:46:48.080365Z"
    }
   },
   "outputs": [],
   "source": [
    "convert_label={\n",
    "    'HATE':0,\n",
    "    'PRFN':1,\n",
    "    'OFFN':2\n",
    "}\n",
    "\n",
    "\n",
    "convert_reverse_label={\n",
    "    0:'HATE',\n",
    "    1:'PRFN',\n",
    "    2:'OFFN'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:48.457654Z",
     "start_time": "2019-08-22T06:46:48.251707Z"
    }
   },
   "outputs": [],
   "source": [
    "labels=eng_train_dataset['task_2'].values\n",
    "engX_commen,engX_lib=load_laser_embeddings()\n",
    "bert_embeds =load_bert_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:48.736068Z",
     "start_time": "2019-08-22T06:46:48.602730Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 407/407 [00:00<00:00, 3325.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(407, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2816"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_all=merge_feature(engX_commen,engX_lib,bert_embeds)\n",
    "len(feat_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:49.357713Z",
     "start_time": "2019-08-22T06:46:49.205215Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "Classifier_Train_X=np.array(feat_all)\n",
    "labels_int=[]\n",
    "for i in range(len(labels)):\n",
    "    labels_int.append(convert_label[labels[i]])\n",
    "\n",
    "Classifier_Train_Y=np.array(labels_int,dtype='float64')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:50.994331Z",
     "start_time": "2019-08-22T06:46:50.977008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiclass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 0.,\n",
       "       2., 0., 2., 0., 0., 0., 0., 0., 1., 1., 2., 2., 0., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 1., 1., 2., 2., 1., 2., 2., 1., 2., 2., 2., 1., 1.,\n",
       "       0., 2., 1., 1., 2., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 0.,\n",
       "       2., 2., 2., 2., 2., 0., 1., 2., 2., 2., 2., 0., 2., 2., 2., 2., 0.,\n",
       "       2., 2., 1., 2., 2., 1., 2., 0., 2., 2., 2., 2., 2., 2., 2., 1., 1.,\n",
       "       0., 0., 2., 2., 2., 2., 2., 2., 0., 2., 2., 2., 2., 0., 0., 0., 1.,\n",
       "       2., 2., 2., 1., 2., 1., 1., 2., 2., 2., 1., 2., 1., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 1., 1., 2., 2., 2., 2., 2., 2., 1., 1., 1., 2., 1.,\n",
       "       1., 1., 2., 1., 2., 1., 1., 1., 1., 1., 2., 2., 1., 1., 2., 1., 2.,\n",
       "       1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 1.,\n",
       "       2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2.,\n",
       "       1., 1., 2., 1., 2., 2., 2., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 0., 2., 1., 0., 0.,\n",
       "       0., 0., 1., 2., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 2., 2., 1., 2., 2., 1., 2., 1., 2., 2., 2., 0.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 1., 1., 2., 1., 1., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 0., 2., 2., 0., 2., 2.,\n",
       "       2., 2., 0., 2., 0., 2., 2., 2., 2., 0., 1., 1., 2., 1., 2., 2., 2.,\n",
       "       1., 2., 2., 0., 1., 1., 1., 2., 0., 2., 0., 1., 1., 2., 1., 1., 2.,\n",
       "       0., 2., 0., 0., 1., 1., 0., 2., 2., 0., 1., 0., 0., 2., 0., 2., 2.,\n",
       "       2., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type_of_target(Classifier_Train_Y))\n",
    "Classifier_Train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:46:51.880048Z",
     "start_time": "2019-08-22T06:46:51.756229Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold as skf\n",
    "\n",
    "\n",
    "###all classifier \n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "import lightgbm as lgbm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:47:02.306297Z",
     "start_time": "2019-08-22T06:47:02.284196Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_no_ext(Classifier_Train_X,Classifier_Train_Y,model_type,save_model=False):\n",
    "    kf = skf(n_splits=10,shuffle=True)\n",
    "    y_total_preds=[] \n",
    "    y_total=[]\n",
    "    count=0\n",
    "    img_name = 'cm.png'\n",
    "    report_name = 'report.csv'\n",
    "    \n",
    "    scale=list(Classifier_Train_Y).count(0)/list(Classifier_Train_Y).count(1)\n",
    "    print(scale)\n",
    "    \n",
    "    if(save_model==True):\n",
    "        Classifier=get_model(scale,m_type=model_type)\n",
    "        Classifier.fit(Classifier_Train_X,Classifier_Train_Y)\n",
    "        filename = model_type+'_ger_task_2.joblib.pkl'\n",
    "        joblib.dump(Classifier, filename, compress=9)\n",
    "#         filename1 = model_name+'select_features_eng_task1.joblib.pkl'\n",
    "#         joblib.dump(model_featureSelection, filename1, compress=9)\n",
    "    else:\n",
    "        for train_index, test_index in kf.split(Classifier_Train_X,Classifier_Train_Y):\n",
    "            X_train, X_test = Classifier_Train_X[train_index], Classifier_Train_X[test_index]\n",
    "            y_train, y_test = Classifier_Train_Y[train_index], Classifier_Train_Y[test_index]\n",
    "\n",
    "            classifier=get_model(scale,m_type=model_type)\n",
    "            print(type(y_train))\n",
    "            classifier.fit(X_train,y_train)\n",
    "            y_preds = classifier.predict(X_test)\n",
    "            for ele in y_test:\n",
    "                y_total.append(ele)\n",
    "            for ele in y_preds:\n",
    "                y_total_preds.append(ele)\n",
    "            y_pred_train = classifier.predict(X_train)\n",
    "            print(y_pred_train)\n",
    "            print(y_train)\n",
    "            count=count+1       \n",
    "            print('accuracy_train:',accuracy_score(y_train, y_pred_train),'accuracy_test:',accuracy_score(y_test, y_preds))\n",
    "            print('TRAINING:')\n",
    "            print(classification_report( y_train, y_pred_train ))\n",
    "            print(\"TESTING:\")\n",
    "            print(classification_report( y_test, y_preds ))\n",
    "\n",
    "        report = classification_report( y_total, y_total_preds )\n",
    "        cm=confusion_matrix(y_total, y_total_preds)\n",
    "        plt=plot_confusion_matrix(cm,normalize= True,target_names = ['HATE','PRFN','OFFN'],title = \"Confusion Matrix\")\n",
    "        plt.savefig('ger_task2_'+model_type+'_'+img_name)\n",
    "        print(classifier)\n",
    "        print(report)\n",
    "        print(accuracy_score(y_total, y_total_preds))\n",
    "        df_result=pandas_classification_report(y_total,y_total_preds)\n",
    "        df_result.to_csv('ger_task2_'+model_type+'_'+report_name,  sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:47:03.285581Z",
     "start_time": "2019-08-22T06:47:03.268288Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(scale,m_type=None):\n",
    "    if not m_type:\n",
    "        print(\"ERROR: Please specify a model type!\")\n",
    "        return None\n",
    "    if m_type == 'decision_tree_classifier':\n",
    "        logreg = tree.DecisionTreeClassifier(max_features=1000,max_depth=3,class_weight='balanced')\n",
    "    elif m_type == 'gaussian':\n",
    "        logreg = GaussianNB()\n",
    "    elif m_type == 'logistic_regression':\n",
    "        logreg = LogisticRegression(n_jobs=10, random_state=42,class_weight='balanced',solver='liblinear')\n",
    "    elif m_type == 'MLPClassifier':\n",
    "#         logreg = neural_network.MLPClassifier((500))\n",
    "        logreg = neural_network.MLPClassifier((100),random_state=42,early_stopping=True)\n",
    "    elif m_type == 'RandomForestClassifier':\n",
    "        logreg = ensemble.RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=12, max_depth=7)\n",
    "    elif m_type == 'SVC':\n",
    "        #logreg = LinearSVC(dual=False,max_iter=200)\n",
    "        logreg = SVC(kernel='linear',random_state=1526)\n",
    "    elif m_type == 'Catboost':\n",
    "        logreg = CatBoostClassifier(iterations=100,learning_rate=0.2,\n",
    "            l2_leaf_reg=500,depth=10,use_best_model=False, random_state=42,loss_function='MultiClass')\n",
    "#         logreg = CatBoostClassifier(scale_pos_weight=0.8, random_seed=42,);\n",
    "    elif m_type == 'XGB_classifier':\n",
    "#         logreg=XGBClassifier(silent=False,eta=0.1,objective='binary:logistic',max_depth=5,min_child_weight=0,gamma=0.2,subsample=0.8, colsample_bytree = 0.8,scale_pos_weight=1,n_estimators=500,reg_lambda=3,nthread=12)\n",
    "        logreg=XGBClassifier(silent=False,objective='multi:softmax',num_class=3,\n",
    "                             reg_lambda=3,nthread=12, random_state=42)\n",
    "    elif m_type == 'light_gbm':\n",
    "        logreg = LGBMClassifier(objective='multiclass',max_depth=3,learning_rate=0.2,num_leaves=20,scale_pos_weight=scale,\n",
    "                                boosting_type='gbdt', metric='multi_logloss',random_state=5,reg_lambda=20,silent=False)\n",
    "    else:\n",
    "        print(\"give correct model\")\n",
    "    print(logreg)\n",
    "    return logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:47:15.209542Z",
     "start_time": "2019-08-22T06:47:15.205163Z"
    }
   },
   "outputs": [],
   "source": [
    "models_name=['decision_tree_classifier','gaussian','logistic_regression','MLPClassifier','RandomForestClassifier',\n",
    "             'SVC','light_gbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T09:11:18.384822Z",
     "start_time": "2019-08-07T09:10:39.082391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2906976744186047\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 0. 0. 0. 2. 2. 0. 2. 0. 2. 2. 2. 1. 0. 2. 2. 2. 0. 2. 0. 1. 0. 0.\n",
      " 0. 2. 2. 0. 2. 0. 1. 1. 2. 2. 0. 1. 2. 2. 2. 1. 1. 0. 0. 1. 1. 0. 2. 1.\n",
      " 2. 0. 2. 0. 2. 2. 2. 0. 2. 0. 1. 2. 2. 2. 2. 0. 1. 2. 2. 0. 0. 0. 0. 2.\n",
      " 0. 0. 2. 1. 2. 0. 2. 0. 2. 1. 0. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 0. 2. 1. 0. 2. 2. 1. 2. 0. 2. 2. 2. 1.\n",
      " 2. 1. 1. 2. 1. 1. 2. 2. 0. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1. 2. 1. 1. 1.\n",
      " 1. 2. 1. 1. 2. 1. 2. 2. 1. 1. 1. 2. 0. 2. 2. 1. 2. 1. 1. 2. 2. 1. 2. 0.\n",
      " 0. 0. 0. 2. 0. 0. 1. 0. 0. 2. 2. 2. 2. 1. 2. 2. 0. 0. 0. 2. 2. 2. 0. 0.\n",
      " 0. 0. 0. 0. 2. 1. 2. 2. 2. 2. 0. 2. 2. 0. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 0. 0. 2. 2. 0. 2. 2. 2. 0. 0. 0. 2. 0. 0. 0. 1. 2. 2. 0. 0. 0.\n",
      " 0. 2. 2. 0. 0. 2. 2. 2. 1. 2. 2. 1. 0. 0. 2. 0. 0. 2. 0. 0. 2. 2. 2. 1.\n",
      " 2. 0. 2. 2. 0. 2. 2. 2. 0. 0. 2. 0. 2. 1. 1. 2. 1. 2. 0. 0. 2. 2. 2. 0.\n",
      " 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 2. 2. 0. 2. 0. 2. 2. 2. 2. 2. 0. 0. 2.\n",
      " 2. 1. 1. 0. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 1. 2. 2. 2. 2.\n",
      " 0. 2. 2. 0. 2. 0. 2. 1. 2. 0. 1. 2. 2. 2. 0. 2. 0. 0. 0. 0. 2. 1. 0. 1.\n",
      " 0. 2. 2. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1. 1. 2. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 0. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 1. 2. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 1.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.6054794520547945 accuracy_test: 0.35714285714285715\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.60      0.52        99\n",
      "         1.0       0.82      0.64      0.72        77\n",
      "         2.0       0.64      0.60      0.62       189\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       365\n",
      "   macro avg       0.64      0.61      0.62       365\n",
      "weighted avg       0.63      0.61      0.61       365\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.42      0.33        12\n",
      "         1.0       0.33      0.22      0.27         9\n",
      "         2.0       0.44      0.38      0.41        21\n",
      "\n",
      "   micro avg       0.36      0.36      0.36        42\n",
      "   macro avg       0.35      0.34      0.34        42\n",
      "weighted avg       0.37      0.36      0.36        42\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "<class 'numpy.ndarray'>\n",
      "[0. 0. 2. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 0. 2. 0. 0. 0. 1. 0. 0. 0. 0. 2. 1. 2. 0. 2. 1. 1. 2. 2. 1. 0.\n",
      " 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 0. 1. 0. 1. 0. 0. 2. 2. 0. 2. 2. 0. 0. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 0. 1. 1. 2. 0. 2. 1. 2. 1.\n",
      " 0. 2. 0. 2. 0. 2. 0. 2. 0. 2. 0. 0. 0. 0. 0. 2. 1. 1. 1. 0. 2. 1. 0. 1.\n",
      " 1. 0. 0. 1. 1. 2. 2. 0. 1. 0. 0. 0. 0. 2. 2. 0. 2. 2. 0. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 2. 2. 2. 0. 0. 0. 0. 2. 0. 0. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 2. 2. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 0. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 2. 0. 0. 2. 1. 1. 0. 0. 2. 0. 0. 0. 0. 2. 2. 2. 2. 0. 1. 1. 0. 1. 0. 1.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 1. 2. 0. 0. 2. 0. 0. 2. 2. 0. 2. 0. 2. 0. 2. 2.\n",
      " 0. 0. 0. 1. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0. 2. 0. 0. 0. 1. 0. 2. 2.\n",
      " 2. 2. 0. 1. 0. 2. 0. 2. 0. 2. 0. 0. 2. 0. 0. 2. 2. 0. 1. 1. 2. 1. 1. 1.\n",
      " 0. 2. 0. 2. 2. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 0. 0. 0. 0. 1. 2.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 1. 2. 2. 1. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 1. 1.\n",
      " 2. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.587431693989071 accuracy_test: 0.3902439024390244\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.81      0.54       100\n",
      "         1.0       0.91      0.53      0.67        77\n",
      "         2.0       0.76      0.49      0.60       189\n",
      "\n",
      "   micro avg       0.59      0.59      0.59       366\n",
      "   macro avg       0.69      0.61      0.60       366\n",
      "weighted avg       0.70      0.59      0.60       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.22      0.36      0.28        11\n",
      "         1.0       0.57      0.44      0.50         9\n",
      "         2.0       0.50      0.38      0.43        21\n",
      "\n",
      "   micro avg       0.39      0.39      0.39        41\n",
      "   macro avg       0.43      0.40      0.40        41\n",
      "weighted avg       0.44      0.39      0.41        41\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 2. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 2. 1. 0. 1. 1. 0. 2. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 2. 1. 1. 0. 2. 0. 0. 0. 1. 1. 1. 0. 0. 2. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 2. 2. 0. 2. 2. 0. 1. 1. 2.\n",
      " 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 2.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 1. 0. 2. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 0. 0. 0. 0.\n",
      " 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 0. 1. 1. 1. 2. 0. 0. 1. 1. 2. 1. 2. 0. 2.\n",
      " 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.4562841530054645 accuracy_test: 0.3170731707317073\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.35      0.89      0.51       100\n",
      "         1.0       0.63      0.78      0.70        77\n",
      "         2.0       0.90      0.10      0.17       189\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       366\n",
      "   macro avg       0.63      0.59      0.46       366\n",
      "weighted avg       0.69      0.46      0.37       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.82      0.42        11\n",
      "         1.0       0.20      0.11      0.14         9\n",
      "         2.0       0.75      0.14      0.24        21\n",
      "\n",
      "   micro avg       0.32      0.32      0.32        41\n",
      "   macro avg       0.41      0.36      0.27        41\n",
      "weighted avg       0.50      0.32      0.27        41\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2. 2.\n",
      " 0. 0. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 2.\n",
      " 0. 1. 2. 2. 0. 2. 2. 2. 2. 1. 0. 2. 2. 2. 2. 2. 0. 2. 2. 1. 0. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 0. 2. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 0. 2. 1. 2. 1. 2. 1.\n",
      " 2. 0. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 1. 0. 1. 1. 1. 2. 1.\n",
      " 1. 0. 1. 1. 2. 2. 1. 0. 1. 2. 1. 0. 2. 1. 1. 2. 0. 2. 2. 1. 2. 1. 1. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 0. 2. 2.\n",
      " 0. 0. 2. 2. 0. 2. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0.\n",
      " 2. 0. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2. 2. 1.\n",
      " 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 0. 0. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 0. 2. 1. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 2. 2. 2. 0. 0. 2. 1. 2. 2. 1. 2. 2. 2.\n",
      " 2. 2. 1. 1. 1. 1. 2. 2. 2. 0. 2. 2. 2. 1. 0. 2. 2. 0. 1. 2. 1. 2. 2. 0.\n",
      " 0. 0. 2. 0. 1. 0. 2. 2. 2. 1. 1. 2. 2. 2. 2. 0. 2. 1. 1. 1. 2. 2. 1. 1.\n",
      " 0. 0. 0. 2. 1. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2.\n",
      " 1. 1. 0. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0.\n",
      " 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 1. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 0. 1. 1. 1. 2. 0. 2. 1. 2. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.639344262295082 accuracy_test: 0.4146341463414634\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.26      0.33       100\n",
      "         1.0       0.72      0.68      0.70        77\n",
      "         2.0       0.66      0.83      0.73       189\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       366\n",
      "   macro avg       0.61      0.59      0.59       366\n",
      "weighted avg       0.62      0.64      0.62       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.17      0.09      0.12        11\n",
      "         1.0       0.43      0.33      0.38         9\n",
      "         2.0       0.46      0.62      0.53        21\n",
      "\n",
      "   micro avg       0.41      0.41      0.41        41\n",
      "   macro avg       0.35      0.35      0.34        41\n",
      "weighted avg       0.38      0.41      0.39        41\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 0. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 1. 2. 0.\n",
      " 1. 2. 2. 2. 2. 1. 1. 2. 2. 0. 0. 2. 2. 1. 0. 2. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 1. 0. 2. 1. 0. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 2. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 2. 2. 0. 2.\n",
      " 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 2. 0. 2. 2. 0.\n",
      " 2. 2. 2. 2. 1. 0. 2. 2. 1. 0. 1. 2. 2. 1. 2. 2. 2. 0. 2. 2. 0. 2. 2. 0.\n",
      " 0. 0. 0. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 0. 0. 2. 1. 2. 0. 0. 0.\n",
      " 0. 0. 2. 0. 2. 1. 2. 2. 2. 2. 2. 2. 1. 0. 0. 0. 2. 0. 2. 1. 2. 2. 2. 0.\n",
      " 2. 2. 0. 2. 0. 0. 0. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 2. 2. 2. 0. 2. 2. 1.\n",
      " 2. 2. 2. 1. 2. 2. 2. 0. 0. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 0. 0. 1. 2. 2. 1. 2. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 2. 1. 2. 1. 2. 2. 1. 0. 0. 0. 1. 1. 2. 2. 2. 2. 0.\n",
      " 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 2. 0. 0. 2. 2. 2. 2. 0. 1. 1. 1. 2. 1. 1.\n",
      " 0. 2. 0. 2. 0. 2.]\n",
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 0. 0. 0. 0. 1. 1. 2. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 0. 2. 1. 1. 2. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 1. 1. 1.\n",
      " 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 1. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1.\n",
      " 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 2. 0. 1. 1. 2. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.6584699453551912 accuracy_test: 0.43902439024390244\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.45      0.52       100\n",
      "         1.0       0.65      0.75      0.70        77\n",
      "         2.0       0.68      0.73      0.70       189\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       366\n",
      "   macro avg       0.65      0.64      0.64       366\n",
      "weighted avg       0.65      0.66      0.65       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.27      0.30        11\n",
      "         1.0       0.29      0.44      0.35         9\n",
      "         2.0       0.61      0.52      0.56        21\n",
      "\n",
      "   micro avg       0.44      0.44      0.44        41\n",
      "   macro avg       0.41      0.41      0.40        41\n",
      "weighted avg       0.47      0.44      0.45        41\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 2. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 1. 2. 2. 0. 2. 2. 0. 2. 1. 2. 0. 2. 2. 2. 1. 0. 0. 1. 2. 2. 0. 1. 1.\n",
      " 2. 1. 1. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 2. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 2. 2. 0. 0. 2. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 0. 1. 2. 1. 0. 2. 1. 1. 2.\n",
      " 1. 1. 2. 1. 1. 1. 2. 2. 0. 0. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 1.\n",
      " 1. 0. 2. 2. 1. 2. 1. 2. 1. 2. 2. 1. 0. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1.\n",
      " 0. 0. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 1. 1. 1. 0. 2. 2. 0. 0. 2. 0.\n",
      " 1. 2. 2. 2. 0. 2. 0. 2. 1. 2. 0. 2. 1. 2. 2. 1. 2. 2. 1. 0. 0. 0. 0. 2.\n",
      " 2. 2. 0. 0. 0. 0. 0. 2. 0. 2. 2. 2. 2. 2. 2. 0. 2. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 1. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 2. 2.\n",
      " 2. 2. 2. 1. 0. 2. 1. 2. 1. 2. 0. 0. 0. 2. 2. 0. 2. 1. 2. 2. 1. 1. 2. 2.\n",
      " 0. 1. 2. 0. 1. 1. 0. 1. 0. 1. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 1.\n",
      " 2. 2. 2. 0. 1. 1. 1. 1. 2. 2. 1. 0. 2. 1. 0. 0. 0. 1. 2. 2. 1. 1. 0. 0.\n",
      " 2. 1. 1. 2. 1. 0. 2. 0. 2. 2. 2. 0. 1. 2. 2. 2. 2. 2. 0. 2. 1. 1. 1. 1.\n",
      " 1. 2. 2. 0. 1. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1.\n",
      " 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 2. 2. 2. 2. 2. 0.\n",
      " 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1.\n",
      " 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 0. 1. 1. 2. 2. 1. 2. 2. 0. 1. 1. 0. 2. 0. 1. 1. 2. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.6475409836065574 accuracy_test: 0.3902439024390244\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.51      0.55       100\n",
      "         1.0       0.63      0.77      0.69        77\n",
      "         2.0       0.68      0.67      0.67       189\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       366\n",
      "   macro avg       0.64      0.65      0.64       366\n",
      "weighted avg       0.65      0.65      0.64       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.23      0.27      0.25        11\n",
      "         1.0       0.50      0.56      0.53         9\n",
      "         2.0       0.44      0.38      0.41        21\n",
      "\n",
      "   micro avg       0.39      0.39      0.39        41\n",
      "   macro avg       0.39      0.40      0.40        41\n",
      "weighted avg       0.40      0.39      0.39        41\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "<class 'numpy.ndarray'>\n",
      "[1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 2. 0. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 2. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 2. 2. 1. 1. 2. 2.\n",
      " 1. 1. 1. 2. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 0. 0. 0. 2. 2. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 2. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 2.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1.\n",
      " 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 0. 0. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 1.\n",
      " 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1.\n",
      " 2. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 1. 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 2. 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 0. 2. 1. 0. 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0.\n",
      " 0. 2. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 1. 1. 1. 2. 0. 2. 0. 1. 1.\n",
      " 2. 1. 1. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.46321525885558584 accuracy_test: 0.3\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.97      0.54       100\n",
      "         1.0       0.63      0.79      0.70        78\n",
      "         2.0       0.85      0.06      0.11       189\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       367\n",
      "   macro avg       0.62      0.61      0.45       367\n",
      "weighted avg       0.67      0.46      0.35       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.82      0.42        11\n",
      "         1.0       0.43      0.38      0.40         8\n",
      "         2.0       0.00      0.00      0.00        21\n",
      "\n",
      "   micro avg       0.30      0.30      0.30        40\n",
      "   macro avg       0.24      0.40      0.27        40\n",
      "weighted avg       0.16      0.30      0.20        40\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 2. 0. 1. 2. 0. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 1. 2. 0.\n",
      " 2. 2. 0. 2. 1. 2. 2. 2. 1. 1. 1. 2. 1. 2. 0. 1. 2. 2. 0. 1. 1. 0. 1. 2.\n",
      " 0. 0. 1. 2. 2. 2. 0. 0. 2. 2. 2. 0. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 0. 1. 1. 2. 0. 0. 0. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0.\n",
      " 2. 2. 0. 0. 0. 1. 2. 2. 0. 0. 0. 1. 0. 2. 2. 1. 0. 1. 0. 2. 2. 1. 1. 2.\n",
      " 0. 2. 2. 2. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 1. 2. 1. 2. 1. 2.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 0. 2. 1. 2. 2. 2. 2. 1. 1.\n",
      " 1. 2. 0. 2. 2. 0. 2. 0. 2. 0. 2. 2. 1. 0. 2. 2. 0. 1. 2. 1. 2. 2. 2. 0.\n",
      " 2. 1. 0. 1. 2. 0. 2. 0. 2. 0. 0. 2. 2. 2. 2. 1. 2. 0. 1. 0. 2. 2. 0. 0.\n",
      " 2. 2. 0. 2. 1. 0. 0. 1. 0. 0. 2. 2. 2. 0. 0. 2. 2. 2. 0. 0. 0. 0. 2. 0.\n",
      " 1. 2. 1. 0. 1. 1. 0. 0. 2. 2. 0. 0. 1. 0. 2. 0. 2. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 2. 0. 2. 2. 0. 1. 2. 1. 2. 0. 2. 1. 0. 1. 0. 2. 0. 1. 2. 0. 2. 2. 0.\n",
      " 1. 1. 1. 2. 1. 1. 2. 0. 2. 2. 2. 2. 2. 0. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 2. 2. 0. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 2. 0. 0. 2. 0. 1. 2.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 1. 2. 0. 1. 0. 2. 2. 2. 0. 0. 2. 0. 0.\n",
      " 1. 2. 2. 0. 1. 0. 2.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 1. 1. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 2.\n",
      " 1. 1. 0. 2. 0. 0. 1. 1. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.6267029972752044 accuracy_test: 0.425\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.56      0.54       100\n",
      "         1.0       0.64      0.72      0.67        78\n",
      "         2.0       0.69      0.62      0.66       189\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       367\n",
      "   macro avg       0.61      0.63      0.62       367\n",
      "weighted avg       0.63      0.63      0.63       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.31      0.36      0.33        11\n",
      "         1.0       0.42      0.62      0.50         8\n",
      "         2.0       0.53      0.38      0.44        21\n",
      "\n",
      "   micro avg       0.42      0.42      0.42        40\n",
      "   macro avg       0.42      0.46      0.43        40\n",
      "weighted avg       0.45      0.42      0.42        40\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 0. 0. 0. 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 2. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 2. 1. 0. 2. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 1. 0. 0. 2. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 2. 0.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 2. 0. 0. 0. 1. 1. 1. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 1. 0. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 2. 1. 2.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 2. 0. 2. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 2. 1. 2. 1. 2. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 2. 1. 0. 0. 1. 1. 1. 0. 1. 0. 2. 2. 0. 2. 0. 0. 0. 1. 1. 2. 1.\n",
      " 1. 2. 0. 2. 2. 0. 0. 1. 0. 0. 2. 2. 0. 0. 0. 0. 2. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 2. 0. 2. 1. 1. 0. 1. 0. 2. 1. 1. 0. 1. 0. 0. 0. 1. 0. 2. 0. 0. 1. 2.\n",
      " 1. 1. 1. 0. 0. 2. 1. 0. 2. 0. 0. 2. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1.\n",
      " 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 1. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1.\n",
      " 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 2. 0. 1. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2.\n",
      " 1. 1. 2. 0. 2. 0. 1. 0. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.5367847411444142 accuracy_test: 0.375\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.89      0.59       100\n",
      "         1.0       0.55      0.85      0.67        78\n",
      "         2.0       0.89      0.22      0.36       189\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       367\n",
      "   macro avg       0.63      0.65      0.54       367\n",
      "weighted avg       0.70      0.54      0.49       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.45      0.36        11\n",
      "         1.0       0.38      0.62      0.48         8\n",
      "         2.0       0.50      0.24      0.32        21\n",
      "\n",
      "   micro avg       0.38      0.38      0.38        40\n",
      "   macro avg       0.39      0.44      0.39        40\n",
      "weighted avg       0.42      0.38      0.36        40\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "<class 'numpy.ndarray'>\n",
      "[0. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 2. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 2. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 2. 0. 1. 0. 1.\n",
      " 0. 0. 1. 1. 0. 1. 1. 2. 1. 1. 0. 2. 0. 0. 1. 1. 1. 1. 2. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 2. 1. 0. 1. 0. 1. 1. 0. 0. 0. 2. 1. 1. 1. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 2. 0. 0. 0. 1. 2. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 2. 0. 0. 1. 2. 1. 0. 0. 2. 0.\n",
      " 2. 0. 0. 1. 1. 1. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 2. 1. 0. 1. 0. 0. 2. 2. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1.\n",
      " 0. 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1.\n",
      " 2. 1. 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 1.\n",
      " 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.46321525885558584 accuracy_test: 0.35\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.88      0.51       100\n",
      "         1.0       0.64      0.79      0.71        78\n",
      "         2.0       0.83      0.11      0.19       189\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       367\n",
      "   macro avg       0.61      0.59      0.47       367\n",
      "weighted avg       0.66      0.46      0.39       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.82      0.43        11\n",
      "         1.0       0.50      0.38      0.43         8\n",
      "         2.0       0.67      0.10      0.17        21\n",
      "\n",
      "   micro avg       0.35      0.35      0.35        40\n",
      "   macro avg       0.49      0.43      0.34        40\n",
      "weighted avg       0.53      0.35      0.29        40\n",
      "\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=3,\n",
      "            max_features=1000, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      0.47      0.35       111\n",
      "         1.0       0.40      0.41      0.40        86\n",
      "         2.0       0.50      0.31      0.39       210\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       407\n",
      "   macro avg       0.39      0.40      0.38       407\n",
      "weighted avg       0.42      0.38      0.38       407\n",
      "\n",
      "0.3759213759213759\n",
      "1.2906976744186047\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2. 2. 1. 2. 2. 1. 2. 0. 2. 1.\n",
      " 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 0.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 1. 0. 2. 1. 2. 0. 2. 1. 2. 2. 2. 2. 2. 2.\n",
      " 1. 0. 0. 2. 2. 2. 2. 2. 2. 0. 2. 1. 2. 0. 0. 0. 2. 1. 2. 1. 1. 2. 1. 1.\n",
      " 2. 1. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 2.\n",
      " 2. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2.\n",
      " 0. 0. 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 1. 0. 2. 1. 2. 1. 2. 0. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 0. 1. 1. 1. 2. 2. 2.\n",
      " 1. 2. 0. 1. 1. 1. 2. 1. 2. 0. 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 0. 1. 1. 0.\n",
      " 2. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1.\n",
      " 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 0. 0. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 2. 1. 2. 1. 1. 2. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2.\n",
      " 1. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2.\n",
      " 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0.\n",
      " 0. 0. 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 1. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 2.\n",
      " 0. 2. 0. 0. 1. 0. 2. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.8575342465753425 accuracy_test: 0.38095238095238093\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.88      0.90        99\n",
      "         1.0       0.74      0.78      0.76        77\n",
      "         2.0       0.88      0.88      0.88       189\n",
      "\n",
      "   micro avg       0.86      0.86      0.86       365\n",
      "   macro avg       0.84      0.85      0.84       365\n",
      "weighted avg       0.86      0.86      0.86       365\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.33      0.35        12\n",
      "         1.0       0.17      0.11      0.13         9\n",
      "         2.0       0.44      0.52      0.48        21\n",
      "\n",
      "   micro avg       0.38      0.38      0.38        42\n",
      "   macro avg       0.32      0.32      0.32        42\n",
      "weighted avg       0.36      0.38      0.37        42\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 0. 2. 2. 2. 2. 1. 1. 2. 1. 2. 0. 2. 1. 0. 2. 1. 1. 2. 2. 1.\n",
      " 2. 0. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 0. 2. 2. 0. 0. 2. 2. 2. 2.\n",
      " 0. 1. 2. 0. 1. 1. 0. 2. 2. 2. 2. 2. 0. 1. 0. 0. 2. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 0. 2. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 1. 2. 1. 2. 1. 1. 2.\n",
      " 2. 1. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 2. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 1. 0. 2. 0. 0. 0. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 2. 2. 2. 0. 1. 0. 1. 0. 0. 0. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 0.\n",
      " 2. 1. 2. 1. 0. 2. 2. 0. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 2. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 0. 1. 1. 1. 2. 1. 2. 2.\n",
      " 1. 2. 0. 1. 1. 2. 2. 0. 1. 2. 0. 0. 2. 2. 2. 0. 2. 2. 1. 2. 0. 1. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 1. 0. 2. 1. 1. 2. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2.\n",
      " 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 0. 0. 0. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2.\n",
      " 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 2. 0. 1. 0. 0. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.8442622950819673 accuracy_test: 0.5121951219512195\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.91      0.87       100\n",
      "         1.0       0.73      0.83      0.78        77\n",
      "         2.0       0.91      0.81      0.86       189\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       366\n",
      "   macro avg       0.82      0.85      0.84       366\n",
      "weighted avg       0.85      0.84      0.85       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.45      0.40        11\n",
      "         1.0       0.62      0.56      0.59         9\n",
      "         2.0       0.58      0.52      0.55        21\n",
      "\n",
      "   micro avg       0.51      0.51      0.51        41\n",
      "   macro avg       0.52      0.51      0.51        41\n",
      "weighted avg       0.53      0.51      0.52        41\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 0. 1. 1. 0.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 1. 2. 1. 0. 2. 1. 2. 0. 2. 1. 2. 2. 2. 2. 2. 0. 0. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 2. 2. 0. 0. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 1. 2. 1. 1. 2. 1. 1. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 0. 2. 1. 2. 1. 0. 2. 0. 0. 0.\n",
      " 2. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 0. 0. 2. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 1. 0. 2.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 0. 2. 1. 2. 0. 2. 2. 0. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 2. 2. 1.\n",
      " 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 0. 0. 1. 1. 1. 2. 2. 2. 2. 1. 2.\n",
      " 0. 1. 1. 2. 0. 2. 2. 0. 1. 2. 0. 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 0. 1. 2.\n",
      " 0. 2. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 2. 2. 1.\n",
      " 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2. 0. 2.\n",
      " 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.8469945355191257 accuracy_test: 0.6097560975609756\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.87      0.87       100\n",
      "         1.0       0.74      0.81      0.77        77\n",
      "         2.0       0.89      0.85      0.87       189\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       366\n",
      "   macro avg       0.83      0.84      0.84       366\n",
      "weighted avg       0.85      0.85      0.85       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.55      0.63        11\n",
      "         1.0       0.45      0.56      0.50         9\n",
      "         2.0       0.64      0.67      0.65        21\n",
      "\n",
      "   micro avg       0.61      0.61      0.61        41\n",
      "   macro avg       0.61      0.59      0.59        41\n",
      "weighted avg       0.63      0.61      0.61        41\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 2. 0. 2. 2. 0. 1. 2. 2. 1. 1. 2. 0. 2. 2. 1. 2. 0. 2. 1. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 2. 0. 1. 0. 2.\n",
      " 2. 0. 0. 2. 2. 0. 1. 2. 1. 0. 2. 1. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 2. 1. 2. 1. 1. 2. 1. 2.\n",
      " 1. 2. 1. 1. 2. 0. 1. 2. 1. 1. 2. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 0. 2. 1. 2. 2. 1. 2. 2.\n",
      " 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 2. 2. 2. 2. 2. 0. 0. 2. 2.\n",
      " 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 2. 1. 0. 2. 1. 2. 1. 2. 2. 0. 1. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 2. 0. 1. 1. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 1. 1. 1. 2.\n",
      " 2. 2. 2. 1. 0. 1. 1. 2. 0. 2. 2. 0. 1. 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 0.\n",
      " 1. 1. 2. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 2. 0. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 1. 0.\n",
      " 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 1. 1. 2.\n",
      " 1. 1. 2. 0. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.8278688524590164 accuracy_test: 0.5365853658536586\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.86      0.85       100\n",
      "         1.0       0.72      0.74      0.73        77\n",
      "         2.0       0.86      0.85      0.86       189\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       366\n",
      "   macro avg       0.81      0.82      0.81       366\n",
      "weighted avg       0.83      0.83      0.83       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.27      0.32        11\n",
      "         1.0       0.43      0.33      0.38         9\n",
      "         2.0       0.62      0.76      0.68        21\n",
      "\n",
      "   micro avg       0.54      0.54      0.54        41\n",
      "   macro avg       0.47      0.46      0.46        41\n",
      "weighted avg       0.51      0.54      0.52        41\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2.\n",
      " 2. 0. 2. 0. 1. 2. 2. 2. 1. 1. 2. 0. 1. 2. 1. 2. 0. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 0. 2. 2.\n",
      " 2. 2. 0. 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 0. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 1. 2. 2.\n",
      " 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 2. 2. 1. 1.\n",
      " 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 0. 2. 2. 2. 1. 0. 2. 2. 0. 0. 2. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 0. 0. 0. 2.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 2. 0. 1. 2. 0. 2. 2. 2. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2.\n",
      " 0. 2. 2. 2. 2. 0. 1. 1. 1. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 1. 1. 1. 2.\n",
      " 2. 2. 2. 0. 1. 2. 0. 2. 2. 2. 1. 2. 0. 2. 0. 2. 2. 2. 2. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1.\n",
      " 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2.\n",
      " 0. 2. 2. 2. 2. 0. 1. 1. 1. 2. 2. 1. 2. 2. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2.\n",
      " 1. 2. 2. 0. 0. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.8306010928961749 accuracy_test: 0.4146341463414634\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.86      0.86       100\n",
      "         1.0       0.75      0.74      0.75        77\n",
      "         2.0       0.85      0.85      0.85       189\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       366\n",
      "   macro avg       0.82      0.82      0.82       366\n",
      "weighted avg       0.83      0.83      0.83       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.14      0.09      0.11        11\n",
      "         1.0       0.50      0.33      0.40         9\n",
      "         2.0       0.46      0.62      0.53        21\n",
      "\n",
      "   micro avg       0.41      0.41      0.41        41\n",
      "   macro avg       0.37      0.35      0.35        41\n",
      "weighted avg       0.39      0.41      0.39        41\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 0. 1. 2. 2. 2. 2. 1. 1. 2. 0. 1. 2. 2. 1. 2. 0. 2. 1.\n",
      " 1. 0. 2. 1. 2. 2. 1. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 0. 1. 2. 1. 0. 2. 1. 2. 0. 2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 0. 2. 2. 2. 2. 1. 2. 1. 1. 2. 1. 2.\n",
      " 1. 2. 1. 1. 2. 0. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2. 1. 2.\n",
      " 2. 1. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 0. 2. 1. 2. 2. 1. 0. 2. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1. 0. 0. 0. 2. 2.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 2. 0. 2. 1. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 0. 2. 2. 0. 2. 0. 2. 1. 1. 2. 2. 2. 1.\n",
      " 2. 0. 1. 1. 2. 0. 2. 2. 2. 1. 2. 0. 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1.\n",
      " 1. 0. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0.\n",
      " 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 1. 1. 2. 1. 2.\n",
      " 1. 1. 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 1. 2.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 0. 1. 1. 2. 0. 2. 1. 1. 2. 1. 1. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.8497267759562842 accuracy_test: 0.4878048780487805\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.88      0.88       100\n",
      "         1.0       0.77      0.78      0.77        77\n",
      "         2.0       0.87      0.86      0.87       189\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       366\n",
      "   macro avg       0.84      0.84      0.84       366\n",
      "weighted avg       0.85      0.85      0.85       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.18      0.25        11\n",
      "         1.0       0.33      0.44      0.38         9\n",
      "         2.0       0.58      0.67      0.62        21\n",
      "\n",
      "   micro avg       0.49      0.49      0.49        41\n",
      "   macro avg       0.44      0.43      0.42        41\n",
      "weighted avg       0.48      0.49      0.47        41\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 1. 2. 2. 0.\n",
      " 2. 2. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 0. 2. 1. 1. 0. 2. 1.\n",
      " 1. 2. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 2. 1. 0. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 2. 2. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 0. 0. 0. 2. 2. 2. 1. 1. 1. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 2. 2. 1. 1. 1. 2. 1. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 0. 2. 0. 2. 1. 2. 1. 0. 2. 2. 0. 0. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 0. 0. 2. 2. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 0. 1. 2. 1. 0. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 0. 0. 1. 1. 2. 2. 2. 1.\n",
      " 2. 0. 1. 1. 2. 0. 2. 2. 2. 1. 0. 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 0. 1. 1.\n",
      " 2. 0. 2. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 1. 2. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 0. 2. 2. 2.\n",
      " 2. 0. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 2. 2. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 0. 0. 0. 1. 2. 2. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 1. 2. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.8474114441416893 accuracy_test: 0.6\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.88      0.88       100\n",
      "         1.0       0.75      0.78      0.77        78\n",
      "         2.0       0.88      0.86      0.87       189\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       367\n",
      "   macro avg       0.83      0.84      0.84       367\n",
      "weighted avg       0.85      0.85      0.85       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.36      0.47        11\n",
      "         1.0       0.50      0.50      0.50         8\n",
      "         2.0       0.62      0.76      0.68        21\n",
      "\n",
      "   micro avg       0.60      0.60      0.60        40\n",
      "   macro avg       0.59      0.54      0.55        40\n",
      "weighted avg       0.61      0.60      0.59        40\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 0. 2. 2. 2. 1. 2. 2. 2. 1. 1. 2. 0. 1. 2. 2. 1. 2. 0. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 1. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 0. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 2. 1. 0. 2. 1. 2. 0. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1.\n",
      " 2. 1. 2. 1. 2. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 2.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 0. 2. 1. 1. 2. 1. 0.\n",
      " 2. 0. 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 2. 2. 2. 2. 1. 0. 0. 1.\n",
      " 2. 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 1. 0. 2. 1. 2. 1. 2. 2. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2.\n",
      " 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2.\n",
      " 2. 0. 2. 0. 2. 2. 2. 0. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 0. 2. 1.\n",
      " 1. 1. 2. 2. 2. 2. 1. 2. 1. 1. 2. 0. 2. 2. 2. 2. 2. 0. 2. 0. 2. 1. 0. 1.\n",
      " 1. 2. 0. 2. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2.\n",
      " 1. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2.\n",
      " 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1.\n",
      " 0. 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2.\n",
      " 2. 0. 2. 0. 2. 2. 2. 0. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0.\n",
      " 1. 1. 2. 1. 1. 2. 0. 2. 0. 1. 1. 0. 2. 2. 0. 0. 2. 0. 2. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.8446866485013624 accuracy_test: 0.575\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.86      0.86       100\n",
      "         1.0       0.73      0.82      0.77        78\n",
      "         2.0       0.89      0.85      0.87       189\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       367\n",
      "   macro avg       0.83      0.84      0.83       367\n",
      "weighted avg       0.85      0.84      0.85       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.45      0.56        11\n",
      "         1.0       0.38      0.38      0.38         8\n",
      "         2.0       0.60      0.71      0.65        21\n",
      "\n",
      "   micro avg       0.57      0.57      0.57        40\n",
      "   macro avg       0.56      0.51      0.53        40\n",
      "weighted avg       0.59      0.57      0.57        40\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 0. 1. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 0. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 1. 1. 2. 2. 1. 2. 0. 2. 1. 2. 2. 2. 2. 2. 2. 1.\n",
      " 0. 0. 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 2. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 2.\n",
      " 1. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2.\n",
      " 1. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 0. 2. 1. 1. 2. 1. 0. 2. 2. 0.\n",
      " 0. 2. 0. 1. 0. 0. 0. 0. 0. 0. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 0. 2. 1. 2. 1. 0. 2. 2. 0. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 0. 1.\n",
      " 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 0. 1. 1. 1. 2. 2. 2. 2. 1. 2.\n",
      " 0. 1. 1. 2. 0. 2. 2. 0. 1. 2. 0. 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 0. 1. 1.\n",
      " 2. 0. 2. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 0. 0. 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2.\n",
      " 1. 2. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0.\n",
      " 0. 0. 0. 1. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 0. 1.\n",
      " 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2. 0. 2.\n",
      " 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.8501362397820164 accuracy_test: 0.6\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.89      0.89       100\n",
      "         1.0       0.73      0.77      0.75        78\n",
      "         2.0       0.88      0.86      0.87       189\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       367\n",
      "   macro avg       0.83      0.84      0.84       367\n",
      "weighted avg       0.85      0.85      0.85       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.27      0.37        11\n",
      "         1.0       0.50      0.62      0.56         8\n",
      "         2.0       0.64      0.76      0.70        21\n",
      "\n",
      "   micro avg       0.60      0.60      0.60        40\n",
      "   macro avg       0.58      0.55      0.54        40\n",
      "weighted avg       0.60      0.60      0.58        40\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2. 2.\n",
      " 0. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 0. 2. 1. 0. 2. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 1. 0. 2. 2. 0. 0. 2.\n",
      " 2. 2. 0. 1. 2. 1. 0. 2. 1. 2. 0. 2. 1. 2. 2. 2. 2. 1. 1. 2. 0. 2. 2. 2.\n",
      " 2. 2. 2. 1. 2. 2. 0. 0. 0. 2. 2. 2. 2. 1. 2. 1. 1. 2. 1. 2. 1. 2. 1. 1.\n",
      " 2. 0. 1. 1. 1. 2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1.\n",
      " 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 0. 2. 1. 2. 2. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2. 2. 2. 2. 1. 0. 1. 2. 0. 0. 2. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 1. 2. 1. 2. 1. 0. 2. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 2. 1. 1. 2. 2. 2.\n",
      " 2. 1. 2. 0. 1. 2. 0. 2. 2. 2. 1. 2. 0. 2. 0. 2. 2. 2. 0. 2. 2. 2. 0. 1.\n",
      " 1. 2. 0. 2. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 0. 2. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 1. 0. 0. 0. 1. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 1. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 0. 2. 0. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.8310626702997275 accuracy_test: 0.525\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.88      0.87       100\n",
      "         1.0       0.72      0.79      0.76        78\n",
      "         2.0       0.87      0.82      0.84       189\n",
      "\n",
      "   micro avg       0.83      0.83      0.83       367\n",
      "   macro avg       0.82      0.83      0.82       367\n",
      "weighted avg       0.83      0.83      0.83       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.27      0.32        11\n",
      "         1.0       0.56      0.62      0.59         8\n",
      "         2.0       0.57      0.62      0.59        21\n",
      "\n",
      "   micro avg       0.53      0.53      0.53        40\n",
      "   macro avg       0.50      0.51      0.50        40\n",
      "weighted avg       0.51      0.53      0.51        40\n",
      "\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.32      0.38       111\n",
      "         1.0       0.45      0.44      0.44        86\n",
      "         2.0       0.57      0.66      0.61       210\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       407\n",
      "   macro avg       0.49      0.48      0.48       407\n",
      "weighted avg       0.51      0.52      0.51       407\n",
      "\n",
      "0.5233415233415234\n",
      "1.2906976744186047\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 1. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1.\n",
      " 1. 2. 0. 2. 2. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 0. 2.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 0. 0. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 0. 0. 0. 0. 1. 2. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 1. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 1. 1. 1. 2. 0. 0. 1. 1. 2. 1. 2. 2.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 0. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 2. 2.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 0. 0. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 0. 0. 0. 0. 1. 2. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 1. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 1. 1. 1. 2. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 0. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.9780821917808219 accuracy_test: 0.47619047619047616\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.99      0.97        99\n",
      "         1.0       0.99      0.99      0.99        77\n",
      "         2.0       0.99      0.97      0.98       189\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       365\n",
      "   macro avg       0.98      0.98      0.98       365\n",
      "weighted avg       0.98      0.98      0.98       365\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.36      0.42      0.38        12\n",
      "         1.0       0.40      0.44      0.42         9\n",
      "         2.0       0.61      0.52      0.56        21\n",
      "\n",
      "   micro avg       0.48      0.48      0.48        42\n",
      "   macro avg       0.46      0.46      0.46        42\n",
      "weighted avg       0.49      0.48      0.48        42\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2.\n",
      " 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2.\n",
      " 0. 2. 2. 2. 0. 2. 2. 1. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 1. 2. 0. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1.\n",
      " 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 1. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2.\n",
      " 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 1. 2. 1. 0. 0. 0.\n",
      " 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 2. 1. 0. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2.\n",
      " 0. 1. 1. 2. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 1. 0. 1. 1. 2. 1. 2. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2.\n",
      " 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2.\n",
      " 0. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1.\n",
      " 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2.\n",
      " 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 1. 2. 1. 0. 0. 0.\n",
      " 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2.\n",
      " 0. 1. 1. 2. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.9672131147540983 accuracy_test: 0.6097560975609756\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.97       100\n",
      "         1.0       0.96      0.97      0.97        77\n",
      "         2.0       0.97      0.96      0.97       189\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       366\n",
      "   macro avg       0.97      0.97      0.97       366\n",
      "weighted avg       0.97      0.97      0.97       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.54      0.64      0.58        11\n",
      "         1.0       0.43      0.33      0.38         9\n",
      "         2.0       0.71      0.71      0.71        21\n",
      "\n",
      "   micro avg       0.61      0.61      0.61        41\n",
      "   macro avg       0.56      0.56      0.56        41\n",
      "weighted avg       0.60      0.61      0.60        41\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 0. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 0. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 1. 2. 0. 2. 2.\n",
      " 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1.\n",
      " 1. 0. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0.\n",
      " 0. 2. 0. 2. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 2. 0. 0. 0. 1. 2. 1. 2. 2. 0.\n",
      " 2. 0. 1. 1. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 0. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 0. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0.\n",
      " 0. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 2. 1. 1. 2. 0.\n",
      " 2. 0. 1. 1. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.9726775956284153 accuracy_test: 0.4878048780487805\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96       100\n",
      "         1.0       0.99      0.99      0.99        77\n",
      "         2.0       0.97      0.97      0.97       189\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       366\n",
      "   macro avg       0.97      0.97      0.97       366\n",
      "weighted avg       0.97      0.97      0.97       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.29      0.18      0.22        11\n",
      "         1.0       0.44      0.44      0.44         9\n",
      "         2.0       0.56      0.67      0.61        21\n",
      "\n",
      "   micro avg       0.49      0.49      0.49        41\n",
      "   macro avg       0.43      0.43      0.43        41\n",
      "weighted avg       0.46      0.49      0.47        41\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 1. 1.\n",
      " 0. 2. 0. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 0. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2.\n",
      " 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 1. 0.\n",
      " 0. 0. 1. 0. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 1. 0. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 0. 0. 2. 2. 2. 2. 0. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1. 2.\n",
      " 1. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 2. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2.\n",
      " 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 1. 0.\n",
      " 0. 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 0. 0. 2. 2. 2. 2. 0. 1. 1. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 0. 2. 1. 2.\n",
      " 1. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.9699453551912568 accuracy_test: 0.5121951219512195\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.97      0.96       100\n",
      "         1.0       0.99      0.99      0.99        77\n",
      "         2.0       0.98      0.96      0.97       189\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       366\n",
      "   macro avg       0.97      0.97      0.97       366\n",
      "weighted avg       0.97      0.97      0.97       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.45      0.42        11\n",
      "         1.0       0.67      0.44      0.53         9\n",
      "         2.0       0.55      0.57      0.56        21\n",
      "\n",
      "   micro avg       0.51      0.51      0.51        41\n",
      "   macro avg       0.53      0.49      0.50        41\n",
      "weighted avg       0.53      0.51      0.51        41\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2.\n",
      " 1. 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2. 2.\n",
      " 1. 1. 0. 0. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 1. 1.\n",
      " 2. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 0. 2.\n",
      " 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2.\n",
      " 2. 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2.\n",
      " 1. 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 0. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 0. 0. 1. 1. 2. 1. 2.\n",
      " 0. 2. 0. 0. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2.\n",
      " 1. 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2.\n",
      " 1. 1. 0. 0. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 1. 1.\n",
      " 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2.\n",
      " 2. 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2.\n",
      " 1. 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 2. 0. 1. 1. 2. 0. 0. 1. 1. 2. 1. 1.\n",
      " 0. 2. 0. 0. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.9754098360655737 accuracy_test: 0.43902439024390244\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.97       100\n",
      "         1.0       0.97      0.99      0.98        77\n",
      "         2.0       0.98      0.97      0.98       189\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       366\n",
      "   macro avg       0.97      0.98      0.97       366\n",
      "weighted avg       0.98      0.98      0.98       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.18      0.21        11\n",
      "         1.0       0.38      0.33      0.35         9\n",
      "         2.0       0.52      0.62      0.57        21\n",
      "\n",
      "   micro avg       0.44      0.44      0.44        41\n",
      "   macro avg       0.38      0.38      0.38        41\n",
      "weighted avg       0.42      0.44      0.42        41\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 1. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1. 0. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 0. 2. 1. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1.\n",
      " 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2.\n",
      " 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 1. 1. 1. 2. 0. 0. 0. 1. 1. 1. 2. 2. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 1. 0. 0. 2. 0. 2. 2. 2. 0. 2. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 0. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1.\n",
      " 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2. 1. 2.\n",
      " 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.9672131147540983 accuracy_test: 0.5853658536585366\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.96       100\n",
      "         1.0       0.97      0.97      0.97        77\n",
      "         2.0       0.97      0.97      0.97       189\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       366\n",
      "   macro avg       0.97      0.97      0.97       366\n",
      "weighted avg       0.97      0.97      0.97       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.45      0.42        11\n",
      "         1.0       0.67      0.44      0.53         9\n",
      "         2.0       0.68      0.71      0.70        21\n",
      "\n",
      "   micro avg       0.59      0.59      0.59        41\n",
      "   macro avg       0.58      0.54      0.55        41\n",
      "weighted avg       0.60      0.59      0.59        41\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 0. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 0. 2. 2. 1. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 2. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 2. 2. 1. 2. 1. 1. 1. 1. 1.\n",
      " 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0.\n",
      " 0. 0. 0. 1. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 2. 2. 1. 0. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 1. 2. 1. 2. 1. 2. 2. 2. 1. 1. 1. 2. 0. 1. 0. 1. 1. 2.\n",
      " 2. 2. 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 0. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 2. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1.\n",
      " 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0.\n",
      " 0. 0. 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 1. 2. 1. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2.\n",
      " 1. 2. 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.9727520435967303 accuracy_test: 0.375\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.97      0.97       100\n",
      "         1.0       0.97      0.97      0.97        78\n",
      "         2.0       0.97      0.97      0.97       189\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       367\n",
      "   macro avg       0.97      0.97      0.97       367\n",
      "weighted avg       0.97      0.97      0.97       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.18      0.19        11\n",
      "         1.0       0.30      0.38      0.33         8\n",
      "         2.0       0.50      0.48      0.49        21\n",
      "\n",
      "   micro avg       0.38      0.38      0.38        40\n",
      "   macro avg       0.33      0.34      0.34        40\n",
      "weighted avg       0.38      0.38      0.38        40\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2.\n",
      " 2. 2. 0. 2. 2. 2. 0. 2. 1. 0. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1.\n",
      " 1. 0. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 2. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 1. 2. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 0. 1. 2. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 2. 0. 1. 0. 1. 1.\n",
      " 2. 1. 2. 2. 0. 2. 0. 0. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2.\n",
      " 2. 2. 0. 2. 2. 2. 0. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 1. 2. 1. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 0. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1.\n",
      " 2. 1. 1. 2. 0. 2. 0. 0. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.9673024523160763 accuracy_test: 0.5\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.97      0.97       100\n",
      "         1.0       0.95      0.99      0.97        78\n",
      "         2.0       0.98      0.96      0.97       189\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       367\n",
      "   macro avg       0.96      0.97      0.97       367\n",
      "weighted avg       0.97      0.97      0.97       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.45      0.45        11\n",
      "         1.0       0.38      0.38      0.38         8\n",
      "         2.0       0.57      0.57      0.57        21\n",
      "\n",
      "   micro avg       0.50      0.50      0.50        40\n",
      "   macro avg       0.47      0.47      0.47        40\n",
      "weighted avg       0.50      0.50      0.50        40\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 1. 2. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 1. 2. 0. 1.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 1. 2. 0. 2. 0.\n",
      " 0. 0. 0. 0. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 1.\n",
      " 0. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 0.\n",
      " 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 2. 0. 0. 0. 1. 1. 2. 1. 2. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 1. 2. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 1. 2. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1.\n",
      " 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 0.\n",
      " 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.9645776566757494 accuracy_test: 0.675\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.96       100\n",
      "         1.0       0.96      0.99      0.97        78\n",
      "         2.0       0.97      0.96      0.97       189\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       367\n",
      "   macro avg       0.96      0.97      0.97       367\n",
      "weighted avg       0.96      0.96      0.96       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.36      0.50        11\n",
      "         1.0       0.56      0.62      0.59         8\n",
      "         2.0       0.69      0.86      0.77        21\n",
      "\n",
      "   micro avg       0.68      0.68      0.68        40\n",
      "   macro avg       0.68      0.62      0.62        40\n",
      "weighted avg       0.69      0.68      0.66        40\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 10.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 0. 1. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 1. 1. 2. 1. 1. 2. 2. 1. 2. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 1. 0. 0. 0. 0. 1.\n",
      " 2. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2.\n",
      " 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0.\n",
      " 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 2. 0. 1. 0. 1. 1. 2. 1. 2. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 2. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 1. 0. 0. 0. 0. 1.\n",
      " 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2.\n",
      " 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0.\n",
      " 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.9673024523160763 accuracy_test: 0.425\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.96      0.96       100\n",
      "         1.0       0.96      0.97      0.97        78\n",
      "         2.0       0.97      0.97      0.97       189\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       367\n",
      "   macro avg       0.97      0.97      0.97       367\n",
      "weighted avg       0.97      0.97      0.97       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.27      0.30        11\n",
      "         1.0       0.33      0.50      0.40         8\n",
      "         2.0       0.53      0.48      0.50        21\n",
      "\n",
      "   micro avg       0.42      0.42      0.42        40\n",
      "   macro avg       0.40      0.42      0.40        40\n",
      "weighted avg       0.43      0.42      0.42        40\n",
      "\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=10, penalty='l2', random_state=42,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.36      0.37       111\n",
      "         1.0       0.44      0.43      0.43        86\n",
      "         2.0       0.59      0.62      0.61       210\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       407\n",
      "   macro avg       0.47      0.47      0.47       407\n",
      "weighted avg       0.50      0.51      0.51       407\n",
      "\n",
      "0.5085995085995086\n",
      "1.2906976744186047\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 1. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 1. 1. 0. 2. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 0. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.5178082191780822 accuracy_test: 0.5\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        99\n",
      "         1.0       0.00      0.00      0.00        77\n",
      "         2.0       0.52      1.00      0.68       189\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       365\n",
      "   macro avg       0.17      0.33      0.23       365\n",
      "weighted avg       0.27      0.52      0.35       365\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        12\n",
      "         1.0       0.00      0.00      0.00         9\n",
      "         2.0       0.50      1.00      0.67        21\n",
      "\n",
      "   micro avg       0.50      0.50      0.50        42\n",
      "   macro avg       0.17      0.33      0.22        42\n",
      "weighted avg       0.25      0.50      0.33        42\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 2. 0. 2. 2. 2. 0. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 0. 2. 2.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 1. 1. 2. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 1. 1. 2. 2. 2. 1. 1. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 1. 2. 2. 0. 0. 0.\n",
      " 2. 0. 0. 1. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 1. 2.\n",
      " 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 0. 2. 2. 2. 2.\n",
      " 2. 0. 2. 1. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 0. 0. 2. 2. 0. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2. 1. 0. 2. 1.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 1.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 0. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.7377049180327869 accuracy_test: 0.5121951219512195\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.52      0.67       100\n",
      "         1.0       0.80      0.53      0.64        77\n",
      "         2.0       0.68      0.94      0.79       189\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       366\n",
      "   macro avg       0.81      0.66      0.70       366\n",
      "weighted avg       0.78      0.74      0.72       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.09      0.13        11\n",
      "         1.0       0.67      0.44      0.53         9\n",
      "         2.0       0.53      0.76      0.63        21\n",
      "\n",
      "   micro avg       0.51      0.51      0.51        41\n",
      "   macro avg       0.47      0.43      0.43        41\n",
      "weighted avg       0.47      0.51      0.47        41\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 2.\n",
      " 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 1. 1. 2. 1. 2. 1. 2.\n",
      " 1. 1. 2. 1. 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2.\n",
      " 1. 2. 1. 1. 2. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 1. 1. 2. 2. 0. 2. 2.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2. 2.\n",
      " 1. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 0. 2. 1. 2. 2.\n",
      " 0. 2. 0. 0. 2. 2. 0. 0. 0. 2. 2. 2. 2. 1. 2. 0. 0. 0. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 1. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2.\n",
      " 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0.\n",
      " 2. 2. 2. 2. 2. 2. 0. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2.\n",
      " 1. 1. 1. 1. 2. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0.\n",
      " 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.7213114754098361 accuracy_test: 0.5609756097560976\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.42      0.58       100\n",
      "         1.0       0.74      0.65      0.69        77\n",
      "         2.0       0.68      0.91      0.78       189\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       366\n",
      "   macro avg       0.78      0.66      0.68       366\n",
      "weighted avg       0.76      0.72      0.71       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       0.50      0.67      0.57         9\n",
      "         2.0       0.59      0.81      0.68        21\n",
      "\n",
      "   micro avg       0.56      0.56      0.56        41\n",
      "   macro avg       0.36      0.49      0.42        41\n",
      "weighted avg       0.41      0.56      0.47        41\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 0. 2. 0. 2. 0. 2. 2. 2.\n",
      " 0. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2.\n",
      " 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 2. 1. 2. 1. 1. 1. 2. 1. 2. 2. 1. 1. 2. 2. 2. 1. 1. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 1. 0. 2. 2. 0. 0. 2. 0. 0.\n",
      " 2. 0. 0. 2. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 0. 0. 0. 0. 2. 0. 0. 2. 2. 2. 2. 2. 0. 0. 1. 0. 0. 2. 1. 2. 0. 2.\n",
      " 0. 0. 2. 0. 0. 2. 0. 0. 2. 2. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 2. 0. 2.\n",
      " 2. 2. 2. 0. 2. 1. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 1. 2. 2. 2.\n",
      " 2. 0. 2. 0. 0. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 2. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2.\n",
      " 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0. 0. 0. 1. 2. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 2. 1. 2. 2. 1. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 2.\n",
      " 2. 2. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 0.7158469945355191 accuracy_test: 0.5365853658536586\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.53      0.63       100\n",
      "         1.0       0.80      0.52      0.63        77\n",
      "         2.0       0.68      0.89      0.77       189\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       366\n",
      "   macro avg       0.76      0.65      0.68       366\n",
      "weighted avg       0.73      0.72      0.70       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.36      0.50        11\n",
      "         1.0       0.33      0.22      0.27         9\n",
      "         2.0       0.53      0.76      0.63        21\n",
      "\n",
      "   micro avg       0.54      0.54      0.54        41\n",
      "   macro avg       0.56      0.45      0.46        41\n",
      "weighted avg       0.56      0.54      0.51        41\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0.\n",
      " 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 0. 1. 2. 2. 2. 2. 2. 1. 1. 0. 2. 2. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 0. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 1. 2. 2. 2.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 1.\n",
      " 1. 2. 1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 1. 0. 2. 2. 0. 2. 0. 0. 2. 0.\n",
      " 0. 0. 2. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 2. 2. 2. 2. 2. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 0. 0. 1. 0. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0.\n",
      " 2. 2. 2. 0. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 0. 0. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0. 2. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 1. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2. 2.\n",
      " 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 0. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 0. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.6475409836065574 accuracy_test: 0.4878048780487805\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.43      0.54       100\n",
      "         1.0       0.75      0.35      0.48        77\n",
      "         2.0       0.61      0.88      0.72       189\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       366\n",
      "   macro avg       0.70      0.55      0.58       366\n",
      "weighted avg       0.68      0.65      0.62       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.09      0.13        11\n",
      "         1.0       0.75      0.33      0.46         9\n",
      "         2.0       0.50      0.76      0.60        21\n",
      "\n",
      "   micro avg       0.49      0.49      0.49        41\n",
      "   macro avg       0.48      0.40      0.40        41\n",
      "weighted avg       0.47      0.49      0.44        41\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 0. 0. 1. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1.\n",
      " 0. 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 2. 0. 1. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 0. 2. 0. 2. 2. 1. 2. 1. 1. 2. 2.\n",
      " 1. 2. 1. 1. 2. 2. 1. 1. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2.\n",
      " 2. 2. 1. 1. 1. 1. 1. 0. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 0. 2. 1. 2. 2.\n",
      " 2. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 0. 0. 0. 2. 0. 0. 1. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0.\n",
      " 2. 0. 0. 2. 1. 2. 0. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 2. 2. 2. 2. 0.\n",
      " 0. 1. 0. 0. 2. 1. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 0. 1.\n",
      " 2. 2. 2. 2. 0. 2. 0. 2. 2. 0. 2. 2. 2. 1. 0. 2. 0. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1.\n",
      " 0. 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 0. 0.\n",
      " 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1.\n",
      " 2. 2. 1. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 2. 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2.\n",
      " 1. 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 1. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 0.\n",
      " 2. 0. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 2. 0. 1.\n",
      " 2. 1. 1. 2. 0. 2. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 0. 0. 1. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.8087431693989071 accuracy_test: 0.6829268292682927\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.81      0.80       100\n",
      "         1.0       0.85      0.65      0.74        77\n",
      "         2.0       0.80      0.87      0.84       189\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       366\n",
      "   macro avg       0.82      0.78      0.79       366\n",
      "weighted avg       0.81      0.81      0.81       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.55      0.60        11\n",
      "         1.0       0.71      0.56      0.63         9\n",
      "         2.0       0.68      0.81      0.74        21\n",
      "\n",
      "   micro avg       0.68      0.68      0.68        41\n",
      "   macro avg       0.69      0.64      0.65        41\n",
      "weighted avg       0.68      0.68      0.68        41\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 1. 1. 1. 2. 1. 2. 1. 2. 1. 1. 2. 2. 1.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 2. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 0. 2. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 1. 2. 0. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 0. 2. 2. 2. 0. 2. 2. 2. 0. 2. 0. 0. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 0. 0. 0. 0. 1. 2. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0.\n",
      " 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 1. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 0. 0. 0. 0. 1.\n",
      " 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0.\n",
      " 1. 1. 2. 1. 2. 2. 2. 1. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.6267029972752044 accuracy_test: 0.625\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.22      0.34       100\n",
      "         1.0       0.75      0.42      0.54        78\n",
      "         2.0       0.59      0.93      0.72       189\n",
      "\n",
      "   micro avg       0.63      0.63      0.63       367\n",
      "   macro avg       0.71      0.52      0.54       367\n",
      "weighted avg       0.68      0.63      0.58       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.36      0.47        11\n",
      "         1.0       0.50      0.25      0.33         8\n",
      "         2.0       0.63      0.90      0.75        21\n",
      "\n",
      "   micro avg       0.62      0.62      0.62        40\n",
      "   macro avg       0.60      0.51      0.52        40\n",
      "weighted avg       0.62      0.62      0.59        40\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 2. 0. 2. 0. 2. 0. 2. 2.\n",
      " 2. 0. 2. 0. 2. 2. 0. 2. 2. 0. 0. 0. 1. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1.\n",
      " 2. 2. 1. 2. 0. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 0. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 1. 1. 2.\n",
      " 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 2. 1. 0. 2. 1. 2. 2. 2. 1. 1. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 2. 0. 0. 0. 2. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 2. 2.\n",
      " 0. 2. 1. 2. 0. 0. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 2. 2. 2. 2. 0. 1. 0. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 2. 2. 2. 2. 1. 0. 0. 1. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 2. 1. 2. 2. 0.\n",
      " 2. 0. 0. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 2. 0. 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1.\n",
      " 0. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 2. 0. 2. 2. 0. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 1. 1. 2. 1. 2. 1. 2. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 0. 1. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.6811989100817438 accuracy_test: 0.6\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.49      0.59       100\n",
      "         1.0       0.75      0.53      0.62        78\n",
      "         2.0       0.65      0.85      0.74       189\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       367\n",
      "   macro avg       0.71      0.62      0.65       367\n",
      "weighted avg       0.69      0.68      0.67       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.27      0.40        11\n",
      "         1.0       0.67      0.25      0.36         8\n",
      "         2.0       0.58      0.90      0.70        21\n",
      "\n",
      "   micro avg       0.60      0.60      0.60        40\n",
      "   macro avg       0.66      0.48      0.49        40\n",
      "weighted avg       0.64      0.60      0.55        40\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1. 2.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 0. 1. 1. 1. 2. 2. 1. 2. 2. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.5149863760217984 accuracy_test: 0.525\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       100\n",
      "         1.0       0.00      0.00      0.00        78\n",
      "         2.0       0.51      1.00      0.68       189\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       367\n",
      "   macro avg       0.17      0.33      0.23       367\n",
      "weighted avg       0.27      0.51      0.35       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       0.00      0.00      0.00         8\n",
      "         2.0       0.53      1.00      0.69        21\n",
      "\n",
      "   micro avg       0.53      0.53      0.53        40\n",
      "   macro avg       0.18      0.33      0.23        40\n",
      "weighted avg       0.28      0.53      0.36        40\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2.\n",
      " 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0.\n",
      " 0. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2.\n",
      " 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 2. 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0.\n",
      " 2. 1. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 2. 0. 1. 2. 1. 2. 2. 2. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2.\n",
      " 1. 1. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 0.5776566757493188 accuracy_test: 0.575\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.14      0.23       100\n",
      "         1.0       0.78      0.23      0.36        78\n",
      "         2.0       0.56      0.95      0.70       189\n",
      "\n",
      "   micro avg       0.58      0.58      0.58       367\n",
      "   macro avg       0.68      0.44      0.43       367\n",
      "weighted avg       0.64      0.58      0.50       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.27      0.37        11\n",
      "         1.0       0.67      0.25      0.36         8\n",
      "         2.0       0.56      0.86      0.68        21\n",
      "\n",
      "   micro avg       0.57      0.57      0.57        40\n",
      "   macro avg       0.61      0.46      0.47        40\n",
      "weighted avg       0.59      0.57      0.53        40\n",
      "\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "       hidden_layer_sizes=100, learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=42, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      0.20      0.29       111\n",
      "         1.0       0.58      0.30      0.40        86\n",
      "         2.0       0.56      0.86      0.68       210\n",
      "\n",
      "   micro avg       0.56      0.56      0.56       407\n",
      "   macro avg       0.57      0.45      0.46       407\n",
      "weighted avg       0.56      0.56      0.51       407\n",
      "\n",
      "0.5601965601965602\n",
      "1.2906976744186047\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2.\n",
      " 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 2. 2. 2. 2.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 1. 0. 0. 0. 1.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1.\n",
      " 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 0. 2. 2. 0. 1. 0. 0. 0. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2.\n",
      " 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 2. 2. 2. 2.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 1. 0. 0. 0. 1.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1.\n",
      " 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 0. 2. 2. 0. 1. 0. 0. 0. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5476190476190477\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        99\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       365\n",
      "   macro avg       1.00      1.00      1.00       365\n",
      "weighted avg       1.00      1.00      1.00       365\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.08      0.14        12\n",
      "         1.0       0.67      0.22      0.33         9\n",
      "         2.0       0.54      0.95      0.69        21\n",
      "\n",
      "   micro avg       0.55      0.55      0.55        42\n",
      "   macro avg       0.57      0.42      0.39        42\n",
      "weighted avg       0.56      0.55      0.46        42\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 1.\n",
      " 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0.\n",
      " 1. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2.\n",
      " 2. 1. 1. 0. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2.\n",
      " 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1.\n",
      " 0. 0. 0. 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 2. 0. 1. 1. 1. 2. 2. 1. 2. 1. 1. 1. 0. 2. 0. 1. 1. 2. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 1.\n",
      " 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0.\n",
      " 1. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2.\n",
      " 2. 1. 1. 0. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2.\n",
      " 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1.\n",
      " 0. 0. 0. 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 2. 0. 1. 1. 1. 2. 2. 1. 2. 1. 1. 1. 0. 2. 0. 1. 1. 2. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5365853658536586\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.09      0.15        11\n",
      "         1.0       0.60      0.33      0.43         9\n",
      "         2.0       0.53      0.86      0.65        21\n",
      "\n",
      "   micro avg       0.54      0.54      0.54        41\n",
      "   macro avg       0.54      0.43      0.41        41\n",
      "weighted avg       0.54      0.54      0.47        41\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0. 1. 1. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 0. 2. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 0. 0. 0. 0. 1. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0. 1. 1. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 0. 2. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 0. 0. 0. 0. 1. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5853658536585366\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       1.00      0.44      0.62         9\n",
      "         2.0       0.56      0.95      0.70        21\n",
      "\n",
      "   micro avg       0.59      0.59      0.59        41\n",
      "   macro avg       0.52      0.47      0.44        41\n",
      "weighted avg       0.50      0.59      0.49        41\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0.\n",
      " 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 2. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0.\n",
      " 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 2. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5365853658536586\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       1.00      0.11      0.20         9\n",
      "         2.0       0.53      1.00      0.69        21\n",
      "\n",
      "   micro avg       0.54      0.54      0.54        41\n",
      "   macro avg       0.51      0.37      0.30        41\n",
      "weighted avg       0.49      0.54      0.40        41\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 1.\n",
      " 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 0. 0. 0. 1. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 0.\n",
      " 2. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 1.\n",
      " 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 0. 0. 0. 1. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 0.\n",
      " 2. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5853658536585366\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       1.00      0.33      0.50         9\n",
      "         2.0       0.55      1.00      0.71        21\n",
      "\n",
      "   micro avg       0.59      0.59      0.59        41\n",
      "   macro avg       0.52      0.44      0.40        41\n",
      "weighted avg       0.50      0.59      0.47        41\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 0. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 2. 1. 2. 2. 1. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1. 2.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 0. 2. 0. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 0. 1.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 2. 1. 2. 2. 1. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1. 2.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 0. 2. 0. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5365853658536586\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.09      0.14        11\n",
      "         1.0       1.00      0.22      0.36         9\n",
      "         2.0       0.53      0.90      0.67        21\n",
      "\n",
      "   micro avg       0.54      0.54      0.54        41\n",
      "   macro avg       0.62      0.41      0.39        41\n",
      "weighted avg       0.58      0.54      0.46        41\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 1. 1.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 2.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 1. 1. 1. 2. 2. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 0. 1. 2. 1. 2. 2. 2. 2. 2. 0. 1. 1. 1. 2. 0. 0. 1. 1. 2. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 1. 1.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 2.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 1. 1. 1. 2. 2. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 0. 1. 2. 1. 2. 2. 2. 2. 2. 0. 1. 1. 1. 2. 0. 0. 1. 1. 2. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.525\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       0.50      0.38      0.43         8\n",
      "         2.0       0.53      0.86      0.65        21\n",
      "\n",
      "   micro avg       0.53      0.53      0.53        40\n",
      "   macro avg       0.34      0.41      0.36        40\n",
      "weighted avg       0.38      0.53      0.43        40\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binnym/anaconda3/envs/HASOC/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1.\n",
      " 2. 1. 1. 2. 0. 2. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1.\n",
      " 2. 1. 1. 2. 0. 2. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.525\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.09      0.14        11\n",
      "         1.0       0.60      0.38      0.46         8\n",
      "         2.0       0.53      0.81      0.64        21\n",
      "\n",
      "   micro avg       0.53      0.53      0.53        40\n",
      "   macro avg       0.49      0.43      0.42        40\n",
      "weighted avg       0.49      0.53      0.47        40\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 0. 1. 2. 1. 2. 2. 1. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 2. 0. 0. 1. 1. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 0. 1. 2. 1. 2. 2. 1. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 2. 0. 0. 1. 1. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.6\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.18      0.29        11\n",
      "         1.0       0.60      0.38      0.46         8\n",
      "         2.0       0.59      0.90      0.72        21\n",
      "\n",
      "   micro avg       0.60      0.60      0.60        40\n",
      "   macro avg       0.62      0.49      0.49        40\n",
      "weighted avg       0.62      0.60      0.55        40\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 0. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 1. 0. 0. 0.\n",
      " 0. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 1. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 2.\n",
      " 2. 2. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1.\n",
      " 1. 2. 0. 0. 0. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 0. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 1. 0. 0. 0.\n",
      " 0. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 1. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 2.\n",
      " 2. 2. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1.\n",
      " 1. 2. 0. 0. 0. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.45\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        11\n",
      "         1.0       0.33      0.12      0.18         8\n",
      "         2.0       0.49      0.81      0.61        21\n",
      "\n",
      "   micro avg       0.45      0.45      0.45        40\n",
      "   macro avg       0.27      0.31      0.26        40\n",
      "weighted avg       0.32      0.45      0.36        40\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=7, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=100, n_jobs=12, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.05      0.09       111\n",
      "         1.0       0.68      0.29      0.41        86\n",
      "         2.0       0.54      0.90      0.67       210\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       407\n",
      "   macro avg       0.53      0.42      0.39       407\n",
      "weighted avg       0.52      0.54      0.46       407\n",
      "\n",
      "0.542997542997543\n",
      "1.2906976744186047\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 2. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1.\n",
      " 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0. 1. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 0. 1. 1. 2. 0. 2. 0. 1. 2. 1. 1. 2. 0.\n",
      " 2. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 2. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1.\n",
      " 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 0. 1. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 0. 1. 1. 2. 0. 2. 0. 1. 2. 1. 1. 2. 0.\n",
      " 2. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.47619047619047616\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        99\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       365\n",
      "   macro avg       1.00      1.00      1.00       365\n",
      "weighted avg       1.00      1.00      1.00       365\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.33      0.33        12\n",
      "         1.0       0.50      0.67      0.57         9\n",
      "         2.0       0.56      0.48      0.51        21\n",
      "\n",
      "   micro avg       0.48      0.48      0.48        42\n",
      "   macro avg       0.46      0.49      0.47        42\n",
      "weighted avg       0.48      0.48      0.47        42\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 2.\n",
      " 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 0. 1. 1. 1. 2. 0. 2. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 2.\n",
      " 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 0. 1. 1. 1. 2. 0. 2. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5365853658536586\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.45      0.48        11\n",
      "         1.0       0.38      0.33      0.35         9\n",
      "         2.0       0.61      0.67      0.64        21\n",
      "\n",
      "   micro avg       0.54      0.54      0.54        41\n",
      "   macro avg       0.49      0.48      0.49        41\n",
      "weighted avg       0.53      0.54      0.53        41\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1.\n",
      " 0. 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 0. 2. 0. 1. 1. 2. 1. 2.\n",
      " 0. 2. 0. 0. 1. 0. 2. 0. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1.\n",
      " 0. 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 0. 2. 0. 1. 1. 2. 1. 2.\n",
      " 0. 2. 0. 0. 1. 0. 2. 0. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.36585365853658536\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.21      0.27      0.24        11\n",
      "         1.0       0.33      0.11      0.17         9\n",
      "         2.0       0.46      0.52      0.49        21\n",
      "\n",
      "   micro avg       0.37      0.37      0.37        41\n",
      "   macro avg       0.34      0.30      0.30        41\n",
      "weighted avg       0.37      0.37      0.35        41\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 0. 2. 1.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2.\n",
      " 2. 2. 0. 2. 1. 2. 2. 1. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0.\n",
      " 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 0. 2. 1.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2.\n",
      " 2. 2. 0. 2. 1. 2. 2. 1. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0.\n",
      " 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.4878048780487805\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.38      0.27      0.32        11\n",
      "         1.0       0.29      0.22      0.25         9\n",
      "         2.0       0.58      0.71      0.64        21\n",
      "\n",
      "   micro avg       0.49      0.49      0.49        41\n",
      "   macro avg       0.41      0.40      0.40        41\n",
      "weighted avg       0.46      0.49      0.47        41\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2.\n",
      " 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1. 2. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2.\n",
      " 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2. 1.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2.\n",
      " 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1. 2. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2.\n",
      " 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5365853658536586\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.36      0.40        11\n",
      "         1.0       0.50      0.44      0.47         9\n",
      "         2.0       0.58      0.67      0.62        21\n",
      "\n",
      "   micro avg       0.54      0.54      0.54        41\n",
      "   macro avg       0.51      0.49      0.50        41\n",
      "weighted avg       0.53      0.54      0.53        41\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0.\n",
      " 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1.\n",
      " 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0.\n",
      " 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 2. 1. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 2. 0. 1. 2. 1.\n",
      " 1. 2. 0. 0. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0.\n",
      " 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0.\n",
      " 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1.\n",
      " 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0.\n",
      " 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 2. 1. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2.\n",
      " 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 2. 0. 1. 2. 1.\n",
      " 1. 2. 0. 0. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5121951219512195\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.45      0.38        11\n",
      "         1.0       0.50      0.33      0.40         9\n",
      "         2.0       0.65      0.62      0.63        21\n",
      "\n",
      "   micro avg       0.51      0.51      0.51        41\n",
      "   macro avg       0.49      0.47      0.47        41\n",
      "weighted avg       0.53      0.51      0.52        41\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 0. 1. 1. 2. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1.\n",
      " 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 0. 0. 1. 1. 2. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 0. 1. 1. 2. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1.\n",
      " 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.625\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.73      0.62        11\n",
      "         1.0       0.60      0.38      0.46         8\n",
      "         2.0       0.70      0.67      0.68        21\n",
      "\n",
      "   micro avg       0.62      0.62      0.62        40\n",
      "   macro avg       0.61      0.59      0.59        40\n",
      "weighted avg       0.63      0.62      0.62        40\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1.\n",
      " 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0.\n",
      " 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 1. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1.\n",
      " 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 1. 2. 2.\n",
      " 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0.\n",
      " 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 1. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.425\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.27      0.26        11\n",
      "         1.0       0.22      0.25      0.24         8\n",
      "         2.0       0.63      0.57      0.60        21\n",
      "\n",
      "   micro avg       0.42      0.42      0.42        40\n",
      "   macro avg       0.37      0.36      0.37        40\n",
      "weighted avg       0.44      0.42      0.43        40\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 1. 1.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 1. 2. 2. 2. 1. 0. 2. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1. 2.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 2. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 1. 1.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 1. 2. 2. 2. 1. 0. 2. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1. 2.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 2. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 1. 1. 2. 0.\n",
      " 2. 0. 0. 1. 1. 0. 2. 2. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.55\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.36      0.38        11\n",
      "         1.0       0.50      0.50      0.50         8\n",
      "         2.0       0.64      0.67      0.65        21\n",
      "\n",
      "   micro avg       0.55      0.55      0.55        40\n",
      "   macro avg       0.51      0.51      0.51        40\n",
      "weighted avg       0.54      0.55      0.55        40\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 1. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 0. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 0. 1. 1. 2.\n",
      " 1. 1. 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 1. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2.\n",
      " 0. 2. 2. 2. 0. 2. 2. 1. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 0. 1. 1. 2.\n",
      " 1. 1. 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.575\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.55      0.55        11\n",
      "         1.0       0.50      0.62      0.56         8\n",
      "         2.0       0.63      0.57      0.60        21\n",
      "\n",
      "   micro avg       0.57      0.57      0.57        40\n",
      "   macro avg       0.56      0.58      0.57        40\n",
      "weighted avg       0.58      0.57      0.58        40\n",
      "\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.39      0.41      0.40       111\n",
      "         1.0       0.43      0.38      0.41        86\n",
      "         2.0       0.60      0.61      0.61       210\n",
      "\n",
      "   micro avg       0.51      0.51      0.51       407\n",
      "   macro avg       0.47      0.47      0.47       407\n",
      "weighted avg       0.51      0.51      0.51       407\n",
      "\n",
      "0.5085995085995086\n",
      "1.2906976744186047\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1.\n",
      " 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 1. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 1. 1. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 1. 1. 1. 2. 0. 2. 0. 1. 2. 1. 1. 2. 0. 2.\n",
      " 0. 0. 1. 1. 0. 2. 2. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0.\n",
      " 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1.\n",
      " 1. 0. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 1. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 1. 1. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1. 2. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 0. 1. 1. 2. 1. 2. 2. 1. 2. 2. 1. 1. 1. 2. 0. 2. 0. 1. 2. 1. 1. 2. 0. 2.\n",
      " 0. 0. 1. 1. 0. 2. 2. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5714285714285714\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        99\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       365\n",
      "   macro avg       1.00      1.00      1.00       365\n",
      "weighted avg       1.00      1.00      1.00       365\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.17      0.25        12\n",
      "         1.0       0.67      0.44      0.53         9\n",
      "         2.0       0.56      0.86      0.68        21\n",
      "\n",
      "   micro avg       0.57      0.57      0.57        42\n",
      "   macro avg       0.58      0.49      0.49        42\n",
      "weighted avg       0.57      0.57      0.53        42\n",
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 2. 1. 0. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 0. 0. 0. 2. 0. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2.\n",
      " 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 0. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 2. 2. 2. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 2. 1. 0. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 0. 0. 0. 2. 0. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2.\n",
      " 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2.\n",
      " 2. 2. 2. 0. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 2. 2. 2. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.6097560975609756\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.45      0.48        11\n",
      "         1.0       0.83      0.56      0.67         9\n",
      "         2.0       0.60      0.71      0.65        21\n",
      "\n",
      "   micro avg       0.61      0.61      0.61        41\n",
      "   macro avg       0.64      0.57      0.60        41\n",
      "weighted avg       0.62      0.61      0.61        41\n",
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2.\n",
      " 0. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 2. 1. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2.\n",
      " 0. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1.\n",
      " 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0. 1.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 2. 1. 2. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 1. 1. 2. 1. 1. 2.\n",
      " 0. 2. 0. 0. 1. 1. 0. 2. 2. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5609756097560976\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.09      0.13        11\n",
      "         1.0       0.83      0.56      0.67         9\n",
      "         2.0       0.55      0.81      0.65        21\n",
      "\n",
      "   micro avg       0.56      0.56      0.56        41\n",
      "   macro avg       0.54      0.49      0.48        41\n",
      "weighted avg       0.53      0.56      0.52        41\n",
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2. 2. 1. 1. 0. 2.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 0. 0. 1. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 1. 2. 2. 1. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 0. 0. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 0. 1. 1.\n",
      " 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2. 2. 1. 1. 0. 2.\n",
      " 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 0. 0. 1. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 1. 2. 2. 1. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2.\n",
      " 2. 0. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 0. 0. 0. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.6097560975609756\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.27      0.35        11\n",
      "         1.0       0.62      0.56      0.59         9\n",
      "         2.0       0.63      0.81      0.71        21\n",
      "\n",
      "   micro avg       0.61      0.61      0.61        41\n",
      "   macro avg       0.58      0.55      0.55        41\n",
      "weighted avg       0.59      0.61      0.59        41\n",
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0.\n",
      " 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1.\n",
      " 2. 1. 1. 2. 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 0. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0.\n",
      " 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 2. 0.\n",
      " 2. 2. 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1.\n",
      " 2. 1. 1. 2. 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 0. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5121951219512195\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.18      0.24        11\n",
      "         1.0       1.00      0.22      0.36         9\n",
      "         2.0       0.52      0.81      0.63        21\n",
      "\n",
      "   micro avg       0.51      0.51      0.51        41\n",
      "   macro avg       0.62      0.40      0.41        41\n",
      "weighted avg       0.57      0.51      0.47        41\n",
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0.\n",
      " 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0.\n",
      " 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 2. 1. 1. 0. 2.\n",
      " 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 0. 1.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 0. 0.\n",
      " 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0.\n",
      " 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0.\n",
      " 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 2. 1. 1. 0. 2.\n",
      " 0. 0. 1. 1. 0. 2. 2. 0. 1. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.5365853658536586\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        77\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       366\n",
      "   macro avg       1.00      1.00      1.00       366\n",
      "weighted avg       1.00      1.00      1.00       366\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.18      0.24        11\n",
      "         1.0       0.67      0.44      0.53         9\n",
      "         2.0       0.55      0.76      0.64        21\n",
      "\n",
      "   micro avg       0.54      0.54      0.54        41\n",
      "   macro avg       0.52      0.46      0.47        41\n",
      "weighted avg       0.52      0.54      0.51        41\n",
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0.\n",
      " 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 2. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 0. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 1. 0. 0. 2. 2. 2.\n",
      " 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 1. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1.\n",
      " 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 0. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0. 0. 0.\n",
      " 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 2.\n",
      " 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 2. 2. 2. 2. 1. 2. 2. 0. 1. 1. 1. 2. 0. 2. 0. 1. 1. 1. 1.\n",
      " 2. 0. 2. 0. 0. 1. 1. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.4\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.27      0.30        11\n",
      "         1.0       0.00      0.00      0.00         8\n",
      "         2.0       0.50      0.62      0.55        21\n",
      "\n",
      "   micro avg       0.40      0.40      0.40        40\n",
      "   macro avg       0.28      0.30      0.28        40\n",
      "weighted avg       0.35      0.40      0.37        40\n",
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 1. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 0.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 1. 1. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1. 1. 0. 2. 1. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 0.\n",
      " 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 2. 1. 0. 0.\n",
      " 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 2. 0. 2. 2. 2. 0. 0.\n",
      " 2. 0. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 0. 1. 1. 2. 0. 2. 0. 1. 1. 2. 1. 1.\n",
      " 2. 0. 2. 0. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.55\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.27      0.35        11\n",
      "         1.0       0.50      0.25      0.33         8\n",
      "         2.0       0.57      0.81      0.67        21\n",
      "\n",
      "   micro avg       0.55      0.55      0.55        40\n",
      "   macro avg       0.52      0.44      0.45        40\n",
      "weighted avg       0.53      0.55      0.51        40\n",
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 0. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0.\n",
      " 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1.\n",
      " 2. 1. 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 1. 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 2. 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 0. 2. 1. 0. 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 0. 1. 1. 1. 2. 2. 2. 1. 2. 0. 1. 1. 2. 0. 0. 1. 1.\n",
      " 2. 1. 2. 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0. 1. 0. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 0. 2. 1. 1. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2. 2. 0.\n",
      " 2. 2. 2. 2. 0. 2. 2. 1. 2. 2. 1. 2. 0. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0.\n",
      " 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 2. 1. 2. 1. 1. 2. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1.\n",
      " 2. 1. 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 1. 1. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 2. 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 1. 0. 2. 1. 0. 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 0. 2. 2.\n",
      " 2. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 2. 0. 1. 1. 1. 2. 2. 2. 1. 2. 0. 1. 1. 2. 0. 0. 1. 1.\n",
      " 2. 1. 2. 0. 2. 0. 0. 1. 1. 0. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.475\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.09      0.13        11\n",
      "         1.0       0.40      0.25      0.31         8\n",
      "         2.0       0.52      0.76      0.62        21\n",
      "\n",
      "   micro avg       0.47      0.47      0.48        40\n",
      "   macro avg       0.39      0.37      0.35        40\n",
      "weighted avg       0.42      0.47      0.42        40\n",
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "<class 'numpy.ndarray'>\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1.\n",
      " 1. 0. 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 2. 2. 0. 1. 1. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 2. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "[2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 0. 0. 0. 0.\n",
      " 0. 1. 1. 2. 2. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 1.\n",
      " 1. 0. 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 1. 2. 2. 2.\n",
      " 2. 0. 2. 2. 2. 2. 0. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 0. 0. 2.\n",
      " 2. 2. 2. 2. 0. 2. 2. 2. 2. 0. 0. 0. 1. 2. 2. 1. 2. 1. 1. 2. 2. 2. 1. 2.\n",
      " 1. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 1. 1.\n",
      " 1. 1. 1. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1.\n",
      " 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 2. 2.\n",
      " 2. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2.\n",
      " 2. 2. 0. 1. 1. 2. 1. 2. 2. 2. 2. 2. 0. 1. 1. 0. 2. 0. 1. 1. 2. 1. 1. 2.\n",
      " 2. 0. 1. 1. 0. 2. 2. 0. 1. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 0. 0. 0. 0.]\n",
      "accuracy_train: 1.0 accuracy_test: 0.55\n",
      "TRAINING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       100\n",
      "         1.0       1.00      1.00      1.00        78\n",
      "         2.0       1.00      1.00      1.00       189\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       367\n",
      "   macro avg       1.00      1.00      1.00       367\n",
      "weighted avg       1.00      1.00      1.00       367\n",
      "\n",
      "TESTING:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.27      0.35        11\n",
      "         1.0       0.25      0.12      0.17         8\n",
      "         2.0       0.60      0.86      0.71        21\n",
      "\n",
      "   micro avg       0.55      0.55      0.55        40\n",
      "   macro avg       0.45      0.42      0.41        40\n",
      "weighted avg       0.50      0.55      0.50        40\n",
      "\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.41      0.23      0.29       111\n",
      "         1.0       0.58      0.35      0.43        86\n",
      "         2.0       0.56      0.78      0.65       210\n",
      "\n",
      "   micro avg       0.54      0.54      0.54       407\n",
      "   macro avg       0.51      0.45      0.46       407\n",
      "weighted avg       0.52      0.54      0.51       407\n",
      "\n",
      "0.538083538083538\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in models_name:\n",
    "    train_model_no_ext(Classifier_Train_X,Classifier_Train_Y,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T06:47:20.863029Z",
     "start_time": "2019-08-22T06:47:19.406527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2906976744186047\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "        importance_type='split', learning_rate=0.2, max_depth=3,\n",
      "        metric='multi_logloss', min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
      "        n_jobs=-1, num_leaves=20, objective='multiclass', random_state=5,\n",
      "        reg_alpha=0.0, reg_lambda=20, scale_pos_weight=1.2906976744186047,\n",
      "        silent=False, subsample=1.0, subsample_for_bin=200000,\n",
      "        subsample_freq=0)\n"
     ]
    }
   ],
   "source": [
    "train_model_no_ext(Classifier_Train_X,Classifier_Train_Y,models_name[-1],save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HASOC]",
   "language": "python",
   "name": "conda-env-HASOC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
