{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T02:27:27.151790Z",
     "start_time": "2019-07-15T02:27:24.835255Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d1c951d7af65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "import itertools\n",
    "from string import punctuation\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from collections import Counter\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn import neural_network\n",
    "from sklearn import linear_model\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "import lightgbm as lgbm\n",
    "import nltk\n",
    "from embeddings import GloveEmbedding, FastTextEmbedding, KazumaCharEmbedding, ConcatEmbedding\n",
    "\n",
    "# from multilabel_data_handler import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T02:28:31.961202Z",
     "start_time": "2019-07-15T02:28:30.953700Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T02:27:57.735357Z",
     "start_time": "2019-07-15T02:27:57.713429Z"
    }
   },
   "outputs": [],
   "source": [
    "# file used to write preserve the results of the classfier\n",
    "# confusion matrix and precision recall fscore matrix\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    \n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T02:28:56.335857Z",
     "start_time": "2019-07-15T02:28:56.322576Z"
    }
   },
   "outputs": [],
   "source": [
    "##saving the classification report\n",
    "def pandas_classification_report(y_true, y_pred):\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='macro'))\n",
    "    avg.append(accuracy_score(y_true, y_pred, normalize=True))\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support','accuracy']\n",
    "    list_all=list(metrics_summary)\n",
    "    list_all.append(cm.diagonal())\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list_all,\n",
    "        index=metrics_sum_index)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    avg[-2] = total\n",
    "\n",
    "    class_report_df['avg / total'] = avg\n",
    "\n",
    "    return class_report_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T02:29:06.765167Z",
     "start_time": "2019-07-15T02:28:57.503523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....start_cleaning.........\n",
      "<hashtag> britain exit <hashtag> rape refugee\n"
     ]
    }
   ],
   "source": [
    "from commen_preprocess import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T02:43:14.583705Z",
     "start_time": "2019-07-15T02:43:14.505627Z"
    }
   },
   "outputs": [],
   "source": [
    "eng_train_dataset = pd.read_csv('../Data/english_dataset/english_dataset_added_features.tsv', sep=',')\n",
    "#hindi_train_dataset = pd.read_csv('../Data/hindi_dataset/hindi_dataset.tsv', sep='\\t',header=None)\n",
    "german_train_dataset = pd.read_csv('../Data/german_dataset/german_dataset_added_features.tsv', sep=',')\n",
    "eng_train_dataset=eng_train_dataset.drop(['Unnamed: 0'], axis=1)\n",
    "german_train_dataset=german_train_dataset.drop(['Unnamed: 0'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T02:43:22.624988Z",
     "start_time": "2019-07-15T02:43:22.605934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>task_1</th>\n",
       "      <th>task_2</th>\n",
       "      <th>task_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasoc_en_1</td>\n",
       "      <td>#DhoniKeepsTheGlove | WATCH: Sports Minister K...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasoc_en_2</td>\n",
       "      <td>@politico No. We should remember very clearly ...</td>\n",
       "      <td>HOF</td>\n",
       "      <td>HATE</td>\n",
       "      <td>TIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasoc_en_3</td>\n",
       "      <td>@cricketworldcup Guess who would be the winner...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasoc_en_4</td>\n",
       "      <td>Corbyn is too politically intellectual for #Bo...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasoc_en_5</td>\n",
       "      <td>All the best to #TeamIndia for another swimmin...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_id                                               text task_1  \\\n",
       "0  hasoc_en_1  #DhoniKeepsTheGlove | WATCH: Sports Minister K...    NOT   \n",
       "1  hasoc_en_2  @politico No. We should remember very clearly ...    HOF   \n",
       "2  hasoc_en_3  @cricketworldcup Guess who would be the winner...    NOT   \n",
       "3  hasoc_en_4  Corbyn is too politically intellectual for #Bo...    NOT   \n",
       "4  hasoc_en_5  All the best to #TeamIndia for another swimmin...    NOT   \n",
       "\n",
       "  task_2 task_3  \n",
       "0   NONE   NONE  \n",
       "1   HATE    TIN  \n",
       "2   NONE   NONE  \n",
       "3   NONE   NONE  \n",
       "4   NONE   NONE  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_train_dataset = pd.read_csv('../AMI@EVALITA2018/en_training.tsv', sep='\\t')\n",
    "# eng_test_dataset = pd.read_csv('../AMI@EVALITA2018/en_testing.tsv', sep='\\t')\n",
    "eng_train_dataset = pd.read_csv('../sem_eval_data/train_en.tsv', sep='\\t')\n",
    "eng_dev_dataset = pd.read_csv('../sem_eval_data/dev_en.tsv', sep='\\t')\n",
    "eng_test_dataset = pd.read_csv('../sem_eval_data/test_en.tsv', sep='\\t')\n",
    "eng_dev_own_dataset = pd.read_csv('../sem_eval_data/101-200 test_en.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "# AMI_train_dataset =pd.read_csv('../sem_eval_data/AMI_train.tsv',sep='\\t')\n",
    "# AMI_train_dataset.head()\n",
    "# AMI_train_dataset=AMI_train_dataset.drop(['misogyny_category'],axis=1)\n",
    "# AMI_train_dataset.columns = ['id', 'text','HS','TR']\n",
    "# AMI_train_dataset['AG'] = 0\n",
    "# dict1 = {\"active\": 1, \"passive\": 0}\n",
    "# AMI_train_dataset=AMI_train_dataset.replace({\"TR\":dict1})\n",
    "# AMI_train_dataset.head()\n",
    "# frames = [eng_train_dataset,AMI_train_dataset]\n",
    "# eng_train_dataset = pd.concat(frames)\n",
    "\n",
    "eng_train_dataset = eng_train_dataset.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eng_dev_own_dataset=eng_dev_own_dataset[0:199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    124\n",
       "0.0     75\n",
       "Name: HS, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_dev_own_dataset['HS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "count10000\n",
      "count20000\n",
      "count30000\n",
      "count40000\n",
      "count50000\n",
      "count60000\n",
      "count70000\n",
      "count80000\n",
      "count90000\n",
      "count100000\n",
      "count110000\n",
      "count120000\n",
      "count130000\n",
      "count140000\n",
      "count150000\n",
      "count160000\n",
      "count170000\n",
      "count180000\n",
      "count190000\n",
      "count200000\n",
      "count210000\n",
      "count220000\n",
      "count230000\n",
      "count240000\n",
      "count250000\n",
      "count260000\n",
      "count270000\n",
      "count280000\n",
      "count290000\n",
      "count300000\n",
      "count310000\n",
      "count320000\n",
      "count330000\n",
      "count340000\n",
      "count350000\n",
      "count360000\n",
      "count370000\n",
      "count380000\n",
      "count390000\n",
      "count400000\n",
      "count410000\n",
      "count420000\n",
      "count430000\n",
      "count440000\n",
      "count450000\n",
      "count460000\n",
      "count470000\n",
      "count480000\n",
      "count490000\n",
      "count500000\n",
      "count510000\n",
      "count520000\n",
      "count530000\n",
      "count540000\n",
      "count550000\n",
      "count560000\n",
      "count570000\n",
      "count580000\n",
      "count590000\n",
      "count600000\n",
      "count610000\n",
      "count620000\n",
      "count630000\n",
      "count640000\n",
      "count650000\n",
      "count660000\n",
      "count670000\n",
      "count680000\n",
      "count690000\n",
      "count700000\n",
      "count710000\n",
      "count720000\n",
      "count730000\n",
      "count740000\n",
      "count750000\n",
      "count760000\n",
      "count770000\n",
      "count780000\n",
      "count790000\n",
      "count800000\n",
      "count810000\n",
      "count820000\n",
      "count830000\n",
      "count840000\n",
      "count850000\n",
      "count860000\n",
      "count870000\n",
      "count880000\n",
      "count890000\n",
      "count900000\n",
      "count910000\n",
      "count920000\n",
      "count930000\n",
      "count940000\n",
      "count950000\n",
      "count960000\n",
      "count970000\n",
      "count980000\n",
      "count990000\n",
      "count1000000\n",
      "count1010000\n",
      "count1020000\n",
      "count1030000\n",
      "count1040000\n",
      "count1050000\n",
      "count1060000\n",
      "count1070000\n",
      "count1080000\n",
      "count1090000\n",
      "count1100000\n",
      "count1110000\n",
      "count1120000\n",
      "count1130000\n",
      "count1140000\n",
      "count1150000\n",
      "count1160000\n",
      "count1170000\n",
      "count1180000\n",
      "count1190000\n",
      "Done. 1193515  words loaded!\n",
      "Loading Glove Model\n",
      "count10000\n",
      "count20000\n",
      "count30000\n",
      "count40000\n",
      "count50000\n",
      "count60000\n",
      "count70000\n",
      "count80000\n",
      "count90000\n",
      "count100000\n",
      "count110000\n",
      "count120000\n",
      "count130000\n",
      "count140000\n",
      "count150000\n",
      "count160000\n",
      "count170000\n",
      "count180000\n",
      "count190000\n",
      "count200000\n",
      "count210000\n",
      "count220000\n",
      "count230000\n",
      "count240000\n",
      "count250000\n",
      "count260000\n",
      "count270000\n",
      "count280000\n",
      "count290000\n",
      "count300000\n",
      "count310000\n",
      "count320000\n",
      "count330000\n",
      "count340000\n",
      "count350000\n",
      "count360000\n",
      "count370000\n",
      "count380000\n",
      "count390000\n",
      "count400000\n",
      "count410000\n",
      "count420000\n",
      "count430000\n",
      "count440000\n",
      "count450000\n",
      "count460000\n",
      "count470000\n",
      "count480000\n",
      "count490000\n",
      "count500000\n",
      "count510000\n",
      "count520000\n",
      "count530000\n",
      "count540000\n",
      "count550000\n",
      "count560000\n",
      "count570000\n",
      "count580000\n",
      "count590000\n",
      "count600000\n",
      "count610000\n",
      "count620000\n",
      "count630000\n",
      "count640000\n",
      "count650000\n",
      "count660000\n",
      "count670000\n",
      "count680000\n",
      "count690000\n",
      "count700000\n",
      "count710000\n",
      "count720000\n",
      "count730000\n",
      "count740000\n",
      "count750000\n",
      "count760000\n",
      "count770000\n",
      "count780000\n",
      "count790000\n",
      "count800000\n",
      "count810000\n",
      "count820000\n",
      "count830000\n",
      "count840000\n",
      "count850000\n",
      "count860000\n",
      "count870000\n",
      "count880000\n",
      "count890000\n",
      "count900000\n",
      "count910000\n",
      "count920000\n",
      "count930000\n",
      "count940000\n",
      "count950000\n",
      "count960000\n",
      "count970000\n",
      "count980000\n",
      "count990000\n",
      "count1000000\n",
      "count1010000\n",
      "count1020000\n",
      "count1030000\n",
      "count1040000\n",
      "count1050000\n",
      "count1060000\n",
      "count1070000\n",
      "count1080000\n",
      "count1090000\n",
      "count1100000\n",
      "count1110000\n",
      "count1120000\n",
      "count1130000\n",
      "count1140000\n",
      "count1150000\n",
      "count1160000\n",
      "count1170000\n",
      "count1180000\n",
      "count1190000\n",
      "count1200000\n",
      "count1210000\n",
      "count1220000\n",
      "count1230000\n",
      "count1240000\n",
      "count1250000\n",
      "count1260000\n",
      "count1270000\n",
      "count1280000\n",
      "count1290000\n",
      "count1300000\n",
      "count1310000\n",
      "count1320000\n",
      "count1330000\n",
      "count1340000\n",
      "count1350000\n",
      "count1360000\n",
      "count1370000\n",
      "count1380000\n",
      "count1390000\n",
      "count1400000\n",
      "count1410000\n",
      "count1420000\n",
      "count1430000\n",
      "count1440000\n",
      "count1450000\n",
      "count1460000\n",
      "count1470000\n",
      "count1480000\n",
      "count1490000\n",
      "count1500000\n",
      "count1510000\n",
      "count1520000\n",
      "count1530000\n",
      "count1540000\n",
      "count1550000\n",
      "count1560000\n",
      "count1570000\n",
      "count1580000\n",
      "count1590000\n",
      "count1600000\n",
      "count1610000\n",
      "count1620000\n",
      "count1630000\n",
      "count1640000\n",
      "count1650000\n",
      "count1660000\n",
      "count1670000\n",
      "count1680000\n",
      "count1690000\n",
      "count1700000\n",
      "count1710000\n",
      "count1720000\n",
      "count1730000\n",
      "count1740000\n",
      "count1750000\n",
      "count1760000\n",
      "count1770000\n",
      "count1780000\n",
      "count1790000\n",
      "count1800000\n",
      "count1810000\n",
      "count1820000\n",
      "count1830000\n",
      "count1840000\n",
      "count1850000\n",
      "count1860000\n",
      "count1870000\n",
      "count1880000\n",
      "count1890000\n",
      "count1900000\n",
      "count1910000\n",
      "count1920000\n",
      "count1930000\n",
      "count1940000\n",
      "count1950000\n",
      "count1960000\n",
      "count1970000\n",
      "count1980000\n",
      "count1990000\n",
      "count2000000\n",
      "count2010000\n",
      "count2020000\n",
      "count2030000\n",
      "count2040000\n",
      "count2050000\n",
      "count2060000\n",
      "count2070000\n",
      "count2080000\n",
      "count2090000\n",
      "count2100000\n",
      "count2110000\n",
      "count2120000\n",
      "count2130000\n",
      "count2140000\n",
      "count2150000\n",
      "count2160000\n",
      "count2170000\n",
      "count2180000\n",
      "count2190000\n",
      "Done. 2196016  words loaded!\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "GLOVE_MODEL_FILE1= \"../../LEAM-master/glove.twitter.27B/glove.twitter.27B.200d.txt\"\n",
    "GLOVE_MODEL_FILE2=\"../../embeddings/glove.840B.300d.txt\"\n",
    "GLOVE_MODEL_FILE3=\"../../embeddings/wiki.en.vec\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding='utf8')\n",
    "    model = {}\n",
    "    i=0\n",
    "    for line in f:\n",
    "        i=i+1\n",
    "        splitLine = line.split(' ')\n",
    "        word = splitLine[0]\n",
    "        embedding = np.asarray(splitLine[1:], dtype='float32')\n",
    "        model[word] = embedding\n",
    "        if(i%10000==0):\n",
    "            print(\"count\"+str(i))\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "\n",
    "# word2vec_model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "word2vec_model1 = loadGloveModel(GLOVE_MODEL_FILE1)\n",
    "word2vec_model2 = loadGloveModel(GLOVE_MODEL_FILE2)\n",
    "word2vec_model3 =  KeyedVectors.load_word2vec_format(GLOVE_MODEL_FILE3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_model4 = Word2Vec.load(\"../../embeddings/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### stopwords and punctuations are not removed but text is cleaned and stemmed\n",
    "def glove_tokenize_norem(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    words = text.split()\n",
    "    words =[ps.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "####stopwords and punctuations are removed along with that text is cleaned ans stemmed\n",
    "def glove_tokenize(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOPWORDS]\n",
    "    words =[ps.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "def pos_tokenize(text):\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    tags = nltk.pos_tag(text.split())\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    return tag_list\n",
    "\n",
    "\n",
    "def glove_tokenize_embed(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text,for_embeddings=True, remove_stopwords=False, remove_punctuations=False)\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in STOPWORDS]\n",
    "    return words\n",
    "\n",
    "def glove_tokenize_vocab(text):\n",
    "    #text = tokenizer(text)\n",
    "    text=clean(text, remove_stopwords=False, remove_punctuations=False)\n",
    "    words = text.split()\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN', 'VBZ', 'VBN', 'NN', 'VBP', 'JJ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"#hastag i am awesome\"\n",
    "pos_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_class_label(input_text):\n",
    "    if input_text==1:\n",
    "        return 'misogyny'\n",
    "    elif input_text==0:\n",
    "        return 'non-misogyny'\n",
    "    else:\n",
    "        print('Wrong Input', input_text)\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/ipykernel/__main__.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Loading Completed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/ipykernel/__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# pd_train = pd.DataFrame(columns=['id','misogynous','text'])\n",
    "eng_train_dataset[\"text\"].replace('', np.nan, inplace=True)\n",
    "eng_train_dataset.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "pd_train_binary = eng_train_dataset[['id','text','HS']]\n",
    "pd_dev_binary = eng_dev_dataset[['id','text','HS']]\n",
    "pd_dev_own_binary = eng_dev_own_dataset[['id','text','HS']]\n",
    "\n",
    "# pd_train_binary = eng_train_dataset[['id','misogynous','text','misogyny_category','target','male','female']]\n",
    "# pd_train_category = eng_train_dataset[['id','misogynous','text','misogyny_category']]\n",
    "# pd_train_target = eng_train_dataset[['id','misogynous','text','target']]\n",
    "pd_test = eng_test_dataset[['id','text']]\n",
    "\n",
    "# pd_train_category = pd_train_category.loc[pd_train_category['misogynous'] == 1]\n",
    "# pd_train_target = pd_train_target.loc[pd_train_target['misogynous'] == 1]\n",
    "# pd_train_target.drop(['misogynous'], axis=1)                                      \n",
    "# pd_train_category.drop(['misogynous'], axis=1)                                      \n",
    "\n",
    "# # pd_train['class'] =pd_train.apply(lambda row: convert_class_label(row['misogynous']), axis=1)\n",
    "\n",
    "pd_train_binary['class'] = pd_train_binary['HS']\n",
    "pd_dev_binary['class'] = pd_dev_binary['HS']\n",
    "pd_dev_own_binary['class'] = pd_dev_own_binary['HS']\n",
    "\n",
    "# pd_train_category['class'] = pd_train_category['misogyny_category']\n",
    "# pd_train_target['class'] = pd_train_target['target']\n",
    "\n",
    "# for count, each in enumerate(train_data):\n",
    "#     try:\n",
    "#         pd_train.loc[count]  = [each['id'], convert_class_label(each['CounterSpeech']), each['Community'],each['Category'],each['commentText']]\n",
    "#     except:\n",
    "#         pass\n",
    "print('Training Data Loading Completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31568</td>\n",
       "      <td>Bro is a bitch, fucking cunt https://t.co/dQ8J...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30734</td>\n",
       "      <td>I'm rewatching Breaking Bad &amp; I could see why ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31227</td>\n",
       "      <td>@charliekirk11 @asia742 364 down only about 12...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31224</td>\n",
       "      <td>&gt;Harasses women and calls them bitch and crabb...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31139</td>\n",
       "      <td>Bitch you ain't gotta call my phone, matter fa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34076</td>\n",
       "      <td>@ABCPolitics A illegal aliens showing exactly ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32479</td>\n",
       "      <td>Bitch....how the fuck. Siri you a dumb ass hoe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32985</td>\n",
       "      <td>#FactCheck true #TuesdayThoughts #WalkAwayFrom...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30284</td>\n",
       "      <td>@GMA @TVMarci His own fault #SENDTHEMBACK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30072</td>\n",
       "      <td>@christianllamar @LindseyGrahamSC stop w 'we h...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34098</td>\n",
       "      <td>@FoxNews @BarackObama Like he ever kept out an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>33818</td>\n",
       "      <td>Im going to explode if I have to listen to the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34138</td>\n",
       "      <td>He is a snake ass bitch. He is a fugly slut wh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>34165</td>\n",
       "      <td>You seem like a hoe    Ok bitch? Did I ever de...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>30974</td>\n",
       "      <td>@FoxNews @GOPLeader @MariaBartiromo This is a ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>33150</td>\n",
       "      <td>happy bday to my big boobie bitch🎉❤️ily and im...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>30574</td>\n",
       "      <td>You A Side Bitch You A Substitute 👩‍🏫 You Wifi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32162</td>\n",
       "      <td>Argentina imported tons of Mestizos and they w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32904</td>\n",
       "      <td>@BlackBreedMe You nasty little slutty whore yo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31484</td>\n",
       "      <td>Open your mouth &amp; take the meat like a hoe you...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31961</td>\n",
       "      <td>&amp; for the hoe that think she don't do what the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>31616</td>\n",
       "      <td>bitch whole pussy out wit some pro-keds on.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31027</td>\n",
       "      <td>Border Patrol Agents Arrest Group of 163 Migra...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>31231</td>\n",
       "      <td>@AnnCoulter Here is an encouraging word: I hea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>33327</td>\n",
       "      <td>@mozartfx @vargas0516 @Bugl3t @JamesLeonardArt...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>31276</td>\n",
       "      <td>Conservative Trump Patriots, it might be the w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>33238</td>\n",
       "      <td>is PROOF that theyre more privleged. also my b...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31004</td>\n",
       "      <td>Tell that bitch to pay me if she EVER tryna ra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>34187</td>\n",
       "      <td>I am a broke bitch that wants the bts puma sho...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>34421</td>\n",
       "      <td>the crazy part is, y'all be moving on soooo qu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>30420</td>\n",
       "      <td>@Sorcelll @FolkloreCunt @LivineGaming BITCH IM...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>32386</td>\n",
       "      <td>NEVER FORGET-it only took 19 radical hijackers...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>34426</td>\n",
       "      <td>@KyrieIrving Fuck your bitch ass. Soft ass muh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>30852</td>\n",
       "      <td>@PatriotessWings @NancyPelosi The Democrats do...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>30950</td>\n",
       "      <td>@MSF_Sea Tow the boat back to the safe ports i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>33043</td>\n",
       "      <td>@pastaelitist SAME SHIT. YOU SKINNY AF BITCH I...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>31223</td>\n",
       "      <td>#BUILDTHATWALL#MAGA|||||||||||||||||||||||||||...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>33055</td>\n",
       "      <td>@RealJamesWoods And because the federal govt u...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>32705</td>\n",
       "      <td>@Patriot_Mom_17 Yes We Need It YesterdayHouse ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>31176</td>\n",
       "      <td>Maybe he's late cause he can't get his huge fe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>30544</td>\n",
       "      <td>@sherry_kiskunas @DMRegister #IllegalAliens ar...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>31012</td>\n",
       "      <td>More than 65 million strong, and growing.#Depo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>30993</td>\n",
       "      <td>LIVE on #Periscope: Do the beat go off? Hell y...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>32742</td>\n",
       "      <td>bitch ass hoe you feel me</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>33278</td>\n",
       "      <td>@x__MMarie @notreallyno19 I can't stand you ho...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>33526</td>\n",
       "      <td>@kaitrosindia They're like omg how do you not ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>30001</td>\n",
       "      <td>@dbongino Dan my #vote was #changed 2x in #del...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>32444</td>\n",
       "      <td>#IllegalAliens. Or as the IRS refers to them #...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>34198</td>\n",
       "      <td>Definition of a whore/trash/piece of dirt/bitc...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>32844</td>\n",
       "      <td>It's still Fuck OJ. Bitch ass nigga. But good ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>31591</td>\n",
       "      <td>Not to sound like a corny ass bitch but like i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>33469</td>\n",
       "      <td>Islamic migrant urges Germans, Austrians to gi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>33763</td>\n",
       "      <td>Depression isn't just a bitch. It's a Cunt! St...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>30092</td>\n",
       "      <td>@Akarnious Part 1 #Sudanese get home loans wit...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>32701</td>\n",
       "      <td>CBS: Driver an Illegal Alien was drunk, high a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>34379</td>\n",
       "      <td>@zusanne222222 @JackLon09169906 @AupolNews #Se...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>31788</td>\n",
       "      <td>I'm not out here entertaining dudes bout shit....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>31331</td>\n",
       "      <td>im gonna be that bitch colorful does NOT equal...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>30353</td>\n",
       "      <td>Bitch they done free'd the OJ 😂😂😂 you hoe mad lol</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>30112</td>\n",
       "      <td>Whoever broke into my car and stole my weed th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text   HS  class\n",
       "0    31568  Bro is a bitch, fucking cunt https://t.co/dQ8J...  1.0    1.0\n",
       "1    30734  I'm rewatching Breaking Bad & I could see why ...  1.0    1.0\n",
       "2    31227  @charliekirk11 @asia742 364 down only about 12...  1.0    1.0\n",
       "3    31224  >Harasses women and calls them bitch and crabb...  1.0    1.0\n",
       "4    31139  Bitch you ain't gotta call my phone, matter fa...  1.0    1.0\n",
       "5    34076  @ABCPolitics A illegal aliens showing exactly ...  1.0    1.0\n",
       "6    32479  Bitch....how the fuck. Siri you a dumb ass hoe...  1.0    1.0\n",
       "7    32985  #FactCheck true #TuesdayThoughts #WalkAwayFrom...  1.0    1.0\n",
       "8    30284          @GMA @TVMarci His own fault #SENDTHEMBACK  1.0    1.0\n",
       "9    30072  @christianllamar @LindseyGrahamSC stop w 'we h...  0.0    0.0\n",
       "10   34098  @FoxNews @BarackObama Like he ever kept out an...  1.0    1.0\n",
       "11   33818  Im going to explode if I have to listen to the...  1.0    1.0\n",
       "12   34138  He is a snake ass bitch. He is a fugly slut wh...  1.0    1.0\n",
       "13   34165  You seem like a hoe    Ok bitch? Did I ever de...  1.0    1.0\n",
       "14   30974  @FoxNews @GOPLeader @MariaBartiromo This is a ...  0.0    0.0\n",
       "15   33150  happy bday to my big boobie bitch🎉❤️ily and im...  0.0    0.0\n",
       "16   30574  You A Side Bitch You A Substitute 👩‍🏫 You Wifi...  1.0    1.0\n",
       "17   32162  Argentina imported tons of Mestizos and they w...  0.0    0.0\n",
       "18   32904  @BlackBreedMe You nasty little slutty whore yo...  1.0    1.0\n",
       "19   31484  Open your mouth & take the meat like a hoe you...  1.0    1.0\n",
       "20   31961  & for the hoe that think she don't do what the...  1.0    1.0\n",
       "21   31616        bitch whole pussy out wit some pro-keds on.  1.0    1.0\n",
       "22   31027  Border Patrol Agents Arrest Group of 163 Migra...  0.0    0.0\n",
       "23   31231  @AnnCoulter Here is an encouraging word: I hea...  0.0    0.0\n",
       "24   33327  @mozartfx @vargas0516 @Bugl3t @JamesLeonardArt...  1.0    1.0\n",
       "25   31276  Conservative Trump Patriots, it might be the w...  0.0    0.0\n",
       "26   33238  is PROOF that theyre more privleged. also my b...  1.0    1.0\n",
       "27   31004  Tell that bitch to pay me if she EVER tryna ra...  1.0    1.0\n",
       "28   34187  I am a broke bitch that wants the bts puma sho...  0.0    0.0\n",
       "29   34421  the crazy part is, y'all be moving on soooo qu...  1.0    1.0\n",
       "..     ...                                                ...  ...    ...\n",
       "169  30420  @Sorcelll @FolkloreCunt @LivineGaming BITCH IM...  1.0    1.0\n",
       "170  32386  NEVER FORGET-it only took 19 radical hijackers...  0.0    0.0\n",
       "171  34426  @KyrieIrving Fuck your bitch ass. Soft ass muh...  1.0    1.0\n",
       "172  30852  @PatriotessWings @NancyPelosi The Democrats do...  0.0    0.0\n",
       "173  30950  @MSF_Sea Tow the boat back to the safe ports i...  1.0    1.0\n",
       "174  33043  @pastaelitist SAME SHIT. YOU SKINNY AF BITCH I...  1.0    1.0\n",
       "175  31223  #BUILDTHATWALL#MAGA|||||||||||||||||||||||||||...  0.0    0.0\n",
       "176  33055  @RealJamesWoods And because the federal govt u...  0.0    0.0\n",
       "177  32705  @Patriot_Mom_17 Yes We Need It YesterdayHouse ...  0.0    0.0\n",
       "178  31176  Maybe he's late cause he can't get his huge fe...  1.0    1.0\n",
       "179  30544  @sherry_kiskunas @DMRegister #IllegalAliens ar...  0.0    0.0\n",
       "180  31012  More than 65 million strong, and growing.#Depo...  0.0    0.0\n",
       "181  30993  LIVE on #Periscope: Do the beat go off? Hell y...  1.0    1.0\n",
       "182  32742                          bitch ass hoe you feel me  1.0    1.0\n",
       "183  33278  @x__MMarie @notreallyno19 I can't stand you ho...  1.0    1.0\n",
       "184  33526  @kaitrosindia They're like omg how do you not ...  1.0    1.0\n",
       "185  30001  @dbongino Dan my #vote was #changed 2x in #del...  0.0    0.0\n",
       "186  32444  #IllegalAliens. Or as the IRS refers to them #...  0.0    0.0\n",
       "187  34198  Definition of a whore/trash/piece of dirt/bitc...  1.0    1.0\n",
       "188  32844  It's still Fuck OJ. Bitch ass nigga. But good ...  1.0    1.0\n",
       "189  31591  Not to sound like a corny ass bitch but like i...  1.0    1.0\n",
       "190  33469  Islamic migrant urges Germans, Austrians to gi...  1.0    1.0\n",
       "191  33763  Depression isn't just a bitch. It's a Cunt! St...  1.0    1.0\n",
       "192  30092  @Akarnious Part 1 #Sudanese get home loans wit...  0.0    0.0\n",
       "193  32701  CBS: Driver an Illegal Alien was drunk, high a...  0.0    0.0\n",
       "194  34379  @zusanne222222 @JackLon09169906 @AupolNews #Se...  0.0    0.0\n",
       "195  31788  I'm not out here entertaining dudes bout shit....  1.0    1.0\n",
       "196  31331  im gonna be that bitch colorful does NOT equal...  1.0    1.0\n",
       "197  30353  Bitch they done free'd the OJ 😂😂😂 you hoe mad lol  1.0    1.0\n",
       "198  30112  Whoever broke into my car and stole my weed th...  0.0    0.0\n",
       "\n",
       "[199 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dev_own_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(pd_train):\n",
    "    comments=pd_train['text'].values\n",
    "    labels=pd_train['class'].values\n",
    "    list_comment=[]\n",
    "    for comment,label in zip(comments,labels):\n",
    "        temp={}\n",
    "        temp['text']=comment\n",
    "        temp['label']=label\n",
    "        list_comment.append(temp)\n",
    "    return list_comment \n",
    "\n",
    "def get_data_test(pd_test):\n",
    "    comments=pd_test['text'].values\n",
    "    list_comment=[]\n",
    "    for comment in comments:\n",
    "        temp={}\n",
    "        temp['text']=comment\n",
    "        list_comment.append(temp)\n",
    "    return list_comment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}\n",
    "def create_vocab(data):\n",
    "    comments = get_data(data)\n",
    "    for comment in comments:\n",
    "        words=glove_tokenize_vocab(comment['text'])\n",
    "        for word in words:\n",
    "            if word in vocab.keys():\n",
    "                vocab[word]=vocab[word]+1 \n",
    "            else:\n",
    "                vocab[word]=1\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_0:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_0\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_1:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_1\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_10:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_10\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_11:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_11\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_12:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_12\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_13:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_13\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_14:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_14\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_15:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_15\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_16:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_16\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_2:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_2\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_3:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_3\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_4:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_4\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_5:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_5\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_6:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_6\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_7:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_7\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_8:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_8\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_9:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Embeddings_en/sharded_9\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/conv1/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/conv1/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/conv1/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/conv1/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/conv2/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/conv2/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/conv2/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/conv2/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/ffn/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/k/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/k/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/output_transform/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/output_transform/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/q/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/q/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/v/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_0/self_attention/multihead_attention/v/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/conv1/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/conv1/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/conv1/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/conv1/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/conv2/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/conv2/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/conv2/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/conv2/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/ffn/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/multihead_attention/k/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/multihead_attention/k/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/multihead_attention/output_transform/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/multihead_attention/output_transform/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/multihead_attention/q/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/multihead_attention/q/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/multihead_attention/v/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_1/self_attention/multihead_attention/v/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/conv1/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/conv1/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/conv1/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/conv1/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/conv2/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/conv2/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/conv2/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/conv2/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/ffn/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/multihead_attention/k/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/multihead_attention/k/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/multihead_attention/output_transform/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/multihead_attention/output_transform/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/multihead_attention/q/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/multihead_attention/q/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/multihead_attention/v/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_2/self_attention/multihead_attention/v/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/conv1/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/conv1/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/conv1/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/conv1/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/conv2/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/conv2/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/conv2/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/conv2/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/ffn/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/multihead_attention/k/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/multihead_attention/k/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/multihead_attention/output_transform/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/multihead_attention/output_transform/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/multihead_attention/q/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/multihead_attention/q/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/multihead_attention/v/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_3/self_attention/multihead_attention/v/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/conv1/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/conv1/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/conv1/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/conv1/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/conv2/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/conv2/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/conv2/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/conv2/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/ffn/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/multihead_attention/k/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/multihead_attention/k/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/multihead_attention/output_transform/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/multihead_attention/output_transform/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/multihead_attention/q/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/multihead_attention/q/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/multihead_attention/v/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_4/self_attention/multihead_attention/v/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/conv1/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/conv1/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/conv1/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/conv1/kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/conv2/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/conv2/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/conv2/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/conv2/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/ffn/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/multihead_attention/k/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/multihead_attention/k/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/multihead_attention/output_transform/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/multihead_attention/output_transform/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/multihead_attention/q/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/multihead_attention/q/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/multihead_attention/v/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_5/self_attention/multihead_attention/v/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_prepostprocess/layer_norm/layer_norm_bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_prepostprocess/layer_norm/layer_norm_bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_prepostprocess/layer_norm/layer_norm_scale:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/TransformerEncodeFast/encoder/layer_prepostprocess/layer_norm/layer_norm_scale\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/dense/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/dense/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/Transformer/dense/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/Transformer/dense/kernel\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/hidden_layers/tanh_layer_0/dense/bias:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/hidden_layers/tanh_layer_0/dense/bias\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/hidden_layers/tanh_layer_0/dense/kernel:0 from checkpoint b'/tmp/tfhub_modules/96e8f1d3d4d90ce86b2db128249eb8143a91db73/variables/variables' with Encoder_en/hidden_layers/tanh_layer_0/dense/kernel\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n",
    "embed = hub.Module(module_url)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=12,\n",
    "                       allow_soft_placement=True, device_count = {'CPU': 12})\n",
    "\n",
    "def get_embeddings(messages):\n",
    "      \n",
    "    with tf.Session(config=config) as session:\n",
    "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "            message_emb = session.run(embed(messages))\n",
    "            \n",
    "    print(\"ending\")\n",
    "    return np.array(message_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from pathlib import Path\n",
    "\n",
    "    # file exists\n",
    "ps = PorterStemmer()\n",
    "\n",
    "TOKENIZER = glove_tokenize\n",
    "\n",
    "#google encoding used where text is cleaned  \n",
    "def gen_data_google(data):\n",
    "    comments = get_data(data)\n",
    "    X, y = [], []\n",
    "    for comment in comments:\n",
    "        y.append(comment['label'])\n",
    "        #X.append(tokenizer(comment['text']))\n",
    "        X.append(clean(comment['text'], remove_stopwords=True, remove_punctuations=True))\n",
    "    #TFIDF_feature = 'bpe_text'\n",
    "\n",
    "    #Word Level Features\n",
    "    X =get_embeddings(X)\n",
    "    # print y\n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "#google encoding used where text is not cleaned \n",
    "def gen_data_google2(data):\n",
    "    comments = get_data(data)\n",
    "    X, y = [],[]\n",
    "    for comment in comments:\n",
    "        y.append(comment['label'])\n",
    "        X.append(clean(comment['text'], remove_stopwords=False, remove_punctuations=False))\n",
    "    #Word Level Features\n",
    "    X =get_embeddings(X)\n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    return X,y\n",
    "\n",
    "###get data of subjectivity\n",
    "def subject(data):\n",
    "    comments = get_data(data)\n",
    "    X = []\n",
    "    for comment in comments:\n",
    "        comment=clean(comment['text'], remove_stopwords=False, remove_punctuations=False)\n",
    "        X.append(TextBlob(comment).sentiment.subjectivity)\n",
    "        #Word Level Features\n",
    "    \n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "### tfidf feature generation was used here where stopwords and punctuations are removed \n",
    "def gen_data_new_tfidf(data):\n",
    "        comments = get_data(data)\n",
    "        comments_test=get_data_test(pd_test)\n",
    "        comments_dev=get_data_test(pd_dev_binary)\n",
    "        \n",
    "        X, y = [], []\n",
    "        for comment in comments:\n",
    "            y.append(comment['label'])\n",
    "            X.append(comment['text'])\n",
    "       \n",
    "        X1=[]\n",
    "        for comment in comments_test:\n",
    "            X1.append(comment['text'])\n",
    "        \n",
    "    \n",
    "        for comment in comments_dev:\n",
    "            X1.append(comment['text'])\n",
    "        \n",
    "        \n",
    "        word_to_vec_file = Path(\"tfidf_word_vectorizer.pk\")\n",
    "        char_to_vec_file = Path(\"tfidf_char_vectorizer.pk\")\n",
    "        if word_to_vec_file.is_file() :\n",
    "            with open('tfidf_word_vectorizer.pk', 'rb') as fout:\n",
    "                word_vectorizer=pickle.load(fout)\n",
    "        else:\n",
    "            word_vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3),\n",
    "                    min_df=1, \n",
    "                    strip_accents='unicode',\n",
    "                    #smooth_idf=1,\n",
    "                    analyzer='word', \n",
    "                    stop_words='english',\n",
    "                    tokenizer=TOKENIZER,             \n",
    "                    max_features=5000)\n",
    "            word_vectorizer.fit(X+X1)\n",
    "            with open('tfidf_word_vectorizer.pk', 'wb') as fout:\n",
    "               pickle.dump(word_vectorizer,fout)\n",
    "\n",
    "\n",
    "\n",
    "        if  char_to_vec_file.is_file():\n",
    "            with open('tfidf_char_vectorizer.pk', 'rb') as fout:\n",
    "                 char_vectorizer=pickle.load(fout)\n",
    "        else:\n",
    "           char_vectorizer = TfidfVectorizer(\n",
    "                            sublinear_tf=True,\n",
    "                            strip_accents='unicode',\n",
    "                            analyzer='char',\n",
    "                            #stop_words='english',\n",
    "                            ngram_range=(2, 6),\n",
    "                            max_features=10000)\n",
    "           char_vectorizer.fit(X+X1)\n",
    "           with open('tfidf_char_vectorizer.pk', 'wb') as fout:\n",
    "               pickle.dump(char_vectorizer,fout)\n",
    "        \n",
    "        pos_to_vec_file = Path(\"tfidf_pos_vectorizer.pk\")\n",
    "    \n",
    "        if  pos_to_vec_file.is_file():\n",
    "            with open('tfidf_pos_vectorizer.pk', 'rb') as fout:\n",
    "                 pos_vectorizer=pickle.load(fout)\n",
    "        else:\n",
    "           #Word Level Features\n",
    "           pos_vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3),\n",
    "                    min_df=1, \n",
    "                    strip_accents='unicode',\n",
    "                    smooth_idf=1,\n",
    "                    analyzer='word', \n",
    "                    stop_words=None,\n",
    "                    tokenizer=pos_tokenize,             \n",
    "                    max_features=5000)\n",
    "\n",
    "\n",
    "           pos_vectorizer.fit(X+X1)\n",
    "           with open('tfidf_pos_vectorizer.pk', 'wb') as fout:\n",
    "             pickle.dump(pos_vectorizer,fout)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        test_pos_features = pos_vectorizer.transform(X)\n",
    "        test_word_features = word_vectorizer.transform(X)\n",
    "        test_char_features = char_vectorizer.transform(X)\n",
    "        X = list(hstack([test_char_features, test_word_features,test_pos_features]).toarray())\n",
    "        #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### tfidf feature generation was used here where stopwords and punctuations are not removed \n",
    "def gen_data_new_tfidf2(data):\n",
    "    comments = get_data(data)\n",
    "    X, y = [], []\n",
    "    for comment in comments:\n",
    "        y.append(comment['label'])\n",
    "        X.append(comment['text'])\n",
    "\n",
    "\n",
    "    #Word Level Features\n",
    "    word_vectorizer = TfidfVectorizer(sublinear_tf=True, ngram_range=(1,3),\n",
    "                min_df=1, \n",
    "                strip_accents='unicode',\n",
    "                #smooth_idf=1,\n",
    "                analyzer='word', \n",
    "                stop_words=None,\n",
    "                tokenizer=glove_tokenize_norem,             \n",
    "                max_features=5000)\n",
    "    \n",
    "    \n",
    "    #charlevel features new\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words=None,\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=10000)\n",
    "    \n",
    "    word_vectorizer.fit(X)\n",
    "    char_vectorizer.fit(X)\n",
    "    \n",
    "    with open('tfidf_word_vectorize_noclean.pk', 'wb') as fout:\n",
    "         pickle.dump(word_vectorizer,fout)\n",
    "\n",
    "    with open('tfidf_char_vectorizer_noclean.pk', 'wb') as fout:\n",
    "         pickle.dump(char_vectorizer,fout)\n",
    "        \n",
    "    test_word_features = word_vectorizer.transform(X)\n",
    "    test_char_features = char_vectorizer.transform(X)\n",
    "    X = list(hstack([test_char_features, test_word_features]).toarray())\n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "## combination of not cleaned google encodings and tfidf features where stopwords and punctuations are not removed \n",
    "def combine_tf_google_rem(data):\n",
    "    X,_=gen_data_google(data)\n",
    "    X1,y=gen_data_new_tfidf(data)\n",
    "#     X1,y=gen_data_old_tfidf()\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y\n",
    "\n",
    "## combination of cleaned google encodings and tfidf features where stopwords and punctuations are ssremoved \n",
    "def combine_tf_google_norem(data):\n",
    "    X,_=gen_data_google2(data)\n",
    "    X1,y=gen_data_new_tfidf2(data)\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y\n",
    "\n",
    "def combine_tf_rem_google_norem(data):\n",
    "    X,_=gen_data_google2(data)\n",
    "    X1,y=gen_data_new_tfidf(data)\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y\n",
    "\n",
    "def combine_tf_norem_google_rem(data):\n",
    "    X,_=gen_data_google(data)\n",
    "    X1,y=gen_data_new_tfidf2(data)\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y\n",
    "\n",
    "def gen_data_embed(data,word2vec,embed_dim):\n",
    "    comments = get_data(data)\n",
    "    X, y = [], []\n",
    "    for comment in comments:\n",
    "        words = glove_tokenize_embed(comment['text'].lower())\n",
    "        emb = np.zeros(embed_dim)\n",
    "        for word in words:\n",
    "            try:\n",
    "                emb += word2vec[word]\n",
    "            except:\n",
    "                pass\n",
    "        if len(words)!=0:\n",
    "            emb /= len(words)\n",
    "        X.append(emb)\n",
    "        y.append(comment['label'])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def combine_tf_rem_google_norem_embed(data):\n",
    "    X,_=gen_data_google2(data)\n",
    "    X1,y=gen_data_new_tfidf(data)\n",
    "    print(\"1loaded \")\n",
    "    X2,_=gen_data_embed(data,word2vec_model3,300)\n",
    "    print(\"2loaded\")\n",
    "    X3,_=gen_data_embed(data,word2vec_model2,300)\n",
    "    print(\"3loaded\")\n",
    "    X4,_=gen_data_embed(data,word2vec_model1,200)\n",
    "    print(\"4loaded\")\n",
    "    X5=  subject(data)\n",
    "    print(\"5loaded\")\n",
    "    X6,_=gen_data_embed(data,word2vec_model4,300)\n",
    "    print(\"6loaded\")\n",
    "    #X=np.concatenate((np.array(X), np.array(X1),np.array(X2),np.array(X3).reshape(len(X3),1),), axis=1)\n",
    "    X=np.concatenate((np.array(X), np.array(X1),np.array(X2),np.array(X3),np.array(X4),np.array(X5).reshape(len(X5),1),np.array(X6)), axis=1)\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "\n",
    "\n",
    "###old tfidf\n",
    "\n",
    "def gen_data_old_tfidf(data):\n",
    "    comments = get_data(data)\n",
    "    X, y = [], []\n",
    "    for comment in comments:\n",
    "        y.append(comment['label'])\n",
    "        X.append(comment['text'])\n",
    "    with open('../tfidf_word_vectorizer.pk', 'rb') as fin:\n",
    "        word_vectorizer = pickle.load(fin)\n",
    "\n",
    "    with open('../tfidf_char_vectorizer.pk', 'rb') as fin:\n",
    "        char_vectorizer = pickle.load(fin)\n",
    "\n",
    "\n",
    "    \n",
    "    word_vectorizer.fit(X)\n",
    "    char_vectorizer.fit(X)\n",
    "    \n",
    "    test_word_features = word_vectorizer.transform(X)\n",
    "    test_char_features = char_vectorizer.transform(X)\n",
    "    X = list(hstack([test_char_features, test_word_features]).toarray())\n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def select_comments_whose_embedding_exists(flag):\n",
    "    # selects the comments as in mean_glove_embedding method\n",
    "    # Processing\n",
    "    comments = get_data(flag)\n",
    "    X, Y = [], []\n",
    "    comment_return = []\n",
    "    for tweet in comments:\n",
    "        #print(tweet)\n",
    "        _emb = 0\n",
    "        words = TOKENIZER(tweet['text'].lower())\n",
    "        for w in words:\n",
    "            #print(w)\n",
    "            if w in word2vec_model and w is not None:  # Check if embeeding there in GLove model\n",
    "                _emb+=1\n",
    "        if _emb:   # Not a blank tweet\n",
    "            comment_return.append(tweet)\n",
    "    print('Comments selected:', len(comment_return))\n",
    "    return comment_return\n",
    "\n",
    "def gen_data():\n",
    "    comments = select_comments_whose_embedding_exists(0)\n",
    "    X, y = [], []\n",
    "    for comment in comments:\n",
    "        words = glove_tokenize(comment['text'].lower())\n",
    "        emb = numpy.zeros(EMBEDDING_DIM)\n",
    "        for word in words:\n",
    "            try:\n",
    "                emb += word2vec_model[word]\n",
    "            except:\n",
    "                pass\n",
    "        emb /= len(words)\n",
    "        X.append(emb)\n",
    "        y.append(comment['label'])\n",
    "\n",
    "    # print y\n",
    "    y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    print\n",
    "    return X, y\n",
    "\n",
    "## combination of not cleaned google encodings and tfidf features where stopwords and punctuations are not removed \n",
    "def combine_tf_google_glove_rem():\n",
    "    X,_=gen_data_google()\n",
    "    X1,y=gen_data_new_tfidf()\n",
    "#     X1,y=gen_data_old_tfidf()\n",
    "    X=np.concatenate((np.array(X), np.array(X1)), axis=1)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_POS_WEIGHT=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.combine import SMOTETomek \n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold as skf\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def binny_classifier_run(X,y,model,model_name,label_map,img_name,report_name,save_model=False):\n",
    "    Classifier_Train_X = np.array(X, copy=False)\n",
    "    Classifier_Train_Y = y\n",
    "    temp=[]\n",
    "    for data in Classifier_Train_Y:\n",
    "        temp.append(label_map[data])\n",
    "    SCALE_POS_WEIGHT=temp.count(0)/temp.count(1)\n",
    "    \n",
    "    Classifier_Train_Y=np.array(temp)\n",
    "    \n",
    "    \n",
    "    #model_featureSelection = SelectFromModel(ensemble.RandomForestClassifier(n_estimators=500, class_weight='balanced',n_jobs=12, max_depth=7))\n",
    "    #model_featureSelection = SelectFromModel(ExtraTreesClassifier(n_estimators=50))\n",
    "    model = LogisticRegression()\n",
    "    model_featureSelection = RFE(model,1300,step=0.5) \n",
    "    \n",
    "    print('Before Num features=',Classifier_Train_X.shape[1], Counter(Classifier_Train_Y))\n",
    "    Classifier_Train_X = model_featureSelection.fit_transform(Classifier_Train_X,Classifier_Train_Y)\n",
    "    print('After Num features=',Classifier_Train_X.shape[1])\n",
    "\n",
    "    \n",
    "    if(save_model==True):\n",
    "        Classifier=model\n",
    "        Classifier.fit(Classifier_Train_X, Classifier_Train_Y)\n",
    "        filename = model_name+'_task_1.joblib.pkl'\n",
    "        joblib.dump(Classifier, filename, compress=9)\n",
    "        filename1 = 'select_features_task1.joblib.pkl'\n",
    "        joblib.dump(model_featureSelection, filename1, compress=9)\n",
    "    else:\n",
    "        kf = skf(n_splits=10,shuffle=True)\n",
    "        y_total_preds=[] \n",
    "        y_total=[]\n",
    "        count=0\n",
    "\n",
    "        for train_index, test_index in kf.split(Classifier_Train_X,Classifier_Train_Y):\n",
    "            X_train, X_test = Classifier_Train_X[train_index], Classifier_Train_X[test_index]\n",
    "            y_train, y_test = Classifier_Train_Y[train_index], Classifier_Train_Y[test_index]\n",
    "\n",
    "            classifier=model \n",
    "            classifier.fit(X_train,y_train)\n",
    "            y_preds = classifier.predict(X_test)\n",
    "            for ele in y_test:\n",
    "                y_total.append(ele)\n",
    "            for ele in y_preds:\n",
    "                y_total_preds.append(ele)\n",
    "            y_pred_train = classifier.predict(X_train)\n",
    "            count=count+1       \n",
    "            print('accuracy_train:',accuracy_score(y_train, y_pred_train),'accuracy_test:',accuracy_score(y_test, y_preds))\n",
    "            print('TRAINING:')\n",
    "            print(classification_report( y_train, y_pred_train ))\n",
    "            print(\"TESTING:\")\n",
    "            print(classification_report( y_test, y_preds ))\n",
    "            \n",
    "        report = classification_report( y_total, y_total_preds )\n",
    "        cm=confusion_matrix(y_total, y_total_preds)\n",
    "        plt=plot_confusion_matrix(cm,normalize= True,target_names = ['non_hatespeech','hatespeech'],title = \"Confusion Matrix\")\n",
    "        import time\n",
    "        import zipfile\n",
    "\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        plt.savefig('task1_'+timestr+'_'+model_name+'_'+img_name)\n",
    "        print(model)\n",
    "        print(report)\n",
    "        prRandomForestClassifierint(accuracy_score(y_total, y_total_preds))\n",
    "        df_result=pandas_classification_report(y_total,y_total_preds)\n",
    "        df_result.to_csv('task1_'+timestr+'__'+model_name+'_'+report_name,  sep=',')\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "def classifier_run(X,y,X_dev,y_dev,X_dev_own,y_dev_own,model,model_name,label_map,img_name,report_name,save_model=False):\n",
    "    Classifier_Train_X = np.array(X, copy=False)\n",
    "    Classifier_Train_Y = y\n",
    "    Classifier_Dev_X = np.array(X_dev, copy=False)\n",
    "    Classifier_Dev_Y = y_dev\n",
    "    X_dev_own = np.array(X_dev_own, copy=False)\n",
    "    \n",
    "    temp=[]\n",
    "    for data in Classifier_Train_Y:\n",
    "        temp.append(label_map[data])\n",
    "    SCALE_POS_WEIGHT=temp.count(0)/temp.count(1)\n",
    "    print(SCALE_POS_WEIGHT)\n",
    "    Classifier_Train_Y=np.array(temp)\n",
    "    \n",
    "    temp=[]\n",
    "    for data in Classifier_Dev_Y:\n",
    "        temp.append(label_map[data])\n",
    "    Classifier_Dev_Y=np.array(temp)\n",
    "    \n",
    "    temp=[]\n",
    "    for data in y_dev_own:\n",
    "        try:\n",
    "            temp.append(int(data))\n",
    "        except:\n",
    "            temp.append(0)\n",
    "    y_dev_own=np.array(temp)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_featureSelection = SelectFromModel(ensemble.RandomForestClassifier(n_estimators=50, class_weight='balanced',n_jobs=12, max_depth=6,random_state=42))\n",
    "    print(model_featureSelection)\n",
    "    \n",
    "    print('Before Num features=',Classifier_Train_X.shape[1], Counter(Classifier_Train_Y))\n",
    "    Classifier_Train_X = model_featureSelection.fit_transform(Classifier_Train_X,Classifier_Train_Y)\n",
    "    print('After Num features=',Classifier_Train_X.shape[1])\n",
    "    print('Before Num features=',Classifier_Dev_X.shape[1], Counter(Classifier_Dev_Y))\n",
    "    Classifier_Dev_X = model_featureSelection.transform(Classifier_Dev_X)\n",
    "    print('After Num features=',Classifier_Dev_X.shape[1])\n",
    "    print('Before Num features=',X_dev_own.shape[1], Counter(y_dev_own))\n",
    "    X_dev_own = model_featureSelection.transform(X_dev_own)\n",
    "    print('After Num features=',X_dev_own.shape[1])\n",
    "\n",
    "    \n",
    "    if(save_model==True):\n",
    "        Classifier=model\n",
    "        X=np.concatenate((np.array(Classifier_Train_X), np.array(Classifier_Dev_X)), axis=0)\n",
    "        Y=np.concatenate((np.array(Classifier_Train_Y), np.array(Classifier_Dev_Y)), axis=0)\n",
    "        \n",
    "        Classifier.fit(X,Y)\n",
    "        filename = model_name+'_task_1.joblib.pkl'\n",
    "        joblib.dump(Classifier, filename, compress=9)\n",
    "        filename1 = model_name+'select_features_task1.joblib.pkl'\n",
    "        joblib.dump(model_featureSelection, filename1, compress=9)\n",
    "    else:\n",
    "        X=np.concatenate((np.array(Classifier_Train_X), np.array(Classifier_Dev_X)), axis=0)\n",
    "        Y=np.concatenate((np.array(Classifier_Train_Y), np.array(Classifier_Dev_Y)), axis=0)\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "        for train_index, test_index in sss.split(X, Y):\n",
    "                print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        model.fit(X_train,y_train)\n",
    "        y_preds = model.predict(X_test) \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_own = model.predict(X_dev_own) \n",
    "        \n",
    "        print('accuracy_train:',accuracy_score(y_train, y_pred_train),'accuracy_test:',accuracy_score(y_test, y_preds),'accuracy_test_own:',accuracy_score(y_dev_own, y_pred_own))\n",
    "        print('TRAINING:')\n",
    "        print(classification_report( y_train, y_pred_train ))\n",
    "        print(\"TESTING:\")\n",
    "        print(classification_report( y_test, y_preds ))\n",
    "        print(\"TESTING_OWN:\")\n",
    "        print(classification_report( y_dev_own, y_pred_own ))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = 'gaussian'\n",
    "feature_model = 'google_nopreprocess_tfidf_preprocess_embed'\n",
    "img_name = 'cm.png'\n",
    "report_name = 'report.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['fifti'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1loaded \n",
      "2loaded\n",
      "3loaded\n",
      "4loaded\n",
      "5loaded\n",
      "6loaded\n",
      "google_nopreprocess_tfidf_preprocess_embed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binny/anaconda3/envs/punyajoy-nogpu/lib/python3.5/site-packages/ipykernel/__main__.py:225: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "data_name= 'pd_train_binary'\n",
    "\n",
    "# X,y=get_feature(f_type=feature_model,data=pd_train_binary)\n",
    "\n",
    "# X_dev,y_dev=get_feature(f_type=feature_model,data=pd_dev_binary)\n",
    "    \n",
    "X_dev_own,y_dev_own=get_feature(f_type=feature_model,data=pd_dev_own_binary)\n",
    "    \n",
    "label_map = {\n",
    "             1: 1,\n",
    "             0: 0\n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 21613)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_own.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('X_train.npy', X)\n",
    "# np.save('X_dev.npy', X_dev)\n",
    "np.save('X_dev_own.npy', X_dev_own)\n",
    "# np.save('y_train.npy', y)\n",
    "# np.save('y_dev.npy', y_dev)\n",
    "np.save('y_dev_own.npy', y_dev_own)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model(m_type=classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-15T02:44:30.187402Z",
     "start_time": "2019-07-15T02:44:30.133068Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-8f5c8cf85d96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mADASYN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikitlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSklearnClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "SCALE_POS_WEIGHT=None\n",
    "\n",
    "def get_model(m_type=None):\n",
    "    if not m_type:\n",
    "        print(\"ERROR: Please specify a model type!\")\n",
    "        return None\n",
    "    if m_type == 'decision_tree_classifier':\n",
    "        logreg = tree.DecisionTreeClassifier()\n",
    "    elif m_type == 'gaussian':\n",
    "        logreg = GaussianNB()\n",
    "    elif m_type == 'logistic_regression':\n",
    "        logreg = LogisticRegression(n_jobs=10, random_state=42)\n",
    "    elif m_type == 'MLPClassifier':\n",
    "#         logreg = neural_network.MLPClassifier((500))\n",
    "        logreg = neural_network.MLPClassifier((100),random_state=42,early_stopping=True)\n",
    "    elif m_type == 'KNeighborsClassifier':\n",
    "#         logreg = neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
    "        logreg = neighbors.KNeighborsClassifier()\n",
    "    elif m_type == 'ExtraTreeClassifier':\n",
    "        logreg = tree.ExtraTreeClassifier()\n",
    "    elif m_type == 'ExtraTreeClassifier_2':\n",
    "        logreg = ensemble.ExtraTreesClassifier()\n",
    "    elif m_type == 'RandomForestClassifier':\n",
    "        logreg = ensemble.RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=12, max_depth=7)\n",
    "    elif m_type == 'SVC':\n",
    "        #logreg = LinearSVC(dual=False,max_iter=200)\n",
    "        logreg = SVC(kernel='linear',random_state=1526)\n",
    "        \n",
    "    elif m_type == 'Catboost':\n",
    "        logreg = CatBoostClassifier(iterations=1000,learning_rate=0.2,l2_leaf_reg=500,depth=10,use_best_model=False, random_state=42,scale_pos_weight=SCALE_POS_WEIGHT)\n",
    "#         logreg = CatBoostClassifier(scale_pos_weight=0.8, random_seed=42,);\n",
    "    elif m_type == 'XGB_classifier':\n",
    "#         logreg=XGBClassifier(silent=False,eta=0.1,objective='binary:logistic',max_depth=5,min_child_weight=0,gamma=0.2,subsample=0.8, colsample_bytree = 0.8,scale_pos_weight=1,n_estimators=500,reg_lambda=3,nthread=12)\n",
    "        logreg=XGBClassifier(silent=False,objective='binary:logistic',scale_pos_weight=0.8,reg_lambda=3,nthread=12, random_state=42)\n",
    "    elif m_type == 'binny_test':\n",
    "        clf1 = ensemble.RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=12, max_depth=6,max_features='auto')\n",
    "        clf2 = tree.DecisionTreeClassifier(random_state=42, class_weight='balanced',max_depth=6)\n",
    "        clf3 = LogisticRegression(class_weight='balanced',penalty=\"l2\",C=0.1, dual=True, random_state=42, n_jobs=3)\n",
    "        clf4 = XGBClassifier(silent=False,objective='binary:logistic',scale_pos_weight=0.8,reg_lambda=3,nthread=12, random_state=42)\n",
    "        est_list = [('lr', clf1), ('rf', clf2), ('gnb', clf3), ('xgb', clf4)]\n",
    "        logreg = ensemble.VotingClassifier(est_list,voting='soft',n_jobs=6)\n",
    "    elif m_type == 'ensem':\n",
    "        ens_list=[]\n",
    "        model= LGBMClassifier(objective='binary',max_depth=7,learning_rate=0.2,num_leaves=50,boosting_type='gbdt',metric='binary_logloss',random_state=5,reg_lambda=20,silent=True,scale_pos_weight=SCALE_POS_WEIGHT)\n",
    "        ens_list.append(('lgb1',model))\n",
    "        model1 = ensemble.RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=12, max_depth=5)\n",
    "        ens_list.append(('rf',model1)) \n",
    "        model2 = neural_network.MLPClassifier((100),random_state=42,early_stopping=True)\n",
    "        ens_list.append(('mlp',model2)) \n",
    "        model3 = SVC(kernel='linear',probability=True,random_state=2018)\n",
    "        ens_list.append(('svc',model3)) \n",
    "        model4 = SVC(kernel='linear',probability=True,random_state=1526)\n",
    "        ens_list.append(('svc1',model4)) \n",
    "#         model5 = SVC(kernel='linear',probability=True,random_state=2019)\n",
    "#         ens_list.append(('svc2',model5)) \n",
    "        \n",
    "        logreg = ensemble.VotingClassifier(ens_list,voting='hard',n_jobs=6)\n",
    "        \n",
    "    elif m_type == 'light_gbm':\n",
    "        logreg = LGBMClassifier(objective='binary',max_depth=3,learning_rate=0.2,num_leaves=20,boosting_type='gbdt',metric='binary_logloss',random_state=5,reg_lambda=20,silent=False)\n",
    "\n",
    "    else:\n",
    "        print(\"give correct model\")\n",
    "    print(logreg)\n",
    "    return logreg\n",
    "\n",
    "def get_feature(f_type=None,data=None):\n",
    "    if not f_type:\n",
    "        print(\"ERROR: Please specify a model type!\")\n",
    "        return None\n",
    "    if f_type == 'google_not_preprocess':\n",
    "        X,y=gen_data_google2(data)\n",
    "    elif f_type == 'word_to_vec_embed':\n",
    "        X,y=gen_data_embed(data)\n",
    "    elif f_type == 'google_preprocess':\n",
    "        X,y=gen_data_google(data)\n",
    "    elif f_type == 'tfidf_not_preprocess':\n",
    "        X,y=gen_data_new_tfidf2(data)\n",
    "    elif f_type == 'tfidf_preprocess':\n",
    "        X,y=gen_data_new_tfidf(data)\n",
    "    elif f_type == 'google_preprocess_tfidf_preprocess':\n",
    "        X,y=combine_tf_google_rem(data)\n",
    "    elif f_type == 'google_nopreprocess_tfidf_nopreprocess':\n",
    "        X,y=combine_tf_google_norem(data)\n",
    "    elif f_type == 'google_preprocess_tfidf_nopreprocess':\n",
    "        X,y=combine_tf_norem_google_rem(data)\n",
    "    elif f_type == 'google_nopreprocess_tfidf_preprocess':\n",
    "        X,y=combine_tf_rem_google_norem(data)\n",
    "    elif f_type == 'google_nopreprocess_tfidf_preprocess_embed':\n",
    "        X,y=combine_tf_rem_google_norem_embed(data)\n",
    "    else:\n",
    "        print(\"give correct feature selection\")    \n",
    "    print(f_type)\n",
    "    return X,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=1526,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "1.3790642347343378\n",
      "SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=6, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=12, oob_score=False, random_state=42,\n",
      "            verbose=0, warm_start=False),\n",
      "        max_features=None, norm_order=1, prefit=False, threshold=None)\n",
      "Before Num features= 21613 Counter({0: 5217, 1: 3783})\n",
      "After Num features= 1342\n",
      "Before Num features= 21613 Counter({0: 573, 1: 427})\n",
      "After Num features= 1342\n",
      "Before Num features= 21613 Counter({1: 124, 0: 75})\n",
      "After Num features= 1342\n",
      "TRAIN: [2065 2412  644 ...  792 9989 9513] TEST: [1599 8281 9835 4910 8357 3133 1595 1779 7549  442 1009 9568 1946 6992\n",
      " 2379 5785 9092 4247 5157 3209 9405 2625  466 7136 9587 1667 7864 7797\n",
      " 7668 5634 6049 5908 1435 1269 8146 3296 6035 8861 4989 9642 1397 2737\n",
      "  906 9848 5539 6769  938 2094 5676 6879 6139 8117 5476  962 2828 3529\n",
      "  977 7984 8248 5696   37 5679 1276 8562  331 2461 9141 4708 3033  673\n",
      " 3050 3774  676 5020 3513 3092 1134 5175 4784  847 7024 7106 9047 3276\n",
      " 1722 4629 8787 3734 5301 9220 6332 7788 6116  510 1414 6561 8390 2744\n",
      " 9216 4044 3928 8920 2161 9630 3902 5769 1998 3895 7877 4617 3754 9049\n",
      " 4856 4802 8532 7781 7989 8243 9427 2806  209 5559 1553 1566 7421    0\n",
      "  496 1417 1505 6517 4608 8462 9449 1747 5412 8551 8579 4086 5158 5014\n",
      " 2252 2740 2334 4675 4489 4303 5179 6494 8860 7606 3462 5917  726 5065\n",
      " 2677 6840 7832 6570 1618 4048 7785 3864 7635 9059 4561 9218 8879  934\n",
      " 7235 1228 3370    1 7839 4159 2752 8797 7479 7889 6336 3094 2282 3561\n",
      " 4538 2692 8180 1203  741 5161 3793  118 1020 8040 7563 9079  765 5881\n",
      " 9834 8582 2004 2325 4396 8446 1658 1475 6195 5962 3246 3445 5563  398\n",
      " 1001 3433 9869 1107 5677 5889 3414 3982 2453 1392 6873 1468 5973 9546\n",
      " 9941 1275 6971 7411 8633  563 3831 9100 9448 5460 8654 7909 1936 6164\n",
      " 9466 9320 2111 6836   38 9756 2889 7577 4588 8730 6226 5062 4216 4594\n",
      " 3726 3869 9840  284 5509 6978 7356 3252 8425 1054 5594  344 9782 4352\n",
      " 5870 7547 9201 2716 2294 2464 7375  914 7672 6367 4739 6496 1883   34\n",
      " 7016  709  279 5058 7454 9353 4318 6986 5103 4998 6789 1821 1543 8118\n",
      " 1795 9369  162 3974 9318 9182 6509 4349 7449  472 2536 1674 2915 5236\n",
      " 8456 1025 2927 6473 6065 5560 6970 2192 1737 4620 6869 6486 3470 9647\n",
      " 9325 8412 7346  156 8238 6312 2063 1961 1065 9612 4160 4632 2985 6334\n",
      " 5956 3269 2466  736 4102 9936 9595 2147 8507 9102 2657 3156 3723 6092\n",
      " 4775 2170 4559  731 7263 7080 9299 6558 1310 3170 7664 7904 6534 5183\n",
      " 9377  301 6071 1215 1768 4115 3971  824 3223  712 4905 9366 4383 7208\n",
      " 7072 4768 6068 9392  957 9508 7518 7472 5989 3648   26 7718 2533 4919\n",
      " 8816 7195 9395 1449 9586 8979 1236 7684 9639  157 8944 8986 2175 1758\n",
      " 5379 9437 5982  333 5401 2682 7481 9308 1281 6772 8821 6347 3304 5398\n",
      " 2963 2805  220 7301 9793 9347  609 8985 3830 5382 6784 3381 2091 7229\n",
      " 7553 9016 9081  984 1554 5030 5830 7119 9251  842 6957 1111 7815  430\n",
      " 9824  556 8043  571  755 1267  254 6513 3403 8114 8172 3199 8623 8865\n",
      " 6690 3325 7713 3079 4036 3977  180 5687 6677 3649  605 7736 5444 9036\n",
      " 2606 8386 8170 9386 8677  800 2969 4477 3797 3616 3884 9128 6719 5160\n",
      " 1955 3919 9576 5494 3748 6649 6263 9263 4071  273  877  513 2034  796\n",
      " 6652 6472 4243 8076 8598 7499 7403   33 5918 6410 9310 3542 4289 5223\n",
      " 4801 9070 1985 1908 2918 5507  202 3112 8441 2411 6001 6098 8660 3657\n",
      " 6572 4584 8722 3943  669 8772   20 7226 7046  326 1759  467 2026 2906\n",
      " 4971 3579 2857 7769 7820 1636 3636 9302 8858 5968 1053 7194 1066  226\n",
      " 3985 8129 4473 9671 3346  242 5129 2406 5883 5396 6327  770 2053 2290\n",
      " 1647 9660 3961 6997 8458 1301 1259 3501 3963 1643 6643 5302 1204 4362\n",
      " 1463 9332 7242 2978 8392 4693  928 3975 2255 1620 9809 7429 8510 1832\n",
      " 5232 2153 1080 7805 5678 6827 8149 2678 2085 8726 1625 3418 6188  646\n",
      " 7640  274 9019 6434 1997 8897 7190 6242 1868  182 3400 9150 5305 4837\n",
      " 7312 5717 6003 8301 5647 7546 1030 2555 4194 3157 4705 4029 9750 5985\n",
      " 3139  615 8941 7802 8974 6243 4117 7588 6401 4493 9655 1993 8612 1455\n",
      " 2505 4726 4256 1083 5280 6308 1379 7437 4701 4587 7819  851 5890 2662\n",
      " 2797 9335 1330 8814 2966 9042 6748 5909 9077 5432 7360 2479 8698 3173\n",
      " 1015 3176 4816 3255  834  256 7723 8472 5340 3655 7712 2297 2586 7631\n",
      " 4659  995 3632  488 2321 1198 8041 4487 5471 4454 9601  896  456 9735\n",
      " 4760 9379 6740 2913 5270 2812 4412 3876 4914 3498 3858 4060 1760 5762\n",
      " 3535 8693 3197 3220 2856 3613 6224 4138 4010 1777 8664 1270 1605 2626\n",
      " 8370 5921 9429  405  107 9454 6028 3717 5986 8683 1256 7918 8733 2163\n",
      " 6586 4709 7475 1010 2825  751 7890 5040  548 4695 3391 5102 2522 2289\n",
      " 2941 1353 4034 2235 5174 5824 5572 9030 7065  882 6105 1954 2287 4061\n",
      " 3360 4636 3699 4753 8614 2525 2072 2988 6151 4095  163 1389 4624 7103\n",
      " 9891 4933  356 4604 9565 7779 1859 8938 5113 6617 6704 8313  346 3198\n",
      " 6659  990 6728 9399 1425 8983 4164 5656 5698 3921 2876 8932 9878 3735\n",
      " 8404 5518 6895  993 4120 9385 6758 8368 5744 5330 9080 5230 7863 2959\n",
      " 9368 1284 6910 8790 1581 9461 8545 5538 5015 2899 2115 3328 2520 4767\n",
      " 8971 3302 1929 4187 5368 6638 6454 9765 1189 8751 6837 3366 9288 2099\n",
      " 8540 8376 7970 1895 7599 6036 2242 1487 5543 7484 5801 6274 6965 9923\n",
      "  293 1473 7639 5730 8099 6760 4947  300 2048 8088  783 5691 6919 9668\n",
      " 1592 9487   39 3776 6699 2790 3298 5541 3407 5736 4731 4480 5193 6915\n",
      " 9300 3292 9511 9522 7860  328 8065 6465 8792 3075 3208 3126 4900 5101\n",
      " 6485 9119  916 6877 8521 3620 2396 1178 8561 4904 9619  221 3416 1305\n",
      " 3242 7645 5134  231 7750 9713 6021 3051 5328 4218 5069 5605 3728 2706\n",
      " 4854 6722 4724 4384 5669 2393 4827 1156 7305 8268 2182 3846 4296 5568\n",
      " 1339 9529 2143 2544 2884 4913 3916 3676 8150 7428 9349 1333 9937 5512\n",
      "  317 8417 4481 4467 6440 5737 2377 5983 1440 7906 4773 9955 9968 6592\n",
      " 2095 3598 9839 9411 6363 9388]\n"
     ]
    }
   ],
   "source": [
    "#list_of_model = ['decision_tree_classifier', 'gaussian', 'logistic_regression', 'MLPClassifier', 'RandomForestClassifier',\n",
    "#                 'SVC', 'Catboost', 'XGB_classifier']\n",
    "list_of_model = ['SVC']\n",
    "#list_of_model = ['logistic_regression']\n",
    "label_map = {\n",
    "             1: 1,\n",
    "             0: 0\n",
    "                 }\n",
    "img_name = 'cm.png'\n",
    "report_name = 'report.csv'\n",
    "\n",
    "for each_model in list_of_model:\n",
    "    model=get_model(m_type=each_model)\n",
    "    y=np.load('y_train.npy')\n",
    "    y_dev=np.load('y_dev.npy')\n",
    "    y_dev_own=np.load('y_dev_own.npy')\n",
    "    X=np.load('X_train.npy')\n",
    "    X_dev=np.load('X_dev.npy')\n",
    "    X_dev_own=np.load('X_dev_own.npy')\n",
    "    classifier_run(X,y,X_dev,y_dev,X_dev_own,y_dev_own,model,each_model,label_map,img_name,report_name,save_model=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_own.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_word_vectorizer.pk', 'rb') as fout:\n",
    "         word_vectorizer=pickle.load(fout)\n",
    "\n",
    "with open('tfidf_char_vectorizer.pk', 'rb') as fout:\n",
    "        char_vectorizer=pickle.load(fout)\n",
    "clf_task1=joblib.load('Catboost_task_1.joblib.pkl')\n",
    "select_task1=joblib.load('select_features_task1.joblib.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### adding pesudo labelling \n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "TOKENIZER = glove_tokenize\n",
    "\n",
    "\n",
    "#google encoding used where text is not cleaned \n",
    "def gen_data_google_test():\n",
    "    comments = get_data_test(pd_test)\n",
    "    X=[]\n",
    "    for comment in comments:\n",
    "        X.append(clean(comment['text'], remove_stopwords=False, remove_punctuations=False))\n",
    "    #Word Level Features\n",
    "    X =get_embeddings(X)\n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    return X\n",
    "\n",
    "### tfidf feature generation was used here where stopwords and punctuations are removed \n",
    "def gen_data_new_tfidf_test():\n",
    "    comments_test=get_data_test(pd_test)\n",
    "    X=[]\n",
    "    for comment in comments_test:\n",
    "        X.append(comment['text'])\n",
    "      \n",
    "    test_word_features = word_vectorizer.transform(X)\n",
    "    test_char_features = char_vectorizer.transform(X)\n",
    "    X = list(hstack([test_char_features, test_word_features]).toarray())\n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    return X\n",
    "\n",
    "\n",
    "def gen_data_embed_test():\n",
    "    comments = get_data_test(pd_test)\n",
    "    X=[]\n",
    "    for comment in comments:\n",
    "        words = glove_tokenize(comment['text'].lower())\n",
    "        emb = np.zeros(EMBEDDING_DIM)\n",
    "        for word in words:\n",
    "            try:\n",
    "                emb += word2vec_model[word]\n",
    "            except:\n",
    "                pass\n",
    "        if len(words)!=0:\n",
    "            emb /= len(words)\n",
    "        X.append(emb)\n",
    "        \n",
    "    return X\n",
    "\n",
    "def subject_test():\n",
    "    comments = get_data_test(pd_test)\n",
    "    X = []\n",
    "    for comment in comments:\n",
    "        comment=clean(comment['text'], remove_stopwords=False, remove_punctuations=False)\n",
    "        X.append(TextBlob(comment).sentiment.subjectivity)\n",
    "        #Word Level Features\n",
    "    \n",
    "    #y = MultiLabelBinarizer(classes = (1,2,3,4,5,6,7,8,9,10)).fit_transform(y)\n",
    "    return X\n",
    "\n",
    "\n",
    "def combine_tf_rem_google_norem_embed_test():\n",
    "    X=gen_data_google_test()\n",
    "    X1=gen_data_new_tfidf_test()\n",
    "    X2=gen_data_embed_test()\n",
    "    X3=subject_test()\n",
    "    X=np.concatenate((np.array(X), np.array(X1),np.array(X2),np.array(X3).reshape(len(X3),1)), axis=1)\n",
    "    return X\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=combine_tf_rem_google_norem_embed_test()\n",
    "list_1=[]\n",
    "for x in X_test:\n",
    "    x=x.reshape(1,-1)\n",
    "    temp=x\n",
    "    temp=select_task1.transform(temp)\n",
    "    predict1=clf_task1.predict(temp)\n",
    "    list_1.append(int(predict1[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2=y+list_1\n",
    "temp=np.concatenate((X,X_test),axis=0)\n",
    "print(len(temp_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binny_classifier_run_pl(X,y,X_test_pl,y_test_pl,model,model_name,label_map,img_name,report_name,save_model=False):\n",
    "    Classifier_Train_X = np.array(X, copy=False)\n",
    "    Classifier_Train_Y = y\n",
    "    temp=[]\n",
    "    for data in Classifier_Train_Y:\n",
    "        temp.append(label_map[data])\n",
    "    SCALE_POS_WEIGHT=temp.count(0)/temp.count(1)\n",
    "    \n",
    "    Classifier_Train_Y=np.array(temp)\n",
    "    \n",
    "    \n",
    "    print('Before Num features=',Classifier_Train_X.shape[1], Counter(Classifier_Train_Y))\n",
    "    Classifier_Train_X = select_task1.transform(Classifier_Train_X)\n",
    "    X_test_pl = select_task1.transform(X_test_pl)\n",
    "    print('After Num features=',Classifier_Train_X.shape[1])\n",
    "\n",
    "    \n",
    "    if(save_model==True):\n",
    "        Classifier=model\n",
    "        Classifier.fit(Classifier_Train_X, Classifier_Train_Y)\n",
    "        filename = model_name+'_task_1.joblib.pkl'\n",
    "        joblib.dump(Classifier, filename, compress=9)\n",
    "        #filename1 = 'select_features_task1.joblib.pkl'\n",
    "        #joblib.dump(model_featureSelection, filename1, compress=9)\n",
    "    else:\n",
    "        kf = skf(n_splits=10,shuffle=True)\n",
    "        y_total_preds=[] \n",
    "        y_total=[]\n",
    "        count=0\n",
    "\n",
    "        for train_index, test_index in kf.split(Classifier_Train_X,Classifier_Train_Y):\n",
    "    #         print('cv_fold',count)\n",
    "    #         print(train_index, test_index)\n",
    "    #         print(type(Classifier_Train_X), type(Classifier_Train_Y))\n",
    "            X_train, X_test = Classifier_Train_X[train_index], Classifier_Train_X[test_index]\n",
    "            y_train, y_test = Classifier_Train_Y[train_index], Classifier_Train_Y[test_index]\n",
    "\n",
    "        #     model_featureSelection = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l1\",C=0.1, dual=True, random_state=42, n_jobs=1))\n",
    "            \n",
    "        #     ada = ADASYN(random_state=42, ratio='minority')\n",
    "        #     ada = SMOTETomek(random_state=42,n_jobs=4)\n",
    "        #     X_train, y_train = ada.fit_sample(X_train, y_train)\n",
    "        #     print('After OVERSAMPLING Num poinst=', Counter(y_train))\n",
    "            classifier=model \n",
    "            y_pl=np.concatenate((y_train,y_test_pl),axis=0)\n",
    "            X_pl=np.concatenate((X_train,X_test_pl),axis=0)\n",
    "\n",
    "            classifier.fit(X_pl,y_pl)\n",
    "            y_preds = classifier.predict(X_test)\n",
    "            for ele in y_test:\n",
    "                y_total.append(ele)\n",
    "            for ele in y_preds:\n",
    "                y_total_preds.append(ele)\n",
    "            y_pred_train = classifier.predict(X_train)\n",
    "            count=count+1       \n",
    "            print('accuracy_train:',accuracy_score(y_train, y_pred_train),'accuracy_test:',accuracy_score(y_test, y_preds))\n",
    "            print('TRAINING:')\n",
    "            print(classification_report( y_train, y_pred_train ))\n",
    "            print(\"TESTING:\")\n",
    "            print(classification_report( y_test, y_preds ))\n",
    "            \n",
    "        report = classification_report( y_total, y_total_preds )\n",
    "        cm=confusion_matrix(y_total, y_total_preds)\n",
    "        plt=plot_confusion_matrix(cm,normalize= True,target_names = ['non_hatespeech','hatespeech'],title = \"Confusion Matrix\")\n",
    "        import time\n",
    "        import zipfile\n",
    "\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        plt.savefig('task1_'+timestr+'_'+model_name+'_'+img_name)\n",
    "        print(model)\n",
    "        print(report)\n",
    "        print(accuracy_score(y_total, y_total_preds))\n",
    "        df_result=pandas_classification_report(y_total,y_total_preds)\n",
    "        df_result.to_csv('task1_'+timestr+'__'+model_name+'_'+report_name,  sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_model = ['Catboost']\n",
    "#list_of_model = ['logistic_regression']\n",
    "\n",
    "\n",
    "for each_model in list_of_model:\n",
    "    model=get_model(m_type=each_model)\n",
    "    binny_classifier_run_pl(X,y,X_test,list_1,model,each_model,label_map,img_name,report_name,save_model=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:HASOC]",
   "language": "python",
   "name": "conda-env-HASOC-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
